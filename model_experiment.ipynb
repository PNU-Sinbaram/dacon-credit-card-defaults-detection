{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd0bed0ea09e0a217f0cd8c3af8b97b5ce48feb5846fec0e29c23d707f4dd7f9787",
   "display_name": "Python 3.8.3 64-bit ('mlenv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 0) 데이터 전처리\n",
    "* Baseline코드에서 사용했던 전처리 방식을 그대로 사용\n",
    "* index column은 학습에 관련이 없으니 제거"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "train = pd.read_csv(\"../data/train.csv\")                            #r*c = 20000*20, test와 달리 credit이라는 column을 갖고 있고, 이 값을 예측\n",
    "test = pd.read_csv(\"../data/test.csv\")                              #10000*19. train으로 학습시키고 test데이터를 입력으로 넣어서 credit을 예측\n",
    "sample_submission = pd.read_csv(\"../data/sample_submission.csv\")    #예측값은 sample_submission과 형태가 같아야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train, test], axis=0)    # train데이터 밑에 test데이터가 붙음. test에는 credit이 없으므로, 결측치(NaN)형태로 저장됨\n",
    "# 실제로는 결측치를 완전히 날리는건 좋지 않지만, 1회 출제용으로 사용하기때문에 완전히 날림\n",
    "data = data.drop(\"occyp_type\", axis=1) # occyp_type column을 지움. axis : occyp_type이 row에 있는지 column에 있는지 알려줌. 1이면 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_len = data.apply(lambda x : len(x.unique()))  # 각 column별로 unique()를 수행하여, 그 길이를 반환. 즉, 모든 column의 요소의 개수를 출력\n",
    "group_1 = unique_len[unique_len <= 2].index   # 요소의 값이 2개 이하인 column들의 이름을 추출\n",
    "group_2 = unique_len[(unique_len > 2) & (unique_len <= 10)].index\n",
    "group_3 = unique_len[(unique_len > 10)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['gender'] = data['gender'].replace(['F','M'], [0, 1])   # F를 0으로, M을 1로 교체\n",
    "data['car'] = data['car'].replace(['N', 'Y'], [0, 1])\n",
    "data['reality'] = data['reality'].replace(['N', 'Y'], [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['child_num']>2, 'child_num'] = 2  # child_num>2인 child_num column을 가져와서 2로 바꿈\n",
    "data[group_2].apply(lambda x : len(x.unique()))\n",
    "label_encoder = preprocessing.LabelEncoder() # categorical 변수(문자로 되어있는 변수)들을 숫자로 인코딩해주는 함수\n",
    "set(label_encoder.fit_transform(data['income_type'])) # income_type column에서 각 요소들을 숫자로 바꿔줌. fit_transform이 배열을 반환해서 unique()대신 set을 사용\n",
    "data['income_type'] = label_encoder.fit_transform(data['income_type'])\n",
    "data['edu_type'] = label_encoder.fit_transform(data['edu_type'])\n",
    "data['family_type'] = label_encoder.fit_transform(data['family_type'])\n",
    "data['house_type'] = label_encoder.fit_transform(data['house_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, bin_dividers = np.histogram(data['income_total'], bins=7) # 연속형 변수를 입력으로 받아 몇 개의 구간으로 나눌지 설정해줌. 각 구간들의 분절점(나누는 기준) 및 구간별 요소의 개수를 출력해줌\n",
    "data['income_total'] = pd.factorize(pd.cut(data['income_total'], bins = bin_dividers, include_lowest=True, labels=[0,1,2,3,4,5,6]))[0] # pd.cut의 반환 데이터 타입은 category이기 때문에, series타입(int형 배열)으로 바꿔주는 작업을 거쳐야 함\n",
    "#위의 과정을 함수로 만듬\n",
    "def make_bin(array, N):\n",
    "    array = -array      #DAYS_BIRTH등의 column은 음수이기 때문에 양수로 바꿔줌\n",
    "    _, bin_dividers = np.histogram(array, bins = N)       # 여기선 counts 변수를 사용하지 않을 것이기 때문에 사용하지 않는다는 의미로 _로 설정.\n",
    "    cut_categories = pd.cut(array, bin_dividers, labels = [i for i in range(N)], include_lowest=True)\n",
    "    bined_array = pd.factorize(cut_categories)[0]\n",
    "    return bined_array\n",
    "data['DAYS_BIRTH'] = make_bin(data['DAYS_BIRTH'], 10)\n",
    "data['DAYS_EMPLOYED'] = make_bin(data['DAYS_EMPLOYED'], 10)\n",
    "data['begin_month'] = make_bin(data['begin_month'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop('index', axis=1)\n",
    "train = data[:-10000]   # train에 해당하는 값\n",
    "test = data[-10000:]   #test에 해당하는 값\n",
    "train_x = train.drop(\"credit\", axis = 1) # credit은 출력값이고, credit을 제외한 값들이 모델의 입력값이므로 column들 중(axis =1) credit을 찾아 없앰.\n",
    "train_y = train['credit']               # 모델의 출력이 credit\n",
    "test_x = test.drop(\"credit\", axis=1)        # data라는 dataframe을 만들면서 test set에 없던 credit이라는 column이 생겼으므로, 이를 다시 제거"
   ]
  },
  {
   "source": [
    "## 1) RandomForestClassifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, stratify=train_y, test_size=0.25, random_state=10086)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.466047343279552\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, Y_train)\n",
    "predict = clf.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "logloss = log_loss(y_val_onehot, predict)\n",
    "print(logloss)"
   ]
  },
  {
   "source": [
    "### 1_1) 하이퍼파라미터 조정"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_param_grid = {\n",
    "    'n_estimators' : [100, 200],\n",
    "    'max_depth' : [8, 10, 12],\n",
    "    'min_samples_leaf' : [3, 5, 7],\n",
    "    'min_samples_split' : [3, 4, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "0.8262986236456679\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, stratify=train_y, test_size=0.25, random_state=10086)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf_grid = GridSearchCV(clf, param_grid=rf_param_grid, scoring=\"accuracy\", n_jobs=-1, verbose=2)\n",
    "clf_grid.fit(X_train, Y_train)\n",
    "\n",
    "predict = clf_grid.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "logloss=log_loss(y_val_onehot, predict)\n",
    "print(logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[0.8282641291283019, 0.8225467954824329, 0.8253606275571261, 0.8224319510375313, 0.8201274143462735]\n"
     ]
    }
   ],
   "source": [
    "folds = StratifiedKFold(n_splits = 5, shuffle=True, random_state = 55)  # 5개의 fold로 나눔\n",
    "\n",
    "outcomes = []\n",
    "for n_fold, (train_index, val_index) in enumerate(folds.split(train_x, train_y)):\n",
    "    X_train, X_val = train_x.iloc[train_index], train_x.iloc[val_index]\n",
    "    Y_train, Y_val = train_y.iloc[train_index], train_y.iloc[val_index]\n",
    "    clf = RandomForestClassifier()\n",
    "    clf_grid = GridSearchCV(clf, param_grid=rf_param_grid, scoring=\"accuracy\", n_jobs=-1, verbose=2)\n",
    "    clf_grid.fit(X_train, Y_train)\n",
    "    predictions = clf_grid.predict_proba(X_val)\n",
    "    y_val_onehot = pd.get_dummies(Y_val)\n",
    "\n",
    "    logloss = log_loss(y_val_onehot, predictions)\n",
    "    outcomes.append(logloss)\n",
    "\n",
    "print(outcomes)"
   ]
  },
  {
   "source": [
    "## 2) Gradient Boosting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8335746908166416\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, stratify=train_y, test_size=0.25, random_state=10086)\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(X_train, Y_train)\n",
    "predict = gb.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "logloss = log_loss(y_val_onehot, predict)\n",
    "print(logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.8417415560977669, 0.8341085098795408, 0.8386208102239824, 0.8392900501671904, 0.8335760935299279]\n"
     ]
    }
   ],
   "source": [
    "folds = StratifiedKFold(n_splits = 5, shuffle=True, random_state = 55)  # 5개의 fold로 나눔\n",
    "\n",
    "outcomes = []\n",
    "for n_fold, (train_index, val_index) in enumerate(folds.split(train_x, train_y)):\n",
    "    X_train, X_val = train_x.iloc[train_index], train_x.iloc[val_index]\n",
    "    Y_train, Y_val = train_y.iloc[train_index], train_y.iloc[val_index]\n",
    "    gb = GradientBoostingClassifier()\n",
    "    gb.fit(X_train, Y_train)\n",
    "    predictions = gb.predict_proba(X_val)\n",
    "    y_val_onehot = pd.get_dummies(Y_val)\n",
    "\n",
    "    logloss = log_loss(y_val_onehot, predictions)\n",
    "    outcomes.append(logloss)\n",
    "print(outcomes)"
   ]
  },
  {
   "source": [
    "### 2_1) Gradient Boosting parameter 조정\n",
    "* **주의. 한번 실행에 시간 소요가 너무 김**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_param_grid = {\n",
    "    'n_estimators' : [100, 200],\n",
    "    'max_depth' : [8, 10, 12],\n",
    "    'min_samples_leaf' : [5, 7, 10],\n",
    "    'min_samples_split' : [2, 3, 5],\n",
    "    'learning_rate' : [0.05, 0.1, 0.2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-4a78ca480250>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mgb_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgb_param_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mgb_grid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgb_grid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0my_val_onehot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pythonenv\\mlenv\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pythonenv\\mlenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    839\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    842\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pythonenv\\mlenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1294\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1295\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1296\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pythonenv\\mlenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    793\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[0;32m    796\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pythonenv\\mlenv\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1054\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1055\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pythonenv\\mlenv\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    931\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pythonenv\\mlenv\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    432\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "gb_grid = GridSearchCV(gb, param_grid = gb_param_grid, scoring=\"accuracy\", n_jobs=-1, verbose=3)\n",
    "gb_grid.fit(X_train, Y_train)\n",
    "predict = gb_grid.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "logloss = log_loss(y_val_onehot, predict)\n",
    "print(logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'learning_rate': 0.05,\n",
       " 'max_depth': 8,\n",
       " 'min_samples_leaf': 7,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 100}"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "gb_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.8299092537458699, 0.8195784379737493, 0.822702511343288, 0.8219319425895892, 0.8183218626777928]\n"
     ]
    }
   ],
   "source": [
    "folds = StratifiedKFold(n_splits = 5, shuffle=True, random_state = 55)  # 5개의 fold로 나눔\n",
    "\n",
    "outcomes = []\n",
    "for n_fold, (train_index, val_index) in enumerate(folds.split(train_x, train_y)):\n",
    "    X_train, X_val = train_x.iloc[train_index], train_x.iloc[val_index]\n",
    "    Y_train, Y_val = train_y.iloc[train_index], train_y.iloc[val_index]\n",
    "    gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.05, max_depth=8, min_samples_leaf=7, min_samples_split=2)\n",
    "    gb.fit(X_train, Y_train)\n",
    "    predict = gb.predict_proba(X_val)\n",
    "    y_val_onehot = pd.get_dummies(Y_val)\n",
    "    logloss = log_loss(y_val_onehot, predict)\n",
    "    outcomes.append(logloss)\n",
    "print(outcomes)"
   ]
  },
  {
   "source": [
    "## 3) AdaBoost"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.0877788002340585"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, stratify=train_y, test_size=0.25, random_state=10086)\n",
    "\n",
    "ag = AdaBoostClassifier()\n",
    "ag.fit(X_train, Y_train)\n",
    "\n",
    "predict = ag.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "log_loss(y_val_onehot, predict)"
   ]
  },
  {
   "source": [
    "## 4) XgBoost"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[20:56:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "[20:56:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "[20:56:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "[20:56:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "[20:56:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.8324195141525684, 0.8266500276527476, 0.8265990901853171, 0.8235172349839919, 0.8234311175080091]\n",
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "outcomes = []\n",
    "for n_fold, (train_index, val_index) in enumerate(folds.split(train_x, train_y)):\n",
    "    X_train, X_val = train_x.iloc[train_index], train_x.iloc[val_index]\n",
    "    Y_train, Y_val = train_y.iloc[train_index], train_y.iloc[val_index]\n",
    "    xgb = XGBClassifier(n_estimators=500, learning_rate=0.1, max_depth = 4, use_label_encoder=False)\n",
    "    xgb.fit(X_train, Y_train)\n",
    "    predictions = xgb.predict_proba(X_val)\n",
    "    y_val_onehot = pd.get_dummies(Y_val)\n",
    "\n",
    "    logloss = log_loss(y_val_onehot, predictions)\n",
    "    outcomes.append(logloss)\n",
    "print(outcomes)"
   ]
  },
  {
   "source": [
    "### 4_1) XgBoost hyperparameter tuning\n",
    "* **주의. 한번 실행에 시간 소요가 너무 김**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "xgb_param_grid = {\n",
    "    'n_estimators' : [300, 500, 600],\n",
    "    'learning_rate' : [0.01, 0.05, 0.1, 0.15],\n",
    "    'max_depth' : [3, 4, 6, 8]\n",
    "}\n",
    "\n",
    "xgb_grid = GridSearchCV(xgb, param_grid = xgb_param_grid, scoring=\"accuracy\", n_jobs= -1, verbose = 3)\n",
    "xgb_grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "실행 시간이 너무 오래 걸려 전에 돌려놨던 결과로 대체함\n",
    "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
    "c:\\Users\\lijm1\\Desktop\\pythonenv\\mlenv\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
    "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
    "[01:20:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
    "GridSearchCV(estimator=XGBClassifier(base_score=None, booster=None,\n",
    "                                     colsample_bylevel=None,\n",
    "                                     colsample_bynode=None,\n",
    "                                     colsample_bytree=None, gamma=None,\n",
    "                                     gpu_id=None, importance_type='gain',\n",
    "                                     interaction_constraints=None,\n",
    "                                     learning_rate=None, max_delta_step=None,\n",
    "                                     max_depth=None, min_child_weight=None,\n",
    "                                     missing=nan, monotone_constraints=None,\n",
    "                                     n_estimators=100, n_jobs=None,\n",
    "                                     num_parallel_tree=None, random_state=None,\n",
    "                                     reg_alpha=None, reg_lambda=None,\n",
    "                                     scale_pos_weight=None, subsample=None,\n",
    "                                     tree_method=None, validate_parameters=None,\n",
    "                                     verbosity=None),\n",
    "             n_jobs=-1,\n",
    "             param_grid={'learning_rate': [0.01], 'max_depth': [4],\n",
    "                         'n_estimators': [500]},\n",
    "             scoring='accuracy', verbose=3)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[20:57:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8359875737021389"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, stratify=train_y, test_size=0.25, random_state=10086)\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=500, learning_rate=0.01, max_depth = 4, use_label_encoder=False)\n",
    "xgb.fit(X_train, Y_train)\n",
    "predictions = xgb.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "\n",
    "log_loss(y_val_onehot, predictions)"
   ]
  },
  {
   "source": [
    "## 5) LightGBM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8248750259707588"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier, plot_importance\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, stratify=train_y, test_size=0.25, random_state=10086)\n",
    "\n",
    "lgb = LGBMClassifier(n_estimators=400)\n",
    "lgb.fit(X_train, Y_train)\n",
    "predict = lgb.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "log_loss(y_val_onehot, predict)"
   ]
  },
  {
   "source": [
    "### 5_1) LGBM Early stopping 적용"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1]\tvalid_0's multi_logloss: 0.871384\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tvalid_0's multi_logloss: 0.863215\n",
      "[3]\tvalid_0's multi_logloss: 0.857059\n",
      "[4]\tvalid_0's multi_logloss: 0.852325\n",
      "[5]\tvalid_0's multi_logloss: 0.848258\n",
      "[6]\tvalid_0's multi_logloss: 0.84532\n",
      "[7]\tvalid_0's multi_logloss: 0.842952\n",
      "[8]\tvalid_0's multi_logloss: 0.840694\n",
      "[9]\tvalid_0's multi_logloss: 0.83901\n",
      "[10]\tvalid_0's multi_logloss: 0.837603\n",
      "[11]\tvalid_0's multi_logloss: 0.836505\n",
      "[12]\tvalid_0's multi_logloss: 0.835527\n",
      "[13]\tvalid_0's multi_logloss: 0.834671\n",
      "[14]\tvalid_0's multi_logloss: 0.833793\n",
      "[15]\tvalid_0's multi_logloss: 0.833183\n",
      "[16]\tvalid_0's multi_logloss: 0.83234\n",
      "[17]\tvalid_0's multi_logloss: 0.831761\n",
      "[18]\tvalid_0's multi_logloss: 0.831425\n",
      "[19]\tvalid_0's multi_logloss: 0.830885\n",
      "[20]\tvalid_0's multi_logloss: 0.830542\n",
      "[21]\tvalid_0's multi_logloss: 0.830351\n",
      "[22]\tvalid_0's multi_logloss: 0.830166\n",
      "[23]\tvalid_0's multi_logloss: 0.830036\n",
      "[24]\tvalid_0's multi_logloss: 0.829671\n",
      "[25]\tvalid_0's multi_logloss: 0.829519\n",
      "[26]\tvalid_0's multi_logloss: 0.829229\n",
      "[27]\tvalid_0's multi_logloss: 0.829262\n",
      "[28]\tvalid_0's multi_logloss: 0.829037\n",
      "[29]\tvalid_0's multi_logloss: 0.828849\n",
      "[30]\tvalid_0's multi_logloss: 0.828643\n",
      "[31]\tvalid_0's multi_logloss: 0.828665\n",
      "[32]\tvalid_0's multi_logloss: 0.828359\n",
      "[33]\tvalid_0's multi_logloss: 0.828262\n",
      "[34]\tvalid_0's multi_logloss: 0.827804\n",
      "[35]\tvalid_0's multi_logloss: 0.827619\n",
      "[36]\tvalid_0's multi_logloss: 0.827389\n",
      "[37]\tvalid_0's multi_logloss: 0.827356\n",
      "[38]\tvalid_0's multi_logloss: 0.827293\n",
      "[39]\tvalid_0's multi_logloss: 0.826958\n",
      "[40]\tvalid_0's multi_logloss: 0.826816\n",
      "[41]\tvalid_0's multi_logloss: 0.826786\n",
      "[42]\tvalid_0's multi_logloss: 0.826573\n",
      "[43]\tvalid_0's multi_logloss: 0.82634\n",
      "[44]\tvalid_0's multi_logloss: 0.826291\n",
      "[45]\tvalid_0's multi_logloss: 0.826135\n",
      "[46]\tvalid_0's multi_logloss: 0.826101\n",
      "[47]\tvalid_0's multi_logloss: 0.825966\n",
      "[48]\tvalid_0's multi_logloss: 0.825712\n",
      "[49]\tvalid_0's multi_logloss: 0.825559\n",
      "[50]\tvalid_0's multi_logloss: 0.825187\n",
      "[51]\tvalid_0's multi_logloss: 0.825019\n",
      "[52]\tvalid_0's multi_logloss: 0.825123\n",
      "[53]\tvalid_0's multi_logloss: 0.825115\n",
      "[54]\tvalid_0's multi_logloss: 0.824682\n",
      "[55]\tvalid_0's multi_logloss: 0.824667\n",
      "[56]\tvalid_0's multi_logloss: 0.824501\n",
      "[57]\tvalid_0's multi_logloss: 0.824404\n",
      "[58]\tvalid_0's multi_logloss: 0.824241\n",
      "[59]\tvalid_0's multi_logloss: 0.824274\n",
      "[60]\tvalid_0's multi_logloss: 0.824144\n",
      "[61]\tvalid_0's multi_logloss: 0.823991\n",
      "[62]\tvalid_0's multi_logloss: 0.823945\n",
      "[63]\tvalid_0's multi_logloss: 0.823793\n",
      "[64]\tvalid_0's multi_logloss: 0.82363\n",
      "[65]\tvalid_0's multi_logloss: 0.823577\n",
      "[66]\tvalid_0's multi_logloss: 0.823364\n",
      "[67]\tvalid_0's multi_logloss: 0.823236\n",
      "[68]\tvalid_0's multi_logloss: 0.823105\n",
      "[69]\tvalid_0's multi_logloss: 0.823008\n",
      "[70]\tvalid_0's multi_logloss: 0.823012\n",
      "[71]\tvalid_0's multi_logloss: 0.823018\n",
      "[72]\tvalid_0's multi_logloss: 0.822906\n",
      "[73]\tvalid_0's multi_logloss: 0.822797\n",
      "[74]\tvalid_0's multi_logloss: 0.822914\n",
      "[75]\tvalid_0's multi_logloss: 0.822859\n",
      "[76]\tvalid_0's multi_logloss: 0.822457\n",
      "[77]\tvalid_0's multi_logloss: 0.822447\n",
      "[78]\tvalid_0's multi_logloss: 0.822259\n",
      "[79]\tvalid_0's multi_logloss: 0.822138\n",
      "[80]\tvalid_0's multi_logloss: 0.822083\n",
      "[81]\tvalid_0's multi_logloss: 0.822013\n",
      "[82]\tvalid_0's multi_logloss: 0.822043\n",
      "[83]\tvalid_0's multi_logloss: 0.822145\n",
      "[84]\tvalid_0's multi_logloss: 0.822083\n",
      "[85]\tvalid_0's multi_logloss: 0.821986\n",
      "[86]\tvalid_0's multi_logloss: 0.821953\n",
      "[87]\tvalid_0's multi_logloss: 0.821722\n",
      "[88]\tvalid_0's multi_logloss: 0.821543\n",
      "[89]\tvalid_0's multi_logloss: 0.821408\n",
      "[90]\tvalid_0's multi_logloss: 0.821313\n",
      "[91]\tvalid_0's multi_logloss: 0.821416\n",
      "[92]\tvalid_0's multi_logloss: 0.820971\n",
      "[93]\tvalid_0's multi_logloss: 0.820791\n",
      "[94]\tvalid_0's multi_logloss: 0.820557\n",
      "[95]\tvalid_0's multi_logloss: 0.820226\n",
      "[96]\tvalid_0's multi_logloss: 0.820044\n",
      "[97]\tvalid_0's multi_logloss: 0.819827\n",
      "[98]\tvalid_0's multi_logloss: 0.819404\n",
      "[99]\tvalid_0's multi_logloss: 0.819393\n",
      "[100]\tvalid_0's multi_logloss: 0.819325\n",
      "[101]\tvalid_0's multi_logloss: 0.81921\n",
      "[102]\tvalid_0's multi_logloss: 0.819381\n",
      "[103]\tvalid_0's multi_logloss: 0.819383\n",
      "[104]\tvalid_0's multi_logloss: 0.819118\n",
      "[105]\tvalid_0's multi_logloss: 0.818937\n",
      "[106]\tvalid_0's multi_logloss: 0.818774\n",
      "[107]\tvalid_0's multi_logloss: 0.818807\n",
      "[108]\tvalid_0's multi_logloss: 0.818723\n",
      "[109]\tvalid_0's multi_logloss: 0.818886\n",
      "[110]\tvalid_0's multi_logloss: 0.818767\n",
      "[111]\tvalid_0's multi_logloss: 0.818832\n",
      "[112]\tvalid_0's multi_logloss: 0.818768\n",
      "[113]\tvalid_0's multi_logloss: 0.818561\n",
      "[114]\tvalid_0's multi_logloss: 0.818637\n",
      "[115]\tvalid_0's multi_logloss: 0.8185\n",
      "[116]\tvalid_0's multi_logloss: 0.818314\n",
      "[117]\tvalid_0's multi_logloss: 0.818235\n",
      "[118]\tvalid_0's multi_logloss: 0.818215\n",
      "[119]\tvalid_0's multi_logloss: 0.818015\n",
      "[120]\tvalid_0's multi_logloss: 0.818105\n",
      "[121]\tvalid_0's multi_logloss: 0.817966\n",
      "[122]\tvalid_0's multi_logloss: 0.817894\n",
      "[123]\tvalid_0's multi_logloss: 0.817587\n",
      "[124]\tvalid_0's multi_logloss: 0.817499\n",
      "[125]\tvalid_0's multi_logloss: 0.817386\n",
      "[126]\tvalid_0's multi_logloss: 0.817303\n",
      "[127]\tvalid_0's multi_logloss: 0.81739\n",
      "[128]\tvalid_0's multi_logloss: 0.817411\n",
      "[129]\tvalid_0's multi_logloss: 0.817387\n",
      "[130]\tvalid_0's multi_logloss: 0.817349\n",
      "[131]\tvalid_0's multi_logloss: 0.817435\n",
      "[132]\tvalid_0's multi_logloss: 0.817424\n",
      "[133]\tvalid_0's multi_logloss: 0.817413\n",
      "[134]\tvalid_0's multi_logloss: 0.817493\n",
      "[135]\tvalid_0's multi_logloss: 0.817377\n",
      "[136]\tvalid_0's multi_logloss: 0.81738\n",
      "[137]\tvalid_0's multi_logloss: 0.817454\n",
      "[138]\tvalid_0's multi_logloss: 0.817259\n",
      "[139]\tvalid_0's multi_logloss: 0.81714\n",
      "[140]\tvalid_0's multi_logloss: 0.816845\n",
      "[141]\tvalid_0's multi_logloss: 0.816691\n",
      "[142]\tvalid_0's multi_logloss: 0.816643\n",
      "[143]\tvalid_0's multi_logloss: 0.816532\n",
      "[144]\tvalid_0's multi_logloss: 0.81637\n",
      "[145]\tvalid_0's multi_logloss: 0.81613\n",
      "[146]\tvalid_0's multi_logloss: 0.815985\n",
      "[147]\tvalid_0's multi_logloss: 0.816012\n",
      "[148]\tvalid_0's multi_logloss: 0.816032\n",
      "[149]\tvalid_0's multi_logloss: 0.81607\n",
      "[150]\tvalid_0's multi_logloss: 0.815985\n",
      "[151]\tvalid_0's multi_logloss: 0.816081\n",
      "[152]\tvalid_0's multi_logloss: 0.815991\n",
      "[153]\tvalid_0's multi_logloss: 0.815889\n",
      "[154]\tvalid_0's multi_logloss: 0.815904\n",
      "[155]\tvalid_0's multi_logloss: 0.815886\n",
      "[156]\tvalid_0's multi_logloss: 0.815752\n",
      "[157]\tvalid_0's multi_logloss: 0.815806\n",
      "[158]\tvalid_0's multi_logloss: 0.815742\n",
      "[159]\tvalid_0's multi_logloss: 0.815868\n",
      "[160]\tvalid_0's multi_logloss: 0.815914\n",
      "[161]\tvalid_0's multi_logloss: 0.815892\n",
      "[162]\tvalid_0's multi_logloss: 0.815838\n",
      "[163]\tvalid_0's multi_logloss: 0.815685\n",
      "[164]\tvalid_0's multi_logloss: 0.815754\n",
      "[165]\tvalid_0's multi_logloss: 0.815742\n",
      "[166]\tvalid_0's multi_logloss: 0.815818\n",
      "[167]\tvalid_0's multi_logloss: 0.815877\n",
      "[168]\tvalid_0's multi_logloss: 0.815856\n",
      "[169]\tvalid_0's multi_logloss: 0.816004\n",
      "[170]\tvalid_0's multi_logloss: 0.816022\n",
      "[171]\tvalid_0's multi_logloss: 0.816048\n",
      "[172]\tvalid_0's multi_logloss: 0.8161\n",
      "[173]\tvalid_0's multi_logloss: 0.816062\n",
      "[174]\tvalid_0's multi_logloss: 0.816022\n",
      "[175]\tvalid_0's multi_logloss: 0.815977\n",
      "[176]\tvalid_0's multi_logloss: 0.81602\n",
      "[177]\tvalid_0's multi_logloss: 0.816042\n",
      "[178]\tvalid_0's multi_logloss: 0.816101\n",
      "[179]\tvalid_0's multi_logloss: 0.816191\n",
      "[180]\tvalid_0's multi_logloss: 0.816228\n",
      "[181]\tvalid_0's multi_logloss: 0.8162\n",
      "[182]\tvalid_0's multi_logloss: 0.816205\n",
      "[183]\tvalid_0's multi_logloss: 0.816312\n",
      "[184]\tvalid_0's multi_logloss: 0.816287\n",
      "[185]\tvalid_0's multi_logloss: 0.816403\n",
      "[186]\tvalid_0's multi_logloss: 0.816519\n",
      "[187]\tvalid_0's multi_logloss: 0.816444\n",
      "[188]\tvalid_0's multi_logloss: 0.816421\n",
      "[189]\tvalid_0's multi_logloss: 0.816298\n",
      "[190]\tvalid_0's multi_logloss: 0.816265\n",
      "[191]\tvalid_0's multi_logloss: 0.816343\n",
      "[192]\tvalid_0's multi_logloss: 0.816423\n",
      "[193]\tvalid_0's multi_logloss: 0.816623\n",
      "[194]\tvalid_0's multi_logloss: 0.81658\n",
      "[195]\tvalid_0's multi_logloss: 0.816574\n",
      "[196]\tvalid_0's multi_logloss: 0.816605\n",
      "[197]\tvalid_0's multi_logloss: 0.816762\n",
      "[198]\tvalid_0's multi_logloss: 0.816645\n",
      "[199]\tvalid_0's multi_logloss: 0.816797\n",
      "[200]\tvalid_0's multi_logloss: 0.816738\n",
      "[201]\tvalid_0's multi_logloss: 0.816695\n",
      "[202]\tvalid_0's multi_logloss: 0.816749\n",
      "[203]\tvalid_0's multi_logloss: 0.816764\n",
      "[204]\tvalid_0's multi_logloss: 0.816815\n",
      "[205]\tvalid_0's multi_logloss: 0.816856\n",
      "[206]\tvalid_0's multi_logloss: 0.816949\n",
      "[207]\tvalid_0's multi_logloss: 0.816955\n",
      "[208]\tvalid_0's multi_logloss: 0.816852\n",
      "[209]\tvalid_0's multi_logloss: 0.816964\n",
      "[210]\tvalid_0's multi_logloss: 0.817072\n",
      "[211]\tvalid_0's multi_logloss: 0.817013\n",
      "[212]\tvalid_0's multi_logloss: 0.817144\n",
      "[213]\tvalid_0's multi_logloss: 0.817175\n",
      "[214]\tvalid_0's multi_logloss: 0.817195\n",
      "[215]\tvalid_0's multi_logloss: 0.817245\n",
      "[216]\tvalid_0's multi_logloss: 0.817418\n",
      "[217]\tvalid_0's multi_logloss: 0.817516\n",
      "[218]\tvalid_0's multi_logloss: 0.817628\n",
      "[219]\tvalid_0's multi_logloss: 0.817768\n",
      "[220]\tvalid_0's multi_logloss: 0.817828\n",
      "[221]\tvalid_0's multi_logloss: 0.81796\n",
      "[222]\tvalid_0's multi_logloss: 0.818176\n",
      "[223]\tvalid_0's multi_logloss: 0.8182\n",
      "[224]\tvalid_0's multi_logloss: 0.818271\n",
      "[225]\tvalid_0's multi_logloss: 0.818258\n",
      "[226]\tvalid_0's multi_logloss: 0.818392\n",
      "[227]\tvalid_0's multi_logloss: 0.818549\n",
      "[228]\tvalid_0's multi_logloss: 0.818574\n",
      "[229]\tvalid_0's multi_logloss: 0.818664\n",
      "[230]\tvalid_0's multi_logloss: 0.818693\n",
      "[231]\tvalid_0's multi_logloss: 0.818632\n",
      "[232]\tvalid_0's multi_logloss: 0.818626\n",
      "[233]\tvalid_0's multi_logloss: 0.818638\n",
      "[234]\tvalid_0's multi_logloss: 0.818587\n",
      "[235]\tvalid_0's multi_logloss: 0.818552\n",
      "[236]\tvalid_0's multi_logloss: 0.818431\n",
      "[237]\tvalid_0's multi_logloss: 0.818532\n",
      "[238]\tvalid_0's multi_logloss: 0.818426\n",
      "[239]\tvalid_0's multi_logloss: 0.818505\n",
      "[240]\tvalid_0's multi_logloss: 0.818419\n",
      "[241]\tvalid_0's multi_logloss: 0.81839\n",
      "[242]\tvalid_0's multi_logloss: 0.818433\n",
      "[243]\tvalid_0's multi_logloss: 0.818293\n",
      "[244]\tvalid_0's multi_logloss: 0.818186\n",
      "[245]\tvalid_0's multi_logloss: 0.818149\n",
      "[246]\tvalid_0's multi_logloss: 0.818069\n",
      "[247]\tvalid_0's multi_logloss: 0.81819\n",
      "[248]\tvalid_0's multi_logloss: 0.818154\n",
      "[249]\tvalid_0's multi_logloss: 0.818171\n",
      "[250]\tvalid_0's multi_logloss: 0.818223\n",
      "[251]\tvalid_0's multi_logloss: 0.818296\n",
      "[252]\tvalid_0's multi_logloss: 0.818262\n",
      "[253]\tvalid_0's multi_logloss: 0.818232\n",
      "[254]\tvalid_0's multi_logloss: 0.818197\n",
      "[255]\tvalid_0's multi_logloss: 0.818272\n",
      "[256]\tvalid_0's multi_logloss: 0.818405\n",
      "[257]\tvalid_0's multi_logloss: 0.818496\n",
      "[258]\tvalid_0's multi_logloss: 0.818531\n",
      "[259]\tvalid_0's multi_logloss: 0.818558\n",
      "[260]\tvalid_0's multi_logloss: 0.818767\n",
      "[261]\tvalid_0's multi_logloss: 0.81879\n",
      "[262]\tvalid_0's multi_logloss: 0.818777\n",
      "[263]\tvalid_0's multi_logloss: 0.818814\n",
      "Early stopping, best iteration is:\n",
      "[163]\tvalid_0's multi_logloss: 0.815685\n"
     ]
    }
   ],
   "source": [
    "lgb = LGBMClassifier(n_estimators=1000)\n",
    "evals = [(X_val, Y_val)]\n",
    "lgb.fit(X_train, Y_train, early_stopping_rounds=100, eval_metric=\"logloss\", eval_set=evals, verbose=True)\n",
    "predict = lgb.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "logloss = log_loss(y_val_onehot, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8156847808255132"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "logloss"
   ]
  },
  {
   "source": [
    "## 6) TabNet"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "source": [
    "* **주의. 한번 실행에 시간 소요가 너무 김(약 1시간)**\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device used : cuda\n",
      "epoch 0  | loss: 1.30149 | val_0_logloss: 1.60212 |  0:00:06s\n",
      "epoch 1  | loss: 0.93963 | val_0_logloss: 1.04891 |  0:00:11s\n",
      "epoch 2  | loss: 0.9059  | val_0_logloss: 0.9204  |  0:00:17s\n",
      "epoch 3  | loss: 0.88486 | val_0_logloss: 0.88746 |  0:00:23s\n",
      "epoch 4  | loss: 0.87722 | val_0_logloss: 0.88385 |  0:00:28s\n",
      "epoch 5  | loss: 0.87452 | val_0_logloss: 0.87024 |  0:00:34s\n",
      "epoch 6  | loss: 0.86777 | val_0_logloss: 0.87232 |  0:00:40s\n",
      "epoch 7  | loss: 0.85974 | val_0_logloss: 0.87084 |  0:00:45s\n",
      "epoch 8  | loss: 0.85803 | val_0_logloss: 0.87852 |  0:00:51s\n",
      "epoch 9  | loss: 0.85064 | val_0_logloss: 0.85917 |  0:00:56s\n",
      "epoch 10 | loss: 0.84585 | val_0_logloss: 0.86065 |  0:01:02s\n",
      "epoch 11 | loss: 0.84799 | val_0_logloss: 0.85298 |  0:01:08s\n",
      "epoch 12 | loss: 0.84493 | val_0_logloss: 0.84512 |  0:01:14s\n",
      "epoch 13 | loss: 0.84336 | val_0_logloss: 0.8489  |  0:01:19s\n",
      "epoch 14 | loss: 0.84581 | val_0_logloss: 0.85264 |  0:01:25s\n",
      "epoch 15 | loss: 0.84619 | val_0_logloss: 0.85165 |  0:01:31s\n",
      "epoch 16 | loss: 0.8447  | val_0_logloss: 0.85161 |  0:01:36s\n",
      "epoch 17 | loss: 0.84421 | val_0_logloss: 0.84904 |  0:01:42s\n",
      "epoch 18 | loss: 0.84692 | val_0_logloss: 0.84604 |  0:01:48s\n",
      "epoch 19 | loss: 0.8433  | val_0_logloss: 0.84667 |  0:01:53s\n",
      "epoch 20 | loss: 0.84415 | val_0_logloss: 0.84465 |  0:01:59s\n",
      "epoch 21 | loss: 0.84328 | val_0_logloss: 0.84597 |  0:02:05s\n",
      "epoch 22 | loss: 0.84446 | val_0_logloss: 0.86006 |  0:02:11s\n",
      "epoch 23 | loss: 0.84394 | val_0_logloss: 0.84494 |  0:02:17s\n",
      "epoch 24 | loss: 0.84336 | val_0_logloss: 0.85316 |  0:02:23s\n",
      "epoch 25 | loss: 0.8446  | val_0_logloss: 0.84229 |  0:02:28s\n",
      "epoch 26 | loss: 0.84225 | val_0_logloss: 0.84515 |  0:02:34s\n",
      "epoch 27 | loss: 0.84124 | val_0_logloss: 0.84704 |  0:02:39s\n",
      "epoch 28 | loss: 0.84142 | val_0_logloss: 0.84993 |  0:02:45s\n",
      "epoch 29 | loss: 0.84252 | val_0_logloss: 0.84729 |  0:02:51s\n",
      "epoch 30 | loss: 0.84246 | val_0_logloss: 0.84455 |  0:02:56s\n",
      "epoch 31 | loss: 0.84413 | val_0_logloss: 0.84415 |  0:03:02s\n",
      "epoch 32 | loss: 0.84269 | val_0_logloss: 0.8474  |  0:03:07s\n",
      "epoch 33 | loss: 0.84051 | val_0_logloss: 0.84316 |  0:03:13s\n",
      "epoch 34 | loss: 0.83968 | val_0_logloss: 0.84248 |  0:03:18s\n",
      "epoch 35 | loss: 0.84101 | val_0_logloss: 0.84334 |  0:03:24s\n",
      "\n",
      "Early stopping occurred at epoch 35 with best_epoch = 25 and best_val_0_logloss = 0.84229\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1f62c7d4d30>]"
      ]
     },
     "metadata": {},
     "execution_count": 11
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<svg height=\"249.944869pt\" version=\"1.1\" viewBox=\"0 0 384.828125 249.944869\" width=\"384.828125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-05-10T14:20:23.648327</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.4.1, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 249.944869 \r\nL 384.828125 249.944869 \r\nL 384.828125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 42.828125 226.066744 \r\nL 377.628125 226.066744 \r\nL 377.628125 8.626744 \r\nL 42.828125 8.626744 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"maec247e8c9\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"58.046307\" xlink:href=\"#maec247e8c9\" y=\"226.066744\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(54.865057 240.665181)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 4250 \r\nQ 1547 4250 1301 3770 \r\nQ 1056 3291 1056 2328 \r\nQ 1056 1369 1301 889 \r\nQ 1547 409 2034 409 \r\nQ 2525 409 2770 889 \r\nQ 3016 1369 3016 2328 \r\nQ 3016 3291 2770 3770 \r\nQ 2525 4250 2034 4250 \r\nz\r\nM 2034 4750 \r\nQ 2819 4750 3233 4129 \r\nQ 3647 3509 3647 2328 \r\nQ 3647 1150 3233 529 \r\nQ 2819 -91 2034 -91 \r\nQ 1250 -91 836 529 \r\nQ 422 1150 422 2328 \r\nQ 422 3509 836 4129 \r\nQ 1250 4750 2034 4750 \r\nz\r\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"108.77358\" xlink:href=\"#maec247e8c9\" y=\"226.066744\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 5 -->\r\n      <g transform=\"translate(105.59233 240.665181)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 691 4666 \r\nL 3169 4666 \r\nL 3169 4134 \r\nL 1269 4134 \r\nL 1269 2991 \r\nQ 1406 3038 1543 3061 \r\nQ 1681 3084 1819 3084 \r\nQ 2600 3084 3056 2656 \r\nQ 3513 2228 3513 1497 \r\nQ 3513 744 3044 326 \r\nQ 2575 -91 1722 -91 \r\nQ 1428 -91 1123 -41 \r\nQ 819 9 494 109 \r\nL 494 744 \r\nQ 775 591 1075 516 \r\nQ 1375 441 1709 441 \r\nQ 2250 441 2565 725 \r\nQ 2881 1009 2881 1497 \r\nQ 2881 1984 2565 2268 \r\nQ 2250 2553 1709 2553 \r\nQ 1456 2553 1204 2497 \r\nQ 953 2441 691 2322 \r\nL 691 4666 \r\nz\r\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"159.500852\" xlink:href=\"#maec247e8c9\" y=\"226.066744\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 10 -->\r\n      <g transform=\"translate(153.138352 240.665181)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 794 531 \r\nL 1825 531 \r\nL 1825 4091 \r\nL 703 3866 \r\nL 703 4441 \r\nL 1819 4666 \r\nL 2450 4666 \r\nL 2450 531 \r\nL 3481 531 \r\nL 3481 0 \r\nL 794 0 \r\nL 794 531 \r\nz\r\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"210.228125\" xlink:href=\"#maec247e8c9\" y=\"226.066744\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 15 -->\r\n      <g transform=\"translate(203.865625 240.665181)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"260.955398\" xlink:href=\"#maec247e8c9\" y=\"226.066744\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(254.592898 240.665181)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 1228 531 \r\nL 3431 531 \r\nL 3431 0 \r\nL 469 0 \r\nL 469 531 \r\nQ 828 903 1448 1529 \r\nQ 2069 2156 2228 2338 \r\nQ 2531 2678 2651 2914 \r\nQ 2772 3150 2772 3378 \r\nQ 2772 3750 2511 3984 \r\nQ 2250 4219 1831 4219 \r\nQ 1534 4219 1204 4116 \r\nQ 875 4013 500 3803 \r\nL 500 4441 \r\nQ 881 4594 1212 4672 \r\nQ 1544 4750 1819 4750 \r\nQ 2544 4750 2975 4387 \r\nQ 3406 4025 3406 3419 \r\nQ 3406 3131 3298 2873 \r\nQ 3191 2616 2906 2266 \r\nQ 2828 2175 2409 1742 \r\nQ 1991 1309 1228 531 \r\nz\r\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"311.68267\" xlink:href=\"#maec247e8c9\" y=\"226.066744\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 25 -->\r\n      <g transform=\"translate(305.32017 240.665181)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"362.409943\" xlink:href=\"#maec247e8c9\" y=\"226.066744\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 30 -->\r\n      <g transform=\"translate(356.047443 240.665181)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2597 2516 \r\nQ 3050 2419 3304 2112 \r\nQ 3559 1806 3559 1356 \r\nQ 3559 666 3084 287 \r\nQ 2609 -91 1734 -91 \r\nQ 1441 -91 1130 -33 \r\nQ 819 25 488 141 \r\nL 488 750 \r\nQ 750 597 1062 519 \r\nQ 1375 441 1716 441 \r\nQ 2309 441 2620 675 \r\nQ 2931 909 2931 1356 \r\nQ 2931 1769 2642 2001 \r\nQ 2353 2234 1838 2234 \r\nL 1294 2234 \r\nL 1294 2753 \r\nL 1863 2753 \r\nQ 2328 2753 2575 2939 \r\nQ 2822 3125 2822 3475 \r\nQ 2822 3834 2567 4026 \r\nQ 2313 4219 1838 4219 \r\nQ 1578 4219 1281 4162 \r\nQ 984 4106 628 3988 \r\nL 628 4550 \r\nQ 988 4650 1302 4700 \r\nQ 1616 4750 1894 4750 \r\nQ 2613 4750 3031 4423 \r\nQ 3450 4097 3450 3541 \r\nQ 3450 3153 3228 2886 \r\nQ 3006 2619 2597 2516 \r\nz\r\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-33\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_8\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m4b50e86e3c\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"42.828125\" xlink:href=\"#m4b50e86e3c\" y=\"214.536117\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.840 -->\r\n      <g transform=\"translate(7.2 218.335336)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 684 794 \r\nL 1344 794 \r\nL 1344 0 \r\nL 684 0 \r\nL 684 794 \r\nz\r\n\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\r\n        <path d=\"M 2034 2216 \r\nQ 1584 2216 1326 1975 \r\nQ 1069 1734 1069 1313 \r\nQ 1069 891 1326 650 \r\nQ 1584 409 2034 409 \r\nQ 2484 409 2743 651 \r\nQ 3003 894 3003 1313 \r\nQ 3003 1734 2745 1975 \r\nQ 2488 2216 2034 2216 \r\nz\r\nM 1403 2484 \r\nQ 997 2584 770 2862 \r\nQ 544 3141 544 3541 \r\nQ 544 4100 942 4425 \r\nQ 1341 4750 2034 4750 \r\nQ 2731 4750 3128 4425 \r\nQ 3525 4100 3525 3541 \r\nQ 3525 3141 3298 2862 \r\nQ 3072 2584 2669 2484 \r\nQ 3125 2378 3379 2068 \r\nQ 3634 1759 3634 1313 \r\nQ 3634 634 3220 271 \r\nQ 2806 -91 2034 -91 \r\nQ 1263 -91 848 271 \r\nQ 434 634 434 1313 \r\nQ 434 1759 690 2068 \r\nQ 947 2378 1403 2484 \r\nz\r\nM 1172 3481 \r\nQ 1172 3119 1398 2916 \r\nQ 1625 2713 2034 2713 \r\nQ 2441 2713 2670 2916 \r\nQ 2900 3119 2900 3481 \r\nQ 2900 3844 2670 4047 \r\nQ 2441 4250 2034 4250 \r\nQ 1625 4250 1398 4047 \r\nQ 1172 3844 1172 3481 \r\nz\r\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\r\n        <path d=\"M 2419 4116 \r\nL 825 1625 \r\nL 2419 1625 \r\nL 2419 4116 \r\nz\r\nM 2253 4666 \r\nL 3047 4666 \r\nL 3047 1625 \r\nL 3713 1625 \r\nL 3713 1100 \r\nL 3047 1100 \r\nL 3047 0 \r\nL 2419 0 \r\nL 2419 1100 \r\nL 313 1100 \r\nL 313 1709 \r\nL 2253 4666 \r\nz\r\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-34\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"42.828125\" xlink:href=\"#m4b50e86e3c\" y=\"189.094005\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.845 -->\r\n      <g transform=\"translate(7.2 192.893224)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-34\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"42.828125\" xlink:href=\"#m4b50e86e3c\" y=\"163.651892\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.850 -->\r\n      <g transform=\"translate(7.2 167.451111)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"42.828125\" xlink:href=\"#m4b50e86e3c\" y=\"138.20978\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.855 -->\r\n      <g transform=\"translate(7.2 142.008999)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"42.828125\" xlink:href=\"#m4b50e86e3c\" y=\"112.767668\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.860 -->\r\n      <g transform=\"translate(7.2 116.566887)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2113 2584 \r\nQ 1688 2584 1439 2293 \r\nQ 1191 2003 1191 1497 \r\nQ 1191 994 1439 701 \r\nQ 1688 409 2113 409 \r\nQ 2538 409 2786 701 \r\nQ 3034 994 3034 1497 \r\nQ 3034 2003 2786 2293 \r\nQ 2538 2584 2113 2584 \r\nz\r\nM 3366 4563 \r\nL 3366 3988 \r\nQ 3128 4100 2886 4159 \r\nQ 2644 4219 2406 4219 \r\nQ 1781 4219 1451 3797 \r\nQ 1122 3375 1075 2522 \r\nQ 1259 2794 1537 2939 \r\nQ 1816 3084 2150 3084 \r\nQ 2853 3084 3261 2657 \r\nQ 3669 2231 3669 1497 \r\nQ 3669 778 3244 343 \r\nQ 2819 -91 2113 -91 \r\nQ 1303 -91 875 529 \r\nQ 447 1150 447 2328 \r\nQ 447 3434 972 4092 \r\nQ 1497 4750 2381 4750 \r\nQ 2619 4750 2861 4703 \r\nQ 3103 4656 3366 4563 \r\nz\r\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-36\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_6\">\r\n     <g id=\"line2d_13\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"42.828125\" xlink:href=\"#m4b50e86e3c\" y=\"87.325556\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_13\">\r\n      <!-- 0.865 -->\r\n      <g transform=\"translate(7.2 91.124774)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-36\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_7\">\r\n     <g id=\"line2d_14\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"42.828125\" xlink:href=\"#m4b50e86e3c\" y=\"61.883443\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_14\">\r\n      <!-- 0.870 -->\r\n      <g transform=\"translate(7.2 65.682662)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 525 4666 \r\nL 3525 4666 \r\nL 3525 4397 \r\nL 1831 0 \r\nL 1172 0 \r\nL 2766 4134 \r\nL 525 4134 \r\nL 525 4666 \r\nz\r\n\" id=\"DejaVuSans-37\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-37\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_8\">\r\n     <g id=\"line2d_15\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"42.828125\" xlink:href=\"#m4b50e86e3c\" y=\"36.441331\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_15\">\r\n      <!-- 0.875 -->\r\n      <g transform=\"translate(7.2 40.24055)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-37\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_9\">\r\n     <g id=\"line2d_16\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"42.828125\" xlink:href=\"#m4b50e86e3c\" y=\"10.999219\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_16\">\r\n      <!-- 0.880 -->\r\n      <g transform=\"translate(7.2 14.798437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"222.65625\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_17\">\r\n    <path clip-path=\"url(#pf305724530)\" d=\"M 58.046307 38.893348 \r\nL 68.191761 73.223668 \r\nL 78.337216 114.103357 \r\nL 88.48267 122.802389 \r\nL 98.628125 160.41684 \r\nL 108.77358 184.779829 \r\nL 118.919034 173.893723 \r\nL 129.064489 189.4677 \r\nL 139.209943 197.446648 \r\nL 149.355398 184.978173 \r\nL 159.500852 183.058197 \r\nL 169.646307 190.596485 \r\nL 179.791761 193.132627 \r\nL 189.937216 179.306706 \r\nL 200.08267 197.760181 \r\nL 210.228125 193.427176 \r\nL 220.37358 197.864509 \r\nL 230.519034 191.817242 \r\nL 240.664489 194.468688 \r\nL 250.809943 197.463462 \r\nL 260.955398 191.134698 \r\nL 271.100852 203.070507 \r\nL 281.246307 208.247406 \r\nL 291.391761 207.333666 \r\nL 301.537216 201.730304 \r\nL 311.68267 202.001807 \r\nL 321.828125 193.53275 \r\nL 331.97358 200.823402 \r\nL 342.119034 211.948629 \r\nL 352.264489 216.183108 \r\nL 362.409943 209.408138 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_18\">\r\n    <path clip-path=\"url(#pf305724530)\" d=\"M 58.046307 60.685381 \r\nL 68.191761 50.077368 \r\nL 78.337216 57.622731 \r\nL 88.48267 18.51038 \r\nL 98.628125 116.988392 \r\nL 108.77358 109.436429 \r\nL 118.919034 148.50578 \r\nL 129.064489 188.475878 \r\nL 139.209943 169.226652 \r\nL 149.355398 150.2411 \r\nL 159.500852 155.253626 \r\nL 169.646307 155.481542 \r\nL 179.791761 168.525542 \r\nL 189.937216 183.800845 \r\nL 200.08267 180.609241 \r\nL 210.228125 190.851975 \r\nL 220.37358 184.179888 \r\nL 230.519034 112.482654 \r\nL 240.664489 189.39738 \r\nL 250.809943 147.556966 \r\nL 260.955398 202.9013 \r\nL 271.100852 188.349827 \r\nL 281.246307 178.726594 \r\nL 291.391761 164.003627 \r\nL 301.537216 177.435074 \r\nL 311.68267 191.373284 \r\nL 321.828125 193.415777 \r\nL 331.97358 176.872497 \r\nL 342.119034 198.45715 \r\nL 352.264489 201.897771 \r\nL 362.409943 197.552442 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 42.828125 226.066744 \r\nL 42.828125 8.626744 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 377.628125 226.066744 \r\nL 377.628125 8.626744 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 42.828125 226.066744 \r\nL 377.628125 226.066744 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 42.828125 8.626744 \r\nL 377.628125 8.626744 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pf305724530\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"42.828125\" y=\"8.626744\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "from pytorch_tabnet.multitask import TabNetMultiTaskClassifier\n",
    "\n",
    "clf = TabNetMultiTaskClassifier(\n",
    "    n_d=64, n_a=64, n_steps=5,\n",
    "    lambda_sparse=1e-4,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    scheduler_params = {\"gamma\": 0.95, \"step_size\": 20},\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    device_name='cuda'\n",
    ")\n",
    "clf.fit(\n",
    "    X_train = X_train.values, y_train = np.array(Y_train).reshape(Y_train.shape[0],1),\n",
    "    eval_set = [(X_val.values, np.array(Y_val).reshape(Y_val.shape[0],1))],     #validation set 지정\n",
    "    max_epochs=1000,\n",
    "    patience=30,        # ~20 for early stopping\n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=128,\n",
    "    num_workers=1,\n",
    "    drop_last=False\n",
    ")\n",
    "plt.plot(clf.history['loss'][5:])\n",
    "plt.plot(clf.history['val_0_logloss'][5:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = clf.predict_proba(X_val.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = pd.DataFrame(np.reshape(np.array(predict), (6615,3)), columns=[0.0, 1.0, 2.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8414991948141832"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "log_loss(y_val_onehot, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.        , 0.13454969, 0.00372305, 0.00177471, 0.        ,\n",
       "       0.0024967 , 0.        , 0.00975541, 0.        , 0.35324094,\n",
       "       0.        , 0.        , 0.        , 0.011013  , 0.        ,\n",
       "       0.13670576, 0.34674074])"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "clf.feature_importances_"
   ]
  }
 ]
}