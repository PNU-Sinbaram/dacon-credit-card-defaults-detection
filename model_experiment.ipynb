{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd0bed0ea09e0a217f0cd8c3af8b97b5ce48feb5846fec0e29c23d707f4dd7f9787",
   "display_name": "Python 3.8.3 64-bit ('mlenv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 0) 데이터 전처리\n",
    "* Baseline코드에서 사용했던 전처리 방식을 그대로 사용\n",
    "* index column은 학습에 관련이 없으니 제거"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "train = pd.read_csv(\"../data/train.csv\")                            #r*c = 20000*20, test와 달리 credit이라는 column을 갖고 있고, 이 값을 예측\n",
    "test = pd.read_csv(\"../data/test.csv\")                              #10000*19. train으로 학습시키고 test데이터를 입력으로 넣어서 credit을 예측\n",
    "sample_submission = pd.read_csv(\"../data/sample_submission.csv\")    #예측값은 sample_submission과 형태가 같아야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train, test], axis=0)    # train데이터 밑에 test데이터가 붙음. test에는 credit이 없으므로, 결측치(NaN)형태로 저장됨\n",
    "# 실제로는 결측치를 완전히 날리는건 좋지 않지만, 1회 출제용으로 사용하기때문에 완전히 날림\n",
    "data = data.drop(\"occyp_type\", axis=1) # occyp_type column을 지움. axis : occyp_type이 row에 있는지 column에 있는지 알려줌. 1이면 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_len = data.apply(lambda x : len(x.unique()))  # 각 column별로 unique()를 수행하여, 그 길이를 반환. 즉, 모든 column의 요소의 개수를 출력\n",
    "group_1 = unique_len[unique_len <= 2].index   # 요소의 값이 2개 이하인 column들의 이름을 추출\n",
    "group_2 = unique_len[(unique_len > 2) & (unique_len <= 10)].index\n",
    "group_3 = unique_len[(unique_len > 10)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['gender'] = data['gender'].replace(['F','M'], [0, 1])   # F를 0으로, M을 1로 교체\n",
    "data['car'] = data['car'].replace(['N', 'Y'], [0, 1])\n",
    "data['reality'] = data['reality'].replace(['N', 'Y'], [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['child_num']>2, 'child_num'] = 2  # child_num>2인 child_num column을 가져와서 2로 바꿈\n",
    "data[group_2].apply(lambda x : len(x.unique()))\n",
    "label_encoder = preprocessing.LabelEncoder() # categorical 변수(문자로 되어있는 변수)들을 숫자로 인코딩해주는 함수\n",
    "set(label_encoder.fit_transform(data['income_type'])) # income_type column에서 각 요소들을 숫자로 바꿔줌. fit_transform이 배열을 반환해서 unique()대신 set을 사용\n",
    "data['income_type'] = label_encoder.fit_transform(data['income_type'])\n",
    "data['edu_type'] = label_encoder.fit_transform(data['edu_type'])\n",
    "data['family_type'] = label_encoder.fit_transform(data['family_type'])\n",
    "data['house_type'] = label_encoder.fit_transform(data['house_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, bin_dividers = np.histogram(data['income_total'], bins=7) # 연속형 변수를 입력으로 받아 몇 개의 구간으로 나눌지 설정해줌. 각 구간들의 분절점(나누는 기준) 및 구간별 요소의 개수를 출력해줌\n",
    "data['income_total'] = pd.factorize(pd.cut(data['income_total'], bins = bin_dividers, include_lowest=True, labels=[0,1,2,3,4,5,6]))[0] # pd.cut의 반환 데이터 타입은 category이기 때문에, series타입(int형 배열)으로 바꿔주는 작업을 거쳐야 함\n",
    "#위의 과정을 함수로 만듬\n",
    "def make_bin(array, N):\n",
    "    array = -array      #DAYS_BIRTH등의 column은 음수이기 때문에 양수로 바꿔줌\n",
    "    _, bin_dividers = np.histogram(array, bins = N)       # 여기선 counts 변수를 사용하지 않을 것이기 때문에 사용하지 않는다는 의미로 _로 설정.\n",
    "    cut_categories = pd.cut(array, bin_dividers, labels = [i for i in range(N)], include_lowest=True)\n",
    "    bined_array = pd.factorize(cut_categories)[0]\n",
    "    return bined_array\n",
    "data['DAYS_BIRTH'] = make_bin(data['DAYS_BIRTH'], 10)\n",
    "data['DAYS_EMPLOYED'] = make_bin(data['DAYS_EMPLOYED'], 10)\n",
    "data['begin_month'] = make_bin(data['begin_month'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop('index', axis=1)\n",
    "train = data[:-10000]   # train에 해당하는 값\n",
    "test = data[-10000:]   #test에 해당하는 값\n",
    "train_x = train.drop(\"credit\", axis = 1) # credit은 출력값이고, credit을 제외한 값들이 모델의 입력값이므로 column들 중(axis =1) credit을 찾아 없앰.\n",
    "train_y = train['credit']               # 모델의 출력이 credit\n",
    "test_x = test.drop(\"credit\", axis=1)        # data라는 dataframe을 만들면서 test set에 없던 credit이라는 column이 생겼으므로, 이를 다시 제거"
   ]
  },
  {
   "source": [
    "## 1) RandomForestClassifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, stratify=train_y, test_size=0.25, random_state=10086)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.466047343279552\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, Y_train)\n",
    "predict = clf.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "logloss = log_loss(y_val_onehot, predict)\n",
    "print(logloss)"
   ]
  },
  {
   "source": [
    "### 1_1) 하이퍼파라미터 조정"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_param_grid = {\n",
    "    'n_estimators' : [100, 200],\n",
    "    'max_depth' : [8, 10, 12],\n",
    "    'min_samples_leaf' : [3, 5, 7],\n",
    "    'min_samples_split' : [3, 4, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "0.8262986236456679\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, stratify=train_y, test_size=0.25, random_state=10086)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf_grid = GridSearchCV(clf, param_grid=rf_param_grid, scoring=\"accuracy\", n_jobs=-1, verbose=2)\n",
    "clf_grid.fit(X_train, Y_train)\n",
    "\n",
    "predict = clf_grid.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "logloss=log_loss(y_val_onehot, predict)\n",
    "print(logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[0.8282641291283019, 0.8225467954824329, 0.8253606275571261, 0.8224319510375313, 0.8201274143462735]\n"
     ]
    }
   ],
   "source": [
    "folds = StratifiedKFold(n_splits = 5, shuffle=True, random_state = 55)  # 5개의 fold로 나눔\n",
    "\n",
    "outcomes = []\n",
    "for n_fold, (train_index, val_index) in enumerate(folds.split(train_x, train_y)):\n",
    "    X_train, X_val = train_x.iloc[train_index], train_x.iloc[val_index]\n",
    "    Y_train, Y_val = train_y.iloc[train_index], train_y.iloc[val_index]\n",
    "    clf = RandomForestClassifier()\n",
    "    clf_grid = GridSearchCV(clf, param_grid=rf_param_grid, scoring=\"accuracy\", n_jobs=-1, verbose=2)\n",
    "    clf_grid.fit(X_train, Y_train)\n",
    "    predictions = clf_grid.predict_proba(X_val)\n",
    "    y_val_onehot = pd.get_dummies(Y_val)\n",
    "\n",
    "    logloss = log_loss(y_val_onehot, predictions)\n",
    "    outcomes.append(logloss)\n",
    "\n",
    "print(outcomes)"
   ]
  },
  {
   "source": [
    "## 2) Gradient Boosting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8335746908166416\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, stratify=train_y, test_size=0.25, random_state=10086)\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(X_train, Y_train)\n",
    "predict = gb.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "logloss = log_loss(y_val_onehot, predict)\n",
    "print(logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.8417415560977669, 0.8341085098795408, 0.8386208102239824, 0.8392900501671904, 0.8335760935299279]\n"
     ]
    }
   ],
   "source": [
    "folds = StratifiedKFold(n_splits = 5, shuffle=True, random_state = 55)  # 5개의 fold로 나눔\n",
    "\n",
    "outcomes = []\n",
    "for n_fold, (train_index, val_index) in enumerate(folds.split(train_x, train_y)):\n",
    "    X_train, X_val = train_x.iloc[train_index], train_x.iloc[val_index]\n",
    "    Y_train, Y_val = train_y.iloc[train_index], train_y.iloc[val_index]\n",
    "    gb = GradientBoostingClassifier()\n",
    "    gb.fit(X_train, Y_train)\n",
    "    predictions = gb.predict_proba(X_val)\n",
    "    y_val_onehot = pd.get_dummies(Y_val)\n",
    "\n",
    "    logloss = log_loss(y_val_onehot, predictions)\n",
    "    outcomes.append(logloss)\n",
    "print(outcomes)"
   ]
  },
  {
   "source": [
    "### 2_1) Gradient Boosting parameter 조정\n",
    "* **주의. 한번 실행에 시간 소요가 너무 김**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_param_grid = {\n",
    "    'n_estimators' : [100, 200],\n",
    "    'max_depth' : [8, 10, 12],\n",
    "    'min_samples_leaf' : [5, 7, 10],\n",
    "    'min_samples_split' : [2, 3, 5],\n",
    "    'learning_rate' : [0.05, 0.1, 0.2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-4a78ca480250>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mgb_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgb_param_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mgb_grid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgb_grid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0my_val_onehot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pythonenv\\mlenv\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pythonenv\\mlenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    839\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    842\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pythonenv\\mlenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1294\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1295\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1296\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pythonenv\\mlenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    793\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[0;32m    796\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pythonenv\\mlenv\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1054\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1055\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pythonenv\\mlenv\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    931\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pythonenv\\mlenv\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    432\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "gb_grid = GridSearchCV(gb, param_grid = gb_param_grid, scoring=\"accuracy\", n_jobs=-1, verbose=3)\n",
    "gb_grid.fit(X_train, Y_train)\n",
    "predict = gb_grid.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "logloss = log_loss(y_val_onehot, predict)\n",
    "print(logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'learning_rate': 0.05,\n",
       " 'max_depth': 8,\n",
       " 'min_samples_leaf': 7,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 100}"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "gb_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.8299092537458699, 0.8195784379737493, 0.822702511343288, 0.8219319425895892, 0.8183218626777928]\n"
     ]
    }
   ],
   "source": [
    "folds = StratifiedKFold(n_splits = 5, shuffle=True, random_state = 55)  # 5개의 fold로 나눔\n",
    "\n",
    "outcomes = []\n",
    "for n_fold, (train_index, val_index) in enumerate(folds.split(train_x, train_y)):\n",
    "    X_train, X_val = train_x.iloc[train_index], train_x.iloc[val_index]\n",
    "    Y_train, Y_val = train_y.iloc[train_index], train_y.iloc[val_index]\n",
    "    gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.05, max_depth=8, min_samples_leaf=7, min_samples_split=2)\n",
    "    gb.fit(X_train, Y_train)\n",
    "    predict = gb.predict_proba(X_val)\n",
    "    y_val_onehot = pd.get_dummies(Y_val)\n",
    "    logloss = log_loss(y_val_onehot, predict)\n",
    "    outcomes.append(logloss)\n",
    "print(outcomes)"
   ]
  },
  {
   "source": [
    "## 3) AdaBoost"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.0877788002340585"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, stratify=train_y, test_size=0.25, random_state=10086)\n",
    "\n",
    "ag = AdaBoostClassifier()\n",
    "ag.fit(X_train, Y_train)\n",
    "\n",
    "predict = ag.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "log_loss(y_val_onehot, predict)"
   ]
  },
  {
   "source": [
    "## 4) XgBoost"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[20:56:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "[20:56:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "[20:56:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "[20:56:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "[20:56:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.8324195141525684, 0.8266500276527476, 0.8265990901853171, 0.8235172349839919, 0.8234311175080091]\n",
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "outcomes = []\n",
    "for n_fold, (train_index, val_index) in enumerate(folds.split(train_x, train_y)):\n",
    "    X_train, X_val = train_x.iloc[train_index], train_x.iloc[val_index]\n",
    "    Y_train, Y_val = train_y.iloc[train_index], train_y.iloc[val_index]\n",
    "    xgb = XGBClassifier(n_estimators=500, learning_rate=0.1, max_depth = 4, use_label_encoder=False)\n",
    "    xgb.fit(X_train, Y_train)\n",
    "    predictions = xgb.predict_proba(X_val)\n",
    "    y_val_onehot = pd.get_dummies(Y_val)\n",
    "\n",
    "    logloss = log_loss(y_val_onehot, predictions)\n",
    "    outcomes.append(logloss)\n",
    "print(outcomes)"
   ]
  },
  {
   "source": [
    "### 4_1) XgBoost hyperparameter tuning\n",
    "* **주의. 한번 실행에 시간 소요가 너무 김**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "xgb_param_grid = {\n",
    "    'n_estimators' : [300, 500, 600],\n",
    "    'learning_rate' : [0.01, 0.05, 0.1, 0.15],\n",
    "    'max_depth' : [3, 4, 6, 8]\n",
    "}\n",
    "\n",
    "xgb_grid = GridSearchCV(xgb, param_grid = xgb_param_grid, scoring=\"accuracy\", n_jobs= -1, verbose = 3)\n",
    "xgb_grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "실행 시간이 너무 오래 걸려 전에 돌려놨던 결과로 대체함\n",
    "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
    "c:\\Users\\lijm1\\Desktop\\pythonenv\\mlenv\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
    "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
    "[01:20:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
    "GridSearchCV(estimator=XGBClassifier(base_score=None, booster=None,\n",
    "                                     colsample_bylevel=None,\n",
    "                                     colsample_bynode=None,\n",
    "                                     colsample_bytree=None, gamma=None,\n",
    "                                     gpu_id=None, importance_type='gain',\n",
    "                                     interaction_constraints=None,\n",
    "                                     learning_rate=None, max_delta_step=None,\n",
    "                                     max_depth=None, min_child_weight=None,\n",
    "                                     missing=nan, monotone_constraints=None,\n",
    "                                     n_estimators=100, n_jobs=None,\n",
    "                                     num_parallel_tree=None, random_state=None,\n",
    "                                     reg_alpha=None, reg_lambda=None,\n",
    "                                     scale_pos_weight=None, subsample=None,\n",
    "                                     tree_method=None, validate_parameters=None,\n",
    "                                     verbosity=None),\n",
    "             n_jobs=-1,\n",
    "             param_grid={'learning_rate': [0.01], 'max_depth': [4],\n",
    "                         'n_estimators': [500]},\n",
    "             scoring='accuracy', verbose=3)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[20:57:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8359875737021389"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, stratify=train_y, test_size=0.25, random_state=10086)\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=500, learning_rate=0.01, max_depth = 4, use_label_encoder=False)\n",
    "xgb.fit(X_train, Y_train)\n",
    "predictions = xgb.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "\n",
    "log_loss(y_val_onehot, predictions)"
   ]
  },
  {
   "source": [
    "## 5) LightGBM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8248750259707588"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier, plot_importance\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, stratify=train_y, test_size=0.25, random_state=10086)\n",
    "\n",
    "lgb = LGBMClassifier(n_estimators=400)\n",
    "lgb.fit(X_train, Y_train)\n",
    "predict = lgb.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "log_loss(y_val_onehot, predict)"
   ]
  },
  {
   "source": [
    "### 5_1) LGBM Early stopping 적용"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1]\tvalid_0's multi_logloss: 0.871384\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tvalid_0's multi_logloss: 0.863215\n",
      "[3]\tvalid_0's multi_logloss: 0.857059\n",
      "[4]\tvalid_0's multi_logloss: 0.852325\n",
      "[5]\tvalid_0's multi_logloss: 0.848258\n",
      "[6]\tvalid_0's multi_logloss: 0.84532\n",
      "[7]\tvalid_0's multi_logloss: 0.842952\n",
      "[8]\tvalid_0's multi_logloss: 0.840694\n",
      "[9]\tvalid_0's multi_logloss: 0.83901\n",
      "[10]\tvalid_0's multi_logloss: 0.837603\n",
      "[11]\tvalid_0's multi_logloss: 0.836505\n",
      "[12]\tvalid_0's multi_logloss: 0.835527\n",
      "[13]\tvalid_0's multi_logloss: 0.834671\n",
      "[14]\tvalid_0's multi_logloss: 0.833793\n",
      "[15]\tvalid_0's multi_logloss: 0.833183\n",
      "[16]\tvalid_0's multi_logloss: 0.83234\n",
      "[17]\tvalid_0's multi_logloss: 0.831761\n",
      "[18]\tvalid_0's multi_logloss: 0.831425\n",
      "[19]\tvalid_0's multi_logloss: 0.830885\n",
      "[20]\tvalid_0's multi_logloss: 0.830542\n",
      "[21]\tvalid_0's multi_logloss: 0.830351\n",
      "[22]\tvalid_0's multi_logloss: 0.830166\n",
      "[23]\tvalid_0's multi_logloss: 0.830036\n",
      "[24]\tvalid_0's multi_logloss: 0.829671\n",
      "[25]\tvalid_0's multi_logloss: 0.829519\n",
      "[26]\tvalid_0's multi_logloss: 0.829229\n",
      "[27]\tvalid_0's multi_logloss: 0.829262\n",
      "[28]\tvalid_0's multi_logloss: 0.829037\n",
      "[29]\tvalid_0's multi_logloss: 0.828849\n",
      "[30]\tvalid_0's multi_logloss: 0.828643\n",
      "[31]\tvalid_0's multi_logloss: 0.828665\n",
      "[32]\tvalid_0's multi_logloss: 0.828359\n",
      "[33]\tvalid_0's multi_logloss: 0.828262\n",
      "[34]\tvalid_0's multi_logloss: 0.827804\n",
      "[35]\tvalid_0's multi_logloss: 0.827619\n",
      "[36]\tvalid_0's multi_logloss: 0.827389\n",
      "[37]\tvalid_0's multi_logloss: 0.827356\n",
      "[38]\tvalid_0's multi_logloss: 0.827293\n",
      "[39]\tvalid_0's multi_logloss: 0.826958\n",
      "[40]\tvalid_0's multi_logloss: 0.826816\n",
      "[41]\tvalid_0's multi_logloss: 0.826786\n",
      "[42]\tvalid_0's multi_logloss: 0.826573\n",
      "[43]\tvalid_0's multi_logloss: 0.82634\n",
      "[44]\tvalid_0's multi_logloss: 0.826291\n",
      "[45]\tvalid_0's multi_logloss: 0.826135\n",
      "[46]\tvalid_0's multi_logloss: 0.826101\n",
      "[47]\tvalid_0's multi_logloss: 0.825966\n",
      "[48]\tvalid_0's multi_logloss: 0.825712\n",
      "[49]\tvalid_0's multi_logloss: 0.825559\n",
      "[50]\tvalid_0's multi_logloss: 0.825187\n",
      "[51]\tvalid_0's multi_logloss: 0.825019\n",
      "[52]\tvalid_0's multi_logloss: 0.825123\n",
      "[53]\tvalid_0's multi_logloss: 0.825115\n",
      "[54]\tvalid_0's multi_logloss: 0.824682\n",
      "[55]\tvalid_0's multi_logloss: 0.824667\n",
      "[56]\tvalid_0's multi_logloss: 0.824501\n",
      "[57]\tvalid_0's multi_logloss: 0.824404\n",
      "[58]\tvalid_0's multi_logloss: 0.824241\n",
      "[59]\tvalid_0's multi_logloss: 0.824274\n",
      "[60]\tvalid_0's multi_logloss: 0.824144\n",
      "[61]\tvalid_0's multi_logloss: 0.823991\n",
      "[62]\tvalid_0's multi_logloss: 0.823945\n",
      "[63]\tvalid_0's multi_logloss: 0.823793\n",
      "[64]\tvalid_0's multi_logloss: 0.82363\n",
      "[65]\tvalid_0's multi_logloss: 0.823577\n",
      "[66]\tvalid_0's multi_logloss: 0.823364\n",
      "[67]\tvalid_0's multi_logloss: 0.823236\n",
      "[68]\tvalid_0's multi_logloss: 0.823105\n",
      "[69]\tvalid_0's multi_logloss: 0.823008\n",
      "[70]\tvalid_0's multi_logloss: 0.823012\n",
      "[71]\tvalid_0's multi_logloss: 0.823018\n",
      "[72]\tvalid_0's multi_logloss: 0.822906\n",
      "[73]\tvalid_0's multi_logloss: 0.822797\n",
      "[74]\tvalid_0's multi_logloss: 0.822914\n",
      "[75]\tvalid_0's multi_logloss: 0.822859\n",
      "[76]\tvalid_0's multi_logloss: 0.822457\n",
      "[77]\tvalid_0's multi_logloss: 0.822447\n",
      "[78]\tvalid_0's multi_logloss: 0.822259\n",
      "[79]\tvalid_0's multi_logloss: 0.822138\n",
      "[80]\tvalid_0's multi_logloss: 0.822083\n",
      "[81]\tvalid_0's multi_logloss: 0.822013\n",
      "[82]\tvalid_0's multi_logloss: 0.822043\n",
      "[83]\tvalid_0's multi_logloss: 0.822145\n",
      "[84]\tvalid_0's multi_logloss: 0.822083\n",
      "[85]\tvalid_0's multi_logloss: 0.821986\n",
      "[86]\tvalid_0's multi_logloss: 0.821953\n",
      "[87]\tvalid_0's multi_logloss: 0.821722\n",
      "[88]\tvalid_0's multi_logloss: 0.821543\n",
      "[89]\tvalid_0's multi_logloss: 0.821408\n",
      "[90]\tvalid_0's multi_logloss: 0.821313\n",
      "[91]\tvalid_0's multi_logloss: 0.821416\n",
      "[92]\tvalid_0's multi_logloss: 0.820971\n",
      "[93]\tvalid_0's multi_logloss: 0.820791\n",
      "[94]\tvalid_0's multi_logloss: 0.820557\n",
      "[95]\tvalid_0's multi_logloss: 0.820226\n",
      "[96]\tvalid_0's multi_logloss: 0.820044\n",
      "[97]\tvalid_0's multi_logloss: 0.819827\n",
      "[98]\tvalid_0's multi_logloss: 0.819404\n",
      "[99]\tvalid_0's multi_logloss: 0.819393\n",
      "[100]\tvalid_0's multi_logloss: 0.819325\n",
      "[101]\tvalid_0's multi_logloss: 0.81921\n",
      "[102]\tvalid_0's multi_logloss: 0.819381\n",
      "[103]\tvalid_0's multi_logloss: 0.819383\n",
      "[104]\tvalid_0's multi_logloss: 0.819118\n",
      "[105]\tvalid_0's multi_logloss: 0.818937\n",
      "[106]\tvalid_0's multi_logloss: 0.818774\n",
      "[107]\tvalid_0's multi_logloss: 0.818807\n",
      "[108]\tvalid_0's multi_logloss: 0.818723\n",
      "[109]\tvalid_0's multi_logloss: 0.818886\n",
      "[110]\tvalid_0's multi_logloss: 0.818767\n",
      "[111]\tvalid_0's multi_logloss: 0.818832\n",
      "[112]\tvalid_0's multi_logloss: 0.818768\n",
      "[113]\tvalid_0's multi_logloss: 0.818561\n",
      "[114]\tvalid_0's multi_logloss: 0.818637\n",
      "[115]\tvalid_0's multi_logloss: 0.8185\n",
      "[116]\tvalid_0's multi_logloss: 0.818314\n",
      "[117]\tvalid_0's multi_logloss: 0.818235\n",
      "[118]\tvalid_0's multi_logloss: 0.818215\n",
      "[119]\tvalid_0's multi_logloss: 0.818015\n",
      "[120]\tvalid_0's multi_logloss: 0.818105\n",
      "[121]\tvalid_0's multi_logloss: 0.817966\n",
      "[122]\tvalid_0's multi_logloss: 0.817894\n",
      "[123]\tvalid_0's multi_logloss: 0.817587\n",
      "[124]\tvalid_0's multi_logloss: 0.817499\n",
      "[125]\tvalid_0's multi_logloss: 0.817386\n",
      "[126]\tvalid_0's multi_logloss: 0.817303\n",
      "[127]\tvalid_0's multi_logloss: 0.81739\n",
      "[128]\tvalid_0's multi_logloss: 0.817411\n",
      "[129]\tvalid_0's multi_logloss: 0.817387\n",
      "[130]\tvalid_0's multi_logloss: 0.817349\n",
      "[131]\tvalid_0's multi_logloss: 0.817435\n",
      "[132]\tvalid_0's multi_logloss: 0.817424\n",
      "[133]\tvalid_0's multi_logloss: 0.817413\n",
      "[134]\tvalid_0's multi_logloss: 0.817493\n",
      "[135]\tvalid_0's multi_logloss: 0.817377\n",
      "[136]\tvalid_0's multi_logloss: 0.81738\n",
      "[137]\tvalid_0's multi_logloss: 0.817454\n",
      "[138]\tvalid_0's multi_logloss: 0.817259\n",
      "[139]\tvalid_0's multi_logloss: 0.81714\n",
      "[140]\tvalid_0's multi_logloss: 0.816845\n",
      "[141]\tvalid_0's multi_logloss: 0.816691\n",
      "[142]\tvalid_0's multi_logloss: 0.816643\n",
      "[143]\tvalid_0's multi_logloss: 0.816532\n",
      "[144]\tvalid_0's multi_logloss: 0.81637\n",
      "[145]\tvalid_0's multi_logloss: 0.81613\n",
      "[146]\tvalid_0's multi_logloss: 0.815985\n",
      "[147]\tvalid_0's multi_logloss: 0.816012\n",
      "[148]\tvalid_0's multi_logloss: 0.816032\n",
      "[149]\tvalid_0's multi_logloss: 0.81607\n",
      "[150]\tvalid_0's multi_logloss: 0.815985\n",
      "[151]\tvalid_0's multi_logloss: 0.816081\n",
      "[152]\tvalid_0's multi_logloss: 0.815991\n",
      "[153]\tvalid_0's multi_logloss: 0.815889\n",
      "[154]\tvalid_0's multi_logloss: 0.815904\n",
      "[155]\tvalid_0's multi_logloss: 0.815886\n",
      "[156]\tvalid_0's multi_logloss: 0.815752\n",
      "[157]\tvalid_0's multi_logloss: 0.815806\n",
      "[158]\tvalid_0's multi_logloss: 0.815742\n",
      "[159]\tvalid_0's multi_logloss: 0.815868\n",
      "[160]\tvalid_0's multi_logloss: 0.815914\n",
      "[161]\tvalid_0's multi_logloss: 0.815892\n",
      "[162]\tvalid_0's multi_logloss: 0.815838\n",
      "[163]\tvalid_0's multi_logloss: 0.815685\n",
      "[164]\tvalid_0's multi_logloss: 0.815754\n",
      "[165]\tvalid_0's multi_logloss: 0.815742\n",
      "[166]\tvalid_0's multi_logloss: 0.815818\n",
      "[167]\tvalid_0's multi_logloss: 0.815877\n",
      "[168]\tvalid_0's multi_logloss: 0.815856\n",
      "[169]\tvalid_0's multi_logloss: 0.816004\n",
      "[170]\tvalid_0's multi_logloss: 0.816022\n",
      "[171]\tvalid_0's multi_logloss: 0.816048\n",
      "[172]\tvalid_0's multi_logloss: 0.8161\n",
      "[173]\tvalid_0's multi_logloss: 0.816062\n",
      "[174]\tvalid_0's multi_logloss: 0.816022\n",
      "[175]\tvalid_0's multi_logloss: 0.815977\n",
      "[176]\tvalid_0's multi_logloss: 0.81602\n",
      "[177]\tvalid_0's multi_logloss: 0.816042\n",
      "[178]\tvalid_0's multi_logloss: 0.816101\n",
      "[179]\tvalid_0's multi_logloss: 0.816191\n",
      "[180]\tvalid_0's multi_logloss: 0.816228\n",
      "[181]\tvalid_0's multi_logloss: 0.8162\n",
      "[182]\tvalid_0's multi_logloss: 0.816205\n",
      "[183]\tvalid_0's multi_logloss: 0.816312\n",
      "[184]\tvalid_0's multi_logloss: 0.816287\n",
      "[185]\tvalid_0's multi_logloss: 0.816403\n",
      "[186]\tvalid_0's multi_logloss: 0.816519\n",
      "[187]\tvalid_0's multi_logloss: 0.816444\n",
      "[188]\tvalid_0's multi_logloss: 0.816421\n",
      "[189]\tvalid_0's multi_logloss: 0.816298\n",
      "[190]\tvalid_0's multi_logloss: 0.816265\n",
      "[191]\tvalid_0's multi_logloss: 0.816343\n",
      "[192]\tvalid_0's multi_logloss: 0.816423\n",
      "[193]\tvalid_0's multi_logloss: 0.816623\n",
      "[194]\tvalid_0's multi_logloss: 0.81658\n",
      "[195]\tvalid_0's multi_logloss: 0.816574\n",
      "[196]\tvalid_0's multi_logloss: 0.816605\n",
      "[197]\tvalid_0's multi_logloss: 0.816762\n",
      "[198]\tvalid_0's multi_logloss: 0.816645\n",
      "[199]\tvalid_0's multi_logloss: 0.816797\n",
      "[200]\tvalid_0's multi_logloss: 0.816738\n",
      "[201]\tvalid_0's multi_logloss: 0.816695\n",
      "[202]\tvalid_0's multi_logloss: 0.816749\n",
      "[203]\tvalid_0's multi_logloss: 0.816764\n",
      "[204]\tvalid_0's multi_logloss: 0.816815\n",
      "[205]\tvalid_0's multi_logloss: 0.816856\n",
      "[206]\tvalid_0's multi_logloss: 0.816949\n",
      "[207]\tvalid_0's multi_logloss: 0.816955\n",
      "[208]\tvalid_0's multi_logloss: 0.816852\n",
      "[209]\tvalid_0's multi_logloss: 0.816964\n",
      "[210]\tvalid_0's multi_logloss: 0.817072\n",
      "[211]\tvalid_0's multi_logloss: 0.817013\n",
      "[212]\tvalid_0's multi_logloss: 0.817144\n",
      "[213]\tvalid_0's multi_logloss: 0.817175\n",
      "[214]\tvalid_0's multi_logloss: 0.817195\n",
      "[215]\tvalid_0's multi_logloss: 0.817245\n",
      "[216]\tvalid_0's multi_logloss: 0.817418\n",
      "[217]\tvalid_0's multi_logloss: 0.817516\n",
      "[218]\tvalid_0's multi_logloss: 0.817628\n",
      "[219]\tvalid_0's multi_logloss: 0.817768\n",
      "[220]\tvalid_0's multi_logloss: 0.817828\n",
      "[221]\tvalid_0's multi_logloss: 0.81796\n",
      "[222]\tvalid_0's multi_logloss: 0.818176\n",
      "[223]\tvalid_0's multi_logloss: 0.8182\n",
      "[224]\tvalid_0's multi_logloss: 0.818271\n",
      "[225]\tvalid_0's multi_logloss: 0.818258\n",
      "[226]\tvalid_0's multi_logloss: 0.818392\n",
      "[227]\tvalid_0's multi_logloss: 0.818549\n",
      "[228]\tvalid_0's multi_logloss: 0.818574\n",
      "[229]\tvalid_0's multi_logloss: 0.818664\n",
      "[230]\tvalid_0's multi_logloss: 0.818693\n",
      "[231]\tvalid_0's multi_logloss: 0.818632\n",
      "[232]\tvalid_0's multi_logloss: 0.818626\n",
      "[233]\tvalid_0's multi_logloss: 0.818638\n",
      "[234]\tvalid_0's multi_logloss: 0.818587\n",
      "[235]\tvalid_0's multi_logloss: 0.818552\n",
      "[236]\tvalid_0's multi_logloss: 0.818431\n",
      "[237]\tvalid_0's multi_logloss: 0.818532\n",
      "[238]\tvalid_0's multi_logloss: 0.818426\n",
      "[239]\tvalid_0's multi_logloss: 0.818505\n",
      "[240]\tvalid_0's multi_logloss: 0.818419\n",
      "[241]\tvalid_0's multi_logloss: 0.81839\n",
      "[242]\tvalid_0's multi_logloss: 0.818433\n",
      "[243]\tvalid_0's multi_logloss: 0.818293\n",
      "[244]\tvalid_0's multi_logloss: 0.818186\n",
      "[245]\tvalid_0's multi_logloss: 0.818149\n",
      "[246]\tvalid_0's multi_logloss: 0.818069\n",
      "[247]\tvalid_0's multi_logloss: 0.81819\n",
      "[248]\tvalid_0's multi_logloss: 0.818154\n",
      "[249]\tvalid_0's multi_logloss: 0.818171\n",
      "[250]\tvalid_0's multi_logloss: 0.818223\n",
      "[251]\tvalid_0's multi_logloss: 0.818296\n",
      "[252]\tvalid_0's multi_logloss: 0.818262\n",
      "[253]\tvalid_0's multi_logloss: 0.818232\n",
      "[254]\tvalid_0's multi_logloss: 0.818197\n",
      "[255]\tvalid_0's multi_logloss: 0.818272\n",
      "[256]\tvalid_0's multi_logloss: 0.818405\n",
      "[257]\tvalid_0's multi_logloss: 0.818496\n",
      "[258]\tvalid_0's multi_logloss: 0.818531\n",
      "[259]\tvalid_0's multi_logloss: 0.818558\n",
      "[260]\tvalid_0's multi_logloss: 0.818767\n",
      "[261]\tvalid_0's multi_logloss: 0.81879\n",
      "[262]\tvalid_0's multi_logloss: 0.818777\n",
      "[263]\tvalid_0's multi_logloss: 0.818814\n",
      "Early stopping, best iteration is:\n",
      "[163]\tvalid_0's multi_logloss: 0.815685\n"
     ]
    }
   ],
   "source": [
    "lgb = LGBMClassifier(n_estimators=1000)\n",
    "evals = [(X_val, Y_val)]\n",
    "lgb.fit(X_train, Y_train, early_stopping_rounds=100, eval_metric=\"logloss\", eval_set=evals, verbose=True)\n",
    "predict = lgb.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "logloss = log_loss(y_val_onehot, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8156847808255132"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "logloss"
   ]
  },
  {
   "source": [
    "## 6) TabNet"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 10
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "source": [
    "* **주의. 한번 실행에 시간 소요가 너무 김(약 1시간)**\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.83675 |  0:24:53s\n",
      "epoch 460| loss: 0.83724 |  0:24:56s\n",
      "epoch 461| loss: 0.83746 |  0:24:59s\n",
      "epoch 462| loss: 0.83643 |  0:25:03s\n",
      "epoch 463| loss: 0.83691 |  0:25:06s\n",
      "epoch 464| loss: 0.83746 |  0:25:09s\n",
      "epoch 465| loss: 0.83726 |  0:25:12s\n",
      "epoch 466| loss: 0.83746 |  0:25:15s\n",
      "epoch 467| loss: 0.83669 |  0:25:18s\n",
      "epoch 468| loss: 0.83717 |  0:25:21s\n",
      "epoch 469| loss: 0.8374  |  0:25:24s\n",
      "epoch 470| loss: 0.83712 |  0:25:27s\n",
      "epoch 471| loss: 0.83664 |  0:25:30s\n",
      "epoch 472| loss: 0.83695 |  0:25:33s\n",
      "epoch 473| loss: 0.83711 |  0:25:36s\n",
      "epoch 474| loss: 0.83691 |  0:25:39s\n",
      "epoch 475| loss: 0.83709 |  0:25:43s\n",
      "epoch 476| loss: 0.83778 |  0:25:46s\n",
      "epoch 477| loss: 0.83707 |  0:25:49s\n",
      "epoch 478| loss: 0.83857 |  0:25:52s\n",
      "epoch 479| loss: 0.84094 |  0:25:55s\n",
      "epoch 480| loss: 0.84282 |  0:25:58s\n",
      "epoch 481| loss: 0.84093 |  0:26:01s\n",
      "epoch 482| loss: 0.83971 |  0:26:04s\n",
      "epoch 483| loss: 0.83841 |  0:26:07s\n",
      "epoch 484| loss: 0.83954 |  0:26:10s\n",
      "epoch 485| loss: 0.83983 |  0:26:13s\n",
      "epoch 486| loss: 0.83973 |  0:26:16s\n",
      "epoch 487| loss: 0.84165 |  0:26:19s\n",
      "epoch 488| loss: 0.84032 |  0:26:22s\n",
      "epoch 489| loss: 0.83958 |  0:26:26s\n",
      "epoch 490| loss: 0.83906 |  0:26:29s\n",
      "epoch 491| loss: 0.84032 |  0:26:32s\n",
      "epoch 492| loss: 0.84109 |  0:26:35s\n",
      "epoch 493| loss: 0.83995 |  0:26:38s\n",
      "epoch 494| loss: 0.83955 |  0:26:41s\n",
      "epoch 495| loss: 0.83941 |  0:26:44s\n",
      "epoch 496| loss: 0.83937 |  0:26:47s\n",
      "epoch 497| loss: 0.84033 |  0:26:50s\n",
      "epoch 498| loss: 0.84005 |  0:26:53s\n",
      "epoch 499| loss: 0.83949 |  0:26:56s\n",
      "epoch 500| loss: 0.83893 |  0:26:59s\n",
      "epoch 501| loss: 0.83889 |  0:27:02s\n",
      "epoch 502| loss: 0.83944 |  0:27:06s\n",
      "epoch 503| loss: 0.83887 |  0:27:09s\n",
      "epoch 504| loss: 0.83875 |  0:27:12s\n",
      "epoch 505| loss: 0.83889 |  0:27:15s\n",
      "epoch 506| loss: 0.83923 |  0:27:18s\n",
      "epoch 507| loss: 0.83843 |  0:27:21s\n",
      "epoch 508| loss: 0.8395  |  0:27:24s\n",
      "epoch 509| loss: 0.83889 |  0:27:27s\n",
      "epoch 510| loss: 0.83854 |  0:27:30s\n",
      "epoch 511| loss: 0.83973 |  0:27:33s\n",
      "epoch 512| loss: 0.83894 |  0:27:36s\n",
      "epoch 513| loss: 0.83895 |  0:27:39s\n",
      "epoch 514| loss: 0.83949 |  0:27:42s\n",
      "epoch 515| loss: 0.83881 |  0:27:46s\n",
      "epoch 516| loss: 0.83841 |  0:27:49s\n",
      "epoch 517| loss: 0.83875 |  0:27:52s\n",
      "epoch 518| loss: 0.83904 |  0:27:55s\n",
      "epoch 519| loss: 0.83873 |  0:27:58s\n",
      "epoch 520| loss: 0.83844 |  0:28:01s\n",
      "epoch 521| loss: 0.8389  |  0:28:04s\n",
      "epoch 522| loss: 0.839   |  0:28:07s\n",
      "epoch 523| loss: 0.83867 |  0:28:10s\n",
      "epoch 524| loss: 0.83844 |  0:28:13s\n",
      "epoch 525| loss: 0.83829 |  0:28:16s\n",
      "epoch 526| loss: 0.83844 |  0:28:19s\n",
      "epoch 527| loss: 0.83867 |  0:28:22s\n",
      "epoch 528| loss: 0.83847 |  0:28:26s\n",
      "epoch 529| loss: 0.83865 |  0:28:29s\n",
      "epoch 530| loss: 0.83957 |  0:28:32s\n",
      "epoch 531| loss: 0.83855 |  0:28:35s\n",
      "epoch 532| loss: 0.83868 |  0:28:38s\n",
      "epoch 533| loss: 0.83833 |  0:28:41s\n",
      "epoch 534| loss: 0.83824 |  0:28:44s\n",
      "epoch 535| loss: 0.83918 |  0:28:47s\n",
      "epoch 536| loss: 0.83958 |  0:28:50s\n",
      "epoch 537| loss: 0.84052 |  0:28:53s\n",
      "epoch 538| loss: 0.83994 |  0:28:56s\n",
      "epoch 539| loss: 0.83865 |  0:28:59s\n",
      "epoch 540| loss: 0.83861 |  0:29:02s\n",
      "epoch 541| loss: 0.83796 |  0:29:06s\n",
      "epoch 542| loss: 0.83765 |  0:29:09s\n",
      "epoch 543| loss: 0.838   |  0:29:12s\n",
      "epoch 544| loss: 0.83871 |  0:29:15s\n",
      "epoch 545| loss: 0.83849 |  0:29:18s\n",
      "epoch 546| loss: 0.83749 |  0:29:21s\n",
      "epoch 547| loss: 0.83733 |  0:29:24s\n",
      "epoch 548| loss: 0.83794 |  0:29:28s\n",
      "epoch 549| loss: 0.83873 |  0:29:31s\n",
      "epoch 550| loss: 0.83831 |  0:29:34s\n",
      "epoch 551| loss: 0.83765 |  0:29:37s\n",
      "epoch 552| loss: 0.83834 |  0:29:40s\n",
      "epoch 553| loss: 0.83888 |  0:29:43s\n",
      "epoch 554| loss: 0.8381  |  0:29:46s\n",
      "epoch 555| loss: 0.83836 |  0:29:49s\n",
      "epoch 556| loss: 0.83831 |  0:29:52s\n",
      "epoch 557| loss: 0.83758 |  0:29:55s\n",
      "epoch 558| loss: 0.83732 |  0:29:58s\n",
      "epoch 559| loss: 0.83766 |  0:30:01s\n",
      "epoch 560| loss: 0.83688 |  0:30:05s\n",
      "epoch 561| loss: 0.83674 |  0:30:08s\n",
      "epoch 562| loss: 0.83755 |  0:30:11s\n",
      "epoch 563| loss: 0.83725 |  0:30:14s\n",
      "epoch 564| loss: 0.83777 |  0:30:17s\n",
      "epoch 565| loss: 0.83842 |  0:30:20s\n",
      "epoch 566| loss: 0.83793 |  0:30:23s\n",
      "epoch 567| loss: 0.83697 |  0:30:26s\n",
      "epoch 568| loss: 0.8381  |  0:30:29s\n",
      "epoch 569| loss: 0.83837 |  0:30:32s\n",
      "epoch 570| loss: 0.83817 |  0:30:35s\n",
      "epoch 571| loss: 0.8377  |  0:30:38s\n",
      "epoch 572| loss: 0.83781 |  0:30:41s\n",
      "epoch 573| loss: 0.83828 |  0:30:44s\n",
      "epoch 574| loss: 0.83869 |  0:30:48s\n",
      "epoch 575| loss: 0.83819 |  0:30:51s\n",
      "epoch 576| loss: 0.83854 |  0:30:54s\n",
      "epoch 577| loss: 0.83827 |  0:30:57s\n",
      "epoch 578| loss: 0.83789 |  0:31:00s\n",
      "epoch 579| loss: 0.83797 |  0:31:03s\n",
      "epoch 580| loss: 0.83821 |  0:31:06s\n",
      "epoch 581| loss: 0.83835 |  0:31:09s\n",
      "epoch 582| loss: 0.83781 |  0:31:12s\n",
      "epoch 583| loss: 0.83836 |  0:31:15s\n",
      "epoch 584| loss: 0.83824 |  0:31:18s\n",
      "epoch 585| loss: 0.83877 |  0:31:22s\n",
      "epoch 586| loss: 0.83841 |  0:31:25s\n",
      "epoch 587| loss: 0.83828 |  0:31:28s\n",
      "epoch 588| loss: 0.83788 |  0:31:31s\n",
      "epoch 589| loss: 0.83727 |  0:31:34s\n",
      "epoch 590| loss: 0.83787 |  0:31:37s\n",
      "epoch 591| loss: 0.83786 |  0:31:40s\n",
      "epoch 592| loss: 0.83824 |  0:31:43s\n",
      "epoch 593| loss: 0.83832 |  0:31:46s\n",
      "epoch 594| loss: 0.83742 |  0:31:49s\n",
      "epoch 595| loss: 0.8373  |  0:31:52s\n",
      "epoch 596| loss: 0.8374  |  0:31:55s\n",
      "epoch 597| loss: 0.83724 |  0:31:58s\n",
      "epoch 598| loss: 0.8367  |  0:32:01s\n",
      "epoch 599| loss: 0.83696 |  0:32:05s\n",
      "epoch 600| loss: 0.83641 |  0:32:08s\n",
      "epoch 601| loss: 0.83705 |  0:32:11s\n",
      "epoch 602| loss: 0.83708 |  0:32:14s\n",
      "epoch 603| loss: 0.83723 |  0:32:17s\n",
      "epoch 604| loss: 0.83672 |  0:32:20s\n",
      "epoch 605| loss: 0.83647 |  0:32:23s\n",
      "epoch 606| loss: 0.83705 |  0:32:26s\n",
      "epoch 607| loss: 0.83722 |  0:32:29s\n",
      "epoch 608| loss: 0.83764 |  0:32:32s\n",
      "epoch 609| loss: 0.83703 |  0:32:35s\n",
      "epoch 610| loss: 0.83672 |  0:32:38s\n",
      "epoch 611| loss: 0.83785 |  0:32:42s\n",
      "epoch 612| loss: 0.83715 |  0:32:45s\n",
      "epoch 613| loss: 0.83787 |  0:32:48s\n",
      "epoch 614| loss: 0.83723 |  0:32:51s\n",
      "epoch 615| loss: 0.83735 |  0:32:54s\n",
      "epoch 616| loss: 0.83713 |  0:32:57s\n",
      "epoch 617| loss: 0.8375  |  0:33:00s\n",
      "epoch 618| loss: 0.83723 |  0:33:03s\n",
      "epoch 619| loss: 0.83715 |  0:33:06s\n",
      "epoch 620| loss: 0.83891 |  0:33:09s\n",
      "epoch 621| loss: 0.83772 |  0:33:12s\n",
      "epoch 622| loss: 0.83765 |  0:33:15s\n",
      "epoch 623| loss: 0.83824 |  0:33:19s\n",
      "epoch 624| loss: 0.83844 |  0:33:22s\n",
      "epoch 625| loss: 0.83788 |  0:33:25s\n",
      "epoch 626| loss: 0.83808 |  0:33:28s\n",
      "epoch 627| loss: 0.83766 |  0:33:31s\n",
      "epoch 628| loss: 0.83687 |  0:33:34s\n",
      "epoch 629| loss: 0.83741 |  0:33:37s\n",
      "epoch 630| loss: 0.83805 |  0:33:40s\n",
      "epoch 631| loss: 0.83732 |  0:33:43s\n",
      "epoch 632| loss: 0.83812 |  0:33:46s\n",
      "epoch 633| loss: 0.83758 |  0:33:49s\n",
      "epoch 634| loss: 0.83925 |  0:33:52s\n",
      "epoch 635| loss: 0.83882 |  0:33:55s\n",
      "epoch 636| loss: 0.83805 |  0:33:58s\n",
      "epoch 637| loss: 0.83825 |  0:34:02s\n",
      "epoch 638| loss: 0.83874 |  0:34:05s\n",
      "epoch 639| loss: 0.8392  |  0:34:08s\n",
      "epoch 640| loss: 0.8396  |  0:34:11s\n",
      "epoch 641| loss: 0.83847 |  0:34:14s\n",
      "epoch 642| loss: 0.83774 |  0:34:17s\n",
      "epoch 643| loss: 0.83812 |  0:34:20s\n",
      "epoch 644| loss: 0.83765 |  0:34:23s\n",
      "epoch 645| loss: 0.83763 |  0:34:26s\n",
      "epoch 646| loss: 0.83755 |  0:34:29s\n",
      "epoch 647| loss: 0.83861 |  0:34:32s\n",
      "epoch 648| loss: 0.83761 |  0:34:35s\n",
      "epoch 649| loss: 0.83796 |  0:34:38s\n",
      "epoch 650| loss: 0.83813 |  0:34:41s\n",
      "epoch 651| loss: 0.83768 |  0:34:44s\n",
      "epoch 652| loss: 0.83804 |  0:34:47s\n",
      "epoch 653| loss: 0.83821 |  0:34:51s\n",
      "epoch 654| loss: 0.83815 |  0:34:54s\n",
      "epoch 655| loss: 0.83774 |  0:34:57s\n",
      "epoch 656| loss: 0.83757 |  0:35:00s\n",
      "epoch 657| loss: 0.83805 |  0:35:03s\n",
      "epoch 658| loss: 0.83889 |  0:35:06s\n",
      "epoch 659| loss: 0.83841 |  0:35:09s\n",
      "epoch 660| loss: 0.83737 |  0:35:12s\n",
      "epoch 661| loss: 0.83788 |  0:35:15s\n",
      "epoch 662| loss: 0.83827 |  0:35:18s\n",
      "epoch 663| loss: 0.83828 |  0:35:21s\n",
      "epoch 664| loss: 0.83759 |  0:35:24s\n",
      "epoch 665| loss: 0.83796 |  0:35:27s\n",
      "epoch 666| loss: 0.83797 |  0:35:30s\n",
      "epoch 667| loss: 0.83728 |  0:35:34s\n",
      "epoch 668| loss: 0.83735 |  0:35:37s\n",
      "epoch 669| loss: 0.83756 |  0:35:40s\n",
      "epoch 670| loss: 0.83743 |  0:35:43s\n",
      "epoch 671| loss: 0.83816 |  0:35:46s\n",
      "epoch 672| loss: 0.83869 |  0:35:49s\n",
      "epoch 673| loss: 0.83766 |  0:35:52s\n",
      "epoch 674| loss: 0.83752 |  0:35:55s\n",
      "epoch 675| loss: 0.83771 |  0:35:58s\n",
      "epoch 676| loss: 0.83722 |  0:36:01s\n",
      "epoch 677| loss: 0.83772 |  0:36:04s\n",
      "epoch 678| loss: 0.83744 |  0:36:07s\n",
      "epoch 679| loss: 0.83791 |  0:36:11s\n",
      "epoch 680| loss: 0.83776 |  0:36:14s\n",
      "epoch 681| loss: 0.83795 |  0:36:17s\n",
      "epoch 682| loss: 0.83778 |  0:36:20s\n",
      "epoch 683| loss: 0.83878 |  0:36:23s\n",
      "epoch 684| loss: 0.83866 |  0:36:26s\n",
      "epoch 685| loss: 0.83766 |  0:36:29s\n",
      "epoch 686| loss: 0.83775 |  0:36:32s\n",
      "epoch 687| loss: 0.8375  |  0:36:35s\n",
      "epoch 688| loss: 0.83764 |  0:36:38s\n",
      "epoch 689| loss: 0.83773 |  0:36:41s\n",
      "epoch 690| loss: 0.83711 |  0:36:44s\n",
      "epoch 691| loss: 0.8375  |  0:36:47s\n",
      "epoch 692| loss: 0.83719 |  0:36:51s\n",
      "epoch 693| loss: 0.8373  |  0:36:54s\n",
      "epoch 694| loss: 0.83709 |  0:36:57s\n",
      "epoch 695| loss: 0.83774 |  0:37:00s\n",
      "epoch 696| loss: 0.83732 |  0:37:03s\n",
      "epoch 697| loss: 0.8368  |  0:37:06s\n",
      "epoch 698| loss: 0.83777 |  0:37:09s\n",
      "epoch 699| loss: 0.8371  |  0:37:12s\n",
      "epoch 700| loss: 0.83704 |  0:37:15s\n",
      "epoch 701| loss: 0.83814 |  0:37:18s\n",
      "epoch 702| loss: 0.83761 |  0:37:21s\n",
      "epoch 703| loss: 0.83772 |  0:37:24s\n",
      "epoch 704| loss: 0.83711 |  0:37:27s\n",
      "epoch 705| loss: 0.83749 |  0:37:31s\n",
      "epoch 706| loss: 0.8371  |  0:37:34s\n",
      "epoch 707| loss: 0.83728 |  0:37:37s\n",
      "epoch 708| loss: 0.83751 |  0:37:40s\n",
      "epoch 709| loss: 0.83748 |  0:37:43s\n",
      "epoch 710| loss: 0.83708 |  0:37:46s\n",
      "epoch 711| loss: 0.83716 |  0:37:49s\n",
      "epoch 712| loss: 0.83725 |  0:37:52s\n",
      "epoch 713| loss: 0.83763 |  0:37:56s\n",
      "epoch 714| loss: 0.83776 |  0:37:59s\n",
      "epoch 715| loss: 0.83749 |  0:38:02s\n",
      "epoch 716| loss: 0.83796 |  0:38:05s\n",
      "epoch 717| loss: 0.83734 |  0:38:08s\n",
      "epoch 718| loss: 0.83739 |  0:38:11s\n",
      "epoch 719| loss: 0.83767 |  0:38:14s\n",
      "epoch 720| loss: 0.83741 |  0:38:17s\n",
      "epoch 721| loss: 0.83746 |  0:38:20s\n",
      "epoch 722| loss: 0.83823 |  0:38:24s\n",
      "epoch 723| loss: 0.83799 |  0:38:27s\n",
      "epoch 724| loss: 0.83785 |  0:38:30s\n",
      "epoch 725| loss: 0.83773 |  0:38:33s\n",
      "epoch 726| loss: 0.8373  |  0:38:36s\n",
      "epoch 727| loss: 0.83678 |  0:38:39s\n",
      "epoch 728| loss: 0.83703 |  0:38:42s\n",
      "epoch 729| loss: 0.83665 |  0:38:46s\n",
      "epoch 730| loss: 0.83719 |  0:38:49s\n",
      "epoch 731| loss: 0.83753 |  0:38:52s\n",
      "epoch 732| loss: 0.83741 |  0:38:55s\n",
      "epoch 733| loss: 0.83812 |  0:38:58s\n",
      "epoch 734| loss: 0.8369  |  0:39:01s\n",
      "epoch 735| loss: 0.83718 |  0:39:04s\n",
      "epoch 736| loss: 0.83737 |  0:39:08s\n",
      "epoch 737| loss: 0.8373  |  0:39:11s\n",
      "epoch 738| loss: 0.83719 |  0:39:15s\n",
      "epoch 739| loss: 0.83693 |  0:39:18s\n",
      "epoch 740| loss: 0.83764 |  0:39:21s\n",
      "epoch 741| loss: 0.83657 |  0:39:24s\n",
      "epoch 742| loss: 0.83717 |  0:39:27s\n",
      "epoch 743| loss: 0.83763 |  0:39:30s\n",
      "epoch 744| loss: 0.83715 |  0:39:33s\n",
      "epoch 745| loss: 0.83703 |  0:39:36s\n",
      "epoch 746| loss: 0.83699 |  0:39:39s\n",
      "epoch 747| loss: 0.8376  |  0:39:43s\n",
      "epoch 748| loss: 0.83705 |  0:39:46s\n",
      "epoch 749| loss: 0.8376  |  0:39:49s\n",
      "epoch 750| loss: 0.83707 |  0:39:52s\n",
      "epoch 751| loss: 0.83684 |  0:39:55s\n",
      "epoch 752| loss: 0.8371  |  0:39:58s\n",
      "epoch 753| loss: 0.83645 |  0:40:01s\n",
      "epoch 754| loss: 0.83765 |  0:40:04s\n",
      "epoch 755| loss: 0.83733 |  0:40:07s\n",
      "epoch 756| loss: 0.838   |  0:40:10s\n",
      "epoch 757| loss: 0.83672 |  0:40:13s\n",
      "epoch 758| loss: 0.8377  |  0:40:16s\n",
      "epoch 759| loss: 0.83707 |  0:40:19s\n",
      "epoch 760| loss: 0.8372  |  0:40:22s\n",
      "epoch 761| loss: 0.83674 |  0:40:25s\n",
      "epoch 762| loss: 0.83806 |  0:40:29s\n",
      "epoch 763| loss: 0.83745 |  0:40:32s\n",
      "epoch 764| loss: 0.8364  |  0:40:35s\n",
      "epoch 765| loss: 0.83684 |  0:40:38s\n",
      "epoch 766| loss: 0.83703 |  0:40:41s\n",
      "epoch 767| loss: 0.83692 |  0:40:44s\n",
      "epoch 768| loss: 0.83714 |  0:40:47s\n",
      "epoch 769| loss: 0.8372  |  0:40:50s\n",
      "epoch 770| loss: 0.83733 |  0:40:53s\n",
      "epoch 771| loss: 0.83709 |  0:40:56s\n",
      "epoch 772| loss: 0.83698 |  0:40:59s\n",
      "epoch 773| loss: 0.83713 |  0:41:02s\n",
      "epoch 774| loss: 0.83716 |  0:41:06s\n",
      "epoch 775| loss: 0.83699 |  0:41:09s\n",
      "epoch 776| loss: 0.83727 |  0:41:12s\n",
      "epoch 777| loss: 0.83691 |  0:41:15s\n",
      "epoch 778| loss: 0.83705 |  0:41:18s\n",
      "epoch 779| loss: 0.83644 |  0:41:21s\n",
      "epoch 780| loss: 0.83705 |  0:41:24s\n",
      "epoch 781| loss: 0.83624 |  0:41:27s\n",
      "epoch 782| loss: 0.83698 |  0:41:30s\n",
      "epoch 783| loss: 0.83636 |  0:41:33s\n",
      "epoch 784| loss: 0.83657 |  0:41:36s\n",
      "epoch 785| loss: 0.83596 |  0:41:39s\n",
      "epoch 786| loss: 0.83673 |  0:41:42s\n",
      "epoch 787| loss: 0.83644 |  0:41:46s\n",
      "epoch 788| loss: 0.83643 |  0:41:49s\n",
      "epoch 789| loss: 0.83687 |  0:41:52s\n",
      "epoch 790| loss: 0.83705 |  0:41:55s\n",
      "epoch 791| loss: 0.83683 |  0:41:58s\n",
      "epoch 792| loss: 0.83674 |  0:42:01s\n",
      "epoch 793| loss: 0.83698 |  0:42:04s\n",
      "epoch 794| loss: 0.83677 |  0:42:07s\n",
      "epoch 795| loss: 0.83741 |  0:42:10s\n",
      "epoch 796| loss: 0.83719 |  0:42:13s\n",
      "epoch 797| loss: 0.83697 |  0:42:16s\n",
      "epoch 798| loss: 0.83659 |  0:42:19s\n",
      "epoch 799| loss: 0.8368  |  0:42:22s\n",
      "epoch 800| loss: 0.83717 |  0:42:25s\n",
      "epoch 801| loss: 0.83673 |  0:42:29s\n",
      "epoch 802| loss: 0.83819 |  0:42:32s\n",
      "epoch 803| loss: 0.83638 |  0:42:35s\n",
      "epoch 804| loss: 0.83627 |  0:42:38s\n",
      "epoch 805| loss: 0.83639 |  0:42:41s\n",
      "epoch 806| loss: 0.83628 |  0:42:44s\n",
      "epoch 807| loss: 0.83581 |  0:42:47s\n",
      "epoch 808| loss: 0.8365  |  0:42:50s\n",
      "epoch 809| loss: 0.8363  |  0:42:53s\n",
      "epoch 810| loss: 0.83714 |  0:42:56s\n",
      "epoch 811| loss: 0.83861 |  0:42:59s\n",
      "epoch 812| loss: 0.83883 |  0:43:02s\n",
      "epoch 813| loss: 0.83838 |  0:43:05s\n",
      "epoch 814| loss: 0.83829 |  0:43:08s\n",
      "epoch 815| loss: 0.83808 |  0:43:12s\n",
      "epoch 816| loss: 0.83812 |  0:43:15s\n",
      "epoch 817| loss: 0.83772 |  0:43:18s\n",
      "epoch 818| loss: 0.83708 |  0:43:21s\n",
      "epoch 819| loss: 0.83726 |  0:43:24s\n",
      "epoch 820| loss: 0.83694 |  0:43:27s\n",
      "epoch 821| loss: 0.83839 |  0:43:30s\n",
      "epoch 822| loss: 0.83834 |  0:43:33s\n",
      "epoch 823| loss: 0.83787 |  0:43:36s\n",
      "epoch 824| loss: 0.83808 |  0:43:39s\n",
      "epoch 825| loss: 0.83735 |  0:43:42s\n",
      "epoch 826| loss: 0.83893 |  0:43:45s\n",
      "epoch 827| loss: 0.83867 |  0:43:48s\n",
      "epoch 828| loss: 0.83794 |  0:43:51s\n",
      "epoch 829| loss: 0.8386  |  0:43:55s\n",
      "epoch 830| loss: 0.83847 |  0:43:58s\n",
      "epoch 831| loss: 0.83953 |  0:44:01s\n",
      "epoch 832| loss: 0.8392  |  0:44:04s\n",
      "epoch 833| loss: 0.83832 |  0:44:07s\n",
      "epoch 834| loss: 0.83793 |  0:44:10s\n",
      "epoch 835| loss: 0.83887 |  0:44:13s\n",
      "epoch 836| loss: 0.83801 |  0:44:16s\n",
      "epoch 837| loss: 0.83762 |  0:44:19s\n",
      "epoch 838| loss: 0.83877 |  0:44:22s\n",
      "epoch 839| loss: 0.83905 |  0:44:25s\n",
      "epoch 840| loss: 0.83872 |  0:44:28s\n",
      "epoch 841| loss: 0.83867 |  0:44:32s\n",
      "epoch 842| loss: 0.83869 |  0:44:35s\n",
      "epoch 843| loss: 0.83865 |  0:44:38s\n",
      "epoch 844| loss: 0.83984 |  0:44:41s\n",
      "epoch 845| loss: 0.8398  |  0:44:44s\n",
      "epoch 846| loss: 0.83887 |  0:44:47s\n",
      "epoch 847| loss: 0.83812 |  0:44:50s\n",
      "epoch 848| loss: 0.83876 |  0:44:53s\n",
      "epoch 849| loss: 0.83972 |  0:44:56s\n",
      "epoch 850| loss: 0.83931 |  0:44:59s\n",
      "epoch 851| loss: 0.83866 |  0:45:02s\n",
      "epoch 852| loss: 0.83888 |  0:45:05s\n",
      "epoch 853| loss: 0.83838 |  0:45:08s\n",
      "epoch 854| loss: 0.83912 |  0:45:11s\n",
      "epoch 855| loss: 0.83878 |  0:45:15s\n",
      "epoch 856| loss: 0.8386  |  0:45:18s\n",
      "epoch 857| loss: 0.83819 |  0:45:21s\n",
      "epoch 858| loss: 0.83812 |  0:45:24s\n",
      "epoch 859| loss: 0.83769 |  0:45:27s\n",
      "epoch 860| loss: 0.83815 |  0:45:30s\n",
      "epoch 861| loss: 0.83768 |  0:45:33s\n",
      "epoch 862| loss: 0.83786 |  0:45:36s\n",
      "epoch 863| loss: 0.83807 |  0:45:39s\n",
      "epoch 864| loss: 0.83753 |  0:45:42s\n",
      "epoch 865| loss: 0.83765 |  0:45:45s\n",
      "epoch 866| loss: 0.83816 |  0:45:48s\n",
      "epoch 867| loss: 0.83752 |  0:45:51s\n",
      "epoch 868| loss: 0.83769 |  0:45:54s\n",
      "epoch 869| loss: 0.83757 |  0:45:58s\n",
      "epoch 870| loss: 0.83749 |  0:46:01s\n",
      "epoch 871| loss: 0.83787 |  0:46:05s\n",
      "epoch 872| loss: 0.83662 |  0:46:08s\n",
      "epoch 873| loss: 0.83752 |  0:46:11s\n",
      "epoch 874| loss: 0.83808 |  0:46:14s\n",
      "epoch 875| loss: 0.83764 |  0:46:18s\n",
      "epoch 876| loss: 0.83742 |  0:46:21s\n",
      "epoch 877| loss: 0.83833 |  0:46:24s\n",
      "epoch 878| loss: 0.83755 |  0:46:28s\n",
      "epoch 879| loss: 0.837   |  0:46:31s\n",
      "epoch 880| loss: 0.83758 |  0:46:34s\n",
      "epoch 881| loss: 0.83731 |  0:46:37s\n",
      "epoch 882| loss: 0.83788 |  0:46:40s\n",
      "epoch 883| loss: 0.83769 |  0:46:43s\n",
      "epoch 884| loss: 0.83779 |  0:46:46s\n",
      "epoch 885| loss: 0.83732 |  0:46:49s\n",
      "epoch 886| loss: 0.83716 |  0:46:52s\n",
      "epoch 887| loss: 0.83742 |  0:46:55s\n",
      "epoch 888| loss: 0.8373  |  0:46:58s\n",
      "epoch 889| loss: 0.8371  |  0:47:01s\n",
      "epoch 890| loss: 0.83719 |  0:47:04s\n",
      "epoch 891| loss: 0.83701 |  0:47:08s\n",
      "epoch 892| loss: 0.83705 |  0:47:11s\n",
      "epoch 893| loss: 0.83742 |  0:47:14s\n",
      "epoch 894| loss: 0.83734 |  0:47:17s\n",
      "epoch 895| loss: 0.83746 |  0:47:20s\n",
      "epoch 896| loss: 0.83689 |  0:47:23s\n",
      "epoch 897| loss: 0.83708 |  0:47:26s\n",
      "epoch 898| loss: 0.83708 |  0:47:29s\n",
      "epoch 899| loss: 0.83723 |  0:47:32s\n",
      "epoch 900| loss: 0.83709 |  0:47:35s\n",
      "epoch 901| loss: 0.83731 |  0:47:38s\n",
      "epoch 902| loss: 0.83731 |  0:47:41s\n",
      "epoch 903| loss: 0.83802 |  0:47:45s\n",
      "epoch 904| loss: 0.83685 |  0:47:48s\n",
      "epoch 905| loss: 0.8374  |  0:47:51s\n",
      "epoch 906| loss: 0.8369  |  0:47:54s\n",
      "epoch 907| loss: 0.83691 |  0:47:57s\n",
      "epoch 908| loss: 0.83676 |  0:48:00s\n",
      "epoch 909| loss: 0.83715 |  0:48:04s\n",
      "epoch 910| loss: 0.83711 |  0:48:07s\n",
      "epoch 911| loss: 0.83732 |  0:48:10s\n",
      "epoch 912| loss: 0.83664 |  0:48:13s\n",
      "epoch 913| loss: 0.83667 |  0:48:16s\n",
      "epoch 914| loss: 0.83666 |  0:48:19s\n",
      "epoch 915| loss: 0.8369  |  0:48:22s\n",
      "epoch 916| loss: 0.83672 |  0:48:25s\n",
      "epoch 917| loss: 0.8376  |  0:48:28s\n",
      "epoch 918| loss: 0.83702 |  0:48:31s\n",
      "epoch 919| loss: 0.83684 |  0:48:34s\n",
      "epoch 920| loss: 0.83674 |  0:48:37s\n",
      "epoch 921| loss: 0.83748 |  0:48:40s\n",
      "epoch 922| loss: 0.8379  |  0:48:44s\n",
      "epoch 923| loss: 0.83657 |  0:48:47s\n",
      "epoch 924| loss: 0.83772 |  0:48:50s\n",
      "epoch 925| loss: 0.83679 |  0:48:53s\n",
      "epoch 926| loss: 0.83681 |  0:48:56s\n",
      "epoch 927| loss: 0.84592 |  0:48:59s\n",
      "epoch 928| loss: 0.856   |  0:49:02s\n",
      "epoch 929| loss: 0.84519 |  0:49:05s\n",
      "epoch 930| loss: 0.84275 |  0:49:08s\n",
      "epoch 931| loss: 0.84126 |  0:49:11s\n",
      "epoch 932| loss: 0.84088 |  0:49:14s\n",
      "epoch 933| loss: 0.83718 |  0:49:17s\n",
      "epoch 934| loss: 0.83747 |  0:49:21s\n",
      "epoch 935| loss: 0.83745 |  0:49:24s\n",
      "epoch 936| loss: 0.83707 |  0:49:27s\n",
      "epoch 937| loss: 0.83732 |  0:49:30s\n",
      "epoch 938| loss: 0.83658 |  0:49:33s\n",
      "epoch 939| loss: 0.83728 |  0:49:36s\n",
      "epoch 940| loss: 0.83685 |  0:49:39s\n",
      "epoch 941| loss: 0.83671 |  0:49:42s\n",
      "epoch 942| loss: 0.83667 |  0:49:45s\n",
      "epoch 943| loss: 0.83658 |  0:49:48s\n",
      "epoch 944| loss: 0.83738 |  0:49:51s\n",
      "epoch 945| loss: 0.8373  |  0:49:54s\n",
      "epoch 946| loss: 0.837   |  0:49:57s\n",
      "epoch 947| loss: 0.83634 |  0:50:01s\n",
      "epoch 948| loss: 0.8365  |  0:50:04s\n",
      "epoch 949| loss: 0.83636 |  0:50:07s\n",
      "epoch 950| loss: 0.83695 |  0:50:10s\n",
      "epoch 951| loss: 0.83679 |  0:50:13s\n",
      "epoch 952| loss: 0.83667 |  0:50:16s\n",
      "epoch 953| loss: 0.83606 |  0:50:19s\n",
      "epoch 954| loss: 0.83673 |  0:50:22s\n",
      "epoch 955| loss: 0.83637 |  0:50:25s\n",
      "epoch 956| loss: 0.83659 |  0:50:28s\n",
      "epoch 957| loss: 0.83659 |  0:50:31s\n",
      "epoch 958| loss: 0.83666 |  0:50:34s\n",
      "epoch 959| loss: 0.83696 |  0:50:37s\n",
      "epoch 960| loss: 0.83667 |  0:50:40s\n",
      "epoch 961| loss: 0.83686 |  0:50:44s\n",
      "epoch 962| loss: 0.83644 |  0:50:47s\n",
      "epoch 963| loss: 0.83641 |  0:50:50s\n",
      "epoch 964| loss: 0.83734 |  0:50:53s\n",
      "epoch 965| loss: 0.83689 |  0:50:56s\n",
      "epoch 966| loss: 0.83612 |  0:50:59s\n",
      "epoch 967| loss: 0.83631 |  0:51:02s\n",
      "epoch 968| loss: 0.83706 |  0:51:05s\n",
      "epoch 969| loss: 0.83688 |  0:51:08s\n",
      "epoch 970| loss: 0.83693 |  0:51:11s\n",
      "epoch 971| loss: 0.83609 |  0:51:14s\n",
      "epoch 972| loss: 0.83618 |  0:51:17s\n",
      "epoch 973| loss: 0.83657 |  0:51:21s\n",
      "epoch 974| loss: 0.83682 |  0:51:24s\n",
      "epoch 975| loss: 0.83667 |  0:51:27s\n",
      "epoch 976| loss: 0.83631 |  0:51:30s\n",
      "epoch 977| loss: 0.83644 |  0:51:33s\n",
      "epoch 978| loss: 0.83691 |  0:51:36s\n",
      "epoch 979| loss: 0.83619 |  0:51:39s\n",
      "epoch 980| loss: 0.83622 |  0:51:42s\n",
      "epoch 981| loss: 0.83605 |  0:51:45s\n",
      "epoch 982| loss: 0.83654 |  0:51:48s\n",
      "epoch 983| loss: 0.83672 |  0:51:52s\n",
      "epoch 984| loss: 0.83666 |  0:51:55s\n",
      "epoch 985| loss: 0.83713 |  0:51:58s\n",
      "epoch 986| loss: 0.83681 |  0:52:01s\n",
      "epoch 987| loss: 0.83636 |  0:52:04s\n",
      "epoch 988| loss: 0.83656 |  0:52:07s\n",
      "epoch 989| loss: 0.83676 |  0:52:10s\n",
      "epoch 990| loss: 0.83595 |  0:52:13s\n",
      "epoch 991| loss: 0.83625 |  0:52:16s\n",
      "epoch 992| loss: 0.8361  |  0:52:19s\n",
      "epoch 993| loss: 0.8363  |  0:52:22s\n",
      "epoch 994| loss: 0.83643 |  0:52:25s\n",
      "epoch 995| loss: 0.83629 |  0:52:29s\n",
      "epoch 996| loss: 0.83641 |  0:52:32s\n",
      "epoch 997| loss: 0.83635 |  0:52:35s\n",
      "epoch 998| loss: 0.83647 |  0:52:38s\n",
      "epoch 999| loss: 0.8366  |  0:52:41s\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.multitask import TabNetMultiTaskClassifier\n",
    "\n",
    "clf = TabNetMultiTaskClassifier(\n",
    "    n_d=64, n_a=64, n_steps=5,\n",
    "    lambda_sparse=1e-4,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    scheduler_params = {\"gamma\": 0.95, \"step_size\": 20},\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    device_name='cuda'\n",
    ")\n",
    "clf.fit(\n",
    "    X_train.values, np.array(Y_train).reshape(19842,1),\n",
    "    max_epochs=1000,\n",
    "    patience=30,\n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=128,\n",
    "    num_workers=1,\n",
    "    drop_last=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = clf.predict_proba(X_val.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = pd.DataFrame(np.reshape(np.array(predict), (6615,3)), columns=[0.0, 1.0, 2.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8414991948141832"
      ]
     },
     "metadata": {},
     "execution_count": 41
    }
   ],
   "source": [
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "log_loss(y_val_onehot, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.        , 0.13454969, 0.00372305, 0.00177471, 0.        ,\n",
       "       0.0024967 , 0.        , 0.00975541, 0.        , 0.35324094,\n",
       "       0.        , 0.        , 0.        , 0.011013  , 0.        ,\n",
       "       0.13670576, 0.34674074])"
      ]
     },
     "metadata": {},
     "execution_count": 42
    }
   ],
   "source": [
    "clf.feature_importances_"
   ]
  }
 ]
}