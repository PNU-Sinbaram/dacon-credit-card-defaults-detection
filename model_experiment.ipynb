{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd0bed0ea09e0a217f0cd8c3af8b97b5ce48feb5846fec0e29c23d707f4dd7f9787",
   "display_name": "Python 3.8.3 64-bit ('mlenv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 0) 데이터 전처리\n",
    "* Baseline코드에서 사용했던 전처리 방식을 그대로 사용\n",
    "* index column은 학습에 관련이 없으니 제거"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "#train = pd.read_csv(\"../data/train.csv\")                            #r*c = 20000*20, test와 달리 credit이라는 column을 갖고 있고, 이 값을 예측\n",
    "#test = pd.read_csv(\"../data/test.csv\")                              #10000*19. train으로 학습시키고 test데이터를 입력으로 넣어서 credit을 예측\n",
    "#sample_submission = pd.read_csv(\"../data/sample_submission.csv\")    #예측값은 sample_submission과 형태가 같아야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../../PracticeCodes/DACON/data/train.csv\")                            #r*c = 20000*20, test와 달리 credit이라는 column을 갖고 있고, 이 값을 예측\n",
    "test = pd.read_csv(\"../../PracticeCodes/DACON/data/test.csv\")                              #10000*19. train으로 학습시키고 test데이터를 입력으로 넣어서 credit을 예측\n",
    "sample_submission = pd.read_csv(\"../../PracticeCodes/DACON/data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train, test], axis=0)    # train데이터 밑에 test데이터가 붙음. test에는 credit이 없으므로, 결측치(NaN)형태로 저장됨\n",
    "# 실제로는 결측치를 완전히 날리는건 좋지 않지만, 1회 출제용으로 사용하기때문에 완전히 날림\n",
    "data = data.drop(\"occyp_type\", axis=1) # occyp_type column을 지움. axis : occyp_type이 row에 있는지 column에 있는지 알려줌. 1이면 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_len = data.apply(lambda x : len(x.unique()))  # 각 column별로 unique()를 수행하여, 그 길이를 반환. 즉, 모든 column의 요소의 개수를 출력\n",
    "group_1 = unique_len[unique_len <= 2].index   # 요소의 값이 2개 이하인 column들의 이름을 추출\n",
    "group_2 = unique_len[(unique_len > 2) & (unique_len <= 10)].index\n",
    "group_3 = unique_len[(unique_len > 10)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['gender'] = data['gender'].replace(['F','M'], [0, 1])   # F를 0으로, M을 1로 교체\n",
    "data['car'] = data['car'].replace(['N', 'Y'], [0, 1])\n",
    "data['reality'] = data['reality'].replace(['N', 'Y'], [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['child_num']>2, 'child_num'] = 2  # child_num>2인 child_num column을 가져와서 2로 바꿈\n",
    "data[group_2].apply(lambda x : len(x.unique()))\n",
    "label_encoder = preprocessing.LabelEncoder() # categorical 변수(문자로 되어있는 변수)들을 숫자로 인코딩해주는 함수\n",
    "set(label_encoder.fit_transform(data['income_type'])) # income_type column에서 각 요소들을 숫자로 바꿔줌. fit_transform이 배열을 반환해서 unique()대신 set을 사용\n",
    "data['income_type'] = label_encoder.fit_transform(data['income_type'])\n",
    "data['edu_type'] = label_encoder.fit_transform(data['edu_type'])\n",
    "data['family_type'] = label_encoder.fit_transform(data['family_type'])\n",
    "data['house_type'] = label_encoder.fit_transform(data['house_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, bin_dividers = np.histogram(data['income_total'], bins=7) # 연속형 변수를 입력으로 받아 몇 개의 구간으로 나눌지 설정해줌. 각 구간들의 분절점(나누는 기준) 및 구간별 요소의 개수를 출력해줌\n",
    "data['income_total'] = pd.factorize(pd.cut(data['income_total'], bins = bin_dividers, include_lowest=True, labels=[0,1,2,3,4,5,6]))[0] # pd.cut의 반환 데이터 타입은 category이기 때문에, series타입(int형 배열)으로 바꿔주는 작업을 거쳐야 함\n",
    "#위의 과정을 함수로 만듬\n",
    "def make_bin(array, N):\n",
    "    array = -array      #DAYS_BIRTH등의 column은 음수이기 때문에 양수로 바꿔줌\n",
    "    _, bin_dividers = np.histogram(array, bins = N)       # 여기선 counts 변수를 사용하지 않을 것이기 때문에 사용하지 않는다는 의미로 _로 설정.\n",
    "    cut_categories = pd.cut(array, bin_dividers, labels = [i for i in range(N)], include_lowest=True)\n",
    "    bined_array = pd.factorize(cut_categories)[0]\n",
    "    return bined_array\n",
    "data['DAYS_BIRTH'] = make_bin(data['DAYS_BIRTH'], 10)\n",
    "data['DAYS_EMPLOYED'] = make_bin(data['DAYS_EMPLOYED'], 10)\n",
    "data['begin_month'] = make_bin(data['begin_month'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop('index', axis=1)\n",
    "train = data[:-10000]   # train에 해당하는 값\n",
    "test = data[-10000:]   #test에 해당하는 값\n",
    "train_x = train.drop(\"credit\", axis = 1) # credit은 출력값이고, credit을 제외한 값들이 모델의 입력값이므로 column들 중(axis =1) credit을 찾아 없앰.\n",
    "train_y = train['credit']               # 모델의 출력이 credit\n",
    "test_x = test.drop(\"credit\", axis=1)        # data라는 dataframe을 만들면서 test set에 없던 credit이라는 column이 생겼으므로, 이를 다시 제거"
   ]
  },
  {
   "source": [
    "## 1) RandomForestClassifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, stratify=train_y, test_size=0.2, random_state=10086)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.466047343279552\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, Y_train)\n",
    "predict = clf.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "logloss = log_loss(y_val_onehot, predict)\n",
    "print(logloss)"
   ]
  },
  {
   "source": [
    "### 1_1) 하이퍼파라미터 조정"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_param_grid = {\n",
    "    'n_estimators' : [100, 200],\n",
    "    'max_depth' : [8, 10, 12],\n",
    "    'min_samples_leaf' : [3, 5, 7],\n",
    "    'min_samples_split' : [3, 4, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "0.8262986236456679\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, stratify=train_y, test_size=0.25, random_state=10086)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf_grid = GridSearchCV(clf, param_grid=rf_param_grid, scoring=\"accuracy\", n_jobs=-1, verbose=2)\n",
    "clf_grid.fit(X_train, Y_train)\n",
    "\n",
    "predict = clf_grid.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "logloss=log_loss(y_val_onehot, predict)\n",
    "print(logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[0.8282641291283019, 0.8225467954824329, 0.8253606275571261, 0.8224319510375313, 0.8201274143462735]\n"
     ]
    }
   ],
   "source": [
    "folds = StratifiedKFold(n_splits = 5, shuffle=True, random_state = 55)  # 5개의 fold로 나눔\n",
    "\n",
    "outcomes = []\n",
    "for n_fold, (train_index, val_index) in enumerate(folds.split(train_x, train_y)):\n",
    "    X_train, X_val = train_x.iloc[train_index], train_x.iloc[val_index]\n",
    "    Y_train, Y_val = train_y.iloc[train_index], train_y.iloc[val_index]\n",
    "    clf = RandomForestClassifier()\n",
    "    clf_grid = GridSearchCV(clf, param_grid=rf_param_grid, scoring=\"accuracy\", n_jobs=-1, verbose=2)\n",
    "    clf_grid.fit(X_train, Y_train)\n",
    "    predictions = clf_grid.predict_proba(X_val)\n",
    "    y_val_onehot = pd.get_dummies(Y_val)\n",
    "\n",
    "    logloss = log_loss(y_val_onehot, predictions)\n",
    "    outcomes.append(logloss)\n",
    "\n",
    "print(outcomes)"
   ]
  },
  {
   "source": [
    "## 2) Gradient Boosting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8335746908166416\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, stratify=train_y, test_size=0.25, random_state=10086)\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(X_train, Y_train)\n",
    "predict = gb.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "logloss = log_loss(y_val_onehot, predict)\n",
    "print(logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.8417415560977669, 0.8341085098795408, 0.8386208102239824, 0.8392900501671904, 0.8335760935299279]\n"
     ]
    }
   ],
   "source": [
    "folds = StratifiedKFold(n_splits = 5, shuffle=True, random_state = 55)  # 5개의 fold로 나눔\n",
    "\n",
    "outcomes = []\n",
    "for n_fold, (train_index, val_index) in enumerate(folds.split(train_x, train_y)):\n",
    "    X_train, X_val = train_x.iloc[train_index], train_x.iloc[val_index]\n",
    "    Y_train, Y_val = train_y.iloc[train_index], train_y.iloc[val_index]\n",
    "    gb = GradientBoostingClassifier()\n",
    "    gb.fit(X_train, Y_train)\n",
    "    predictions = gb.predict_proba(X_val)\n",
    "    y_val_onehot = pd.get_dummies(Y_val)\n",
    "\n",
    "    logloss = log_loss(y_val_onehot, predictions)\n",
    "    outcomes.append(logloss)\n",
    "print(outcomes)"
   ]
  },
  {
   "source": [
    "### 2_1) Gradient Boosting parameter 조정\n",
    "* **주의. 한번 실행에 시간 소요가 너무 김**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_param_grid = {\n",
    "    'n_estimators' : [100, 200],\n",
    "    'max_depth' : [8, 10, 12],\n",
    "    'min_samples_leaf' : [5, 7, 10],\n",
    "    'min_samples_split' : [2, 3, 5],\n",
    "    'learning_rate' : [0.05, 0.1, 0.2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-4a78ca480250>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mgb_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgb_param_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mgb_grid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgb_grid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0my_val_onehot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pythonenv\\mlenv\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pythonenv\\mlenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    839\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    842\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pythonenv\\mlenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1294\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1295\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1296\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pythonenv\\mlenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    793\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[0;32m    796\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pythonenv\\mlenv\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1054\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1055\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pythonenv\\mlenv\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    931\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pythonenv\\mlenv\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    432\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "gb_grid = GridSearchCV(gb, param_grid = gb_param_grid, scoring=\"accuracy\", n_jobs=-1, verbose=3)\n",
    "gb_grid.fit(X_train, Y_train)\n",
    "predict = gb_grid.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "logloss = log_loss(y_val_onehot, predict)\n",
    "print(logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'learning_rate': 0.05,\n",
       " 'max_depth': 8,\n",
       " 'min_samples_leaf': 7,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 100}"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "gb_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.8299092537458699, 0.8195784379737493, 0.822702511343288, 0.8219319425895892, 0.8183218626777928]\n"
     ]
    }
   ],
   "source": [
    "folds = StratifiedKFold(n_splits = 5, shuffle=True, random_state = 55)  # 5개의 fold로 나눔\n",
    "\n",
    "outcomes = []\n",
    "for n_fold, (train_index, val_index) in enumerate(folds.split(train_x, train_y)):\n",
    "    X_train, X_val = train_x.iloc[train_index], train_x.iloc[val_index]\n",
    "    Y_train, Y_val = train_y.iloc[train_index], train_y.iloc[val_index]\n",
    "    gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.05, max_depth=8, min_samples_leaf=7, min_samples_split=2)\n",
    "    gb.fit(X_train, Y_train)\n",
    "    predict = gb.predict_proba(X_val)\n",
    "    y_val_onehot = pd.get_dummies(Y_val)\n",
    "    logloss = log_loss(y_val_onehot, predict)\n",
    "    outcomes.append(logloss)\n",
    "print(outcomes)"
   ]
  },
  {
   "source": [
    "## 3) AdaBoost"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.0877788002340585"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, stratify=train_y, test_size=0.25, random_state=10086)\n",
    "\n",
    "ag = AdaBoostClassifier()\n",
    "ag.fit(X_train, Y_train)\n",
    "\n",
    "predict = ag.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "log_loss(y_val_onehot, predict)"
   ]
  },
  {
   "source": [
    "## 4) XgBoost"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[20:56:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "[20:56:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "[20:56:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "[20:56:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "[20:56:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.8324195141525684, 0.8266500276527476, 0.8265990901853171, 0.8235172349839919, 0.8234311175080091]\n",
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "outcomes = []\n",
    "for n_fold, (train_index, val_index) in enumerate(folds.split(train_x, train_y)):\n",
    "    X_train, X_val = train_x.iloc[train_index], train_x.iloc[val_index]\n",
    "    Y_train, Y_val = train_y.iloc[train_index], train_y.iloc[val_index]\n",
    "    xgb = XGBClassifier(n_estimators=500, learning_rate=0.1, max_depth = 4, use_label_encoder=False)\n",
    "    xgb.fit(X_train, Y_train)\n",
    "    predictions = xgb.predict_proba(X_val)\n",
    "    y_val_onehot = pd.get_dummies(Y_val)\n",
    "\n",
    "    logloss = log_loss(y_val_onehot, predictions)\n",
    "    outcomes.append(logloss)\n",
    "print(outcomes)"
   ]
  },
  {
   "source": [
    "### 4_1) XgBoost hyperparameter tuning\n",
    "* **주의. 한번 실행에 시간 소요가 너무 김**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "xgb_param_grid = {\n",
    "    'n_estimators' : [300, 500, 600],\n",
    "    'learning_rate' : [0.01, 0.05, 0.1, 0.15],\n",
    "    'max_depth' : [3, 4, 6, 8]\n",
    "}\n",
    "\n",
    "xgb_grid = GridSearchCV(xgb, param_grid = xgb_param_grid, scoring=\"accuracy\", n_jobs= -1, verbose = 3)\n",
    "xgb_grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "실행 시간이 너무 오래 걸려 전에 돌려놨던 결과로 대체함\n",
    "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
    "c:\\Users\\lijm1\\Desktop\\pythonenv\\mlenv\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
    "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
    "[01:20:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
    "GridSearchCV(estimator=XGBClassifier(base_score=None, booster=None,\n",
    "                                     colsample_bylevel=None,\n",
    "                                     colsample_bynode=None,\n",
    "                                     colsample_bytree=None, gamma=None,\n",
    "                                     gpu_id=None, importance_type='gain',\n",
    "                                     interaction_constraints=None,\n",
    "                                     learning_rate=None, max_delta_step=None,\n",
    "                                     max_depth=None, min_child_weight=None,\n",
    "                                     missing=nan, monotone_constraints=None,\n",
    "                                     n_estimators=100, n_jobs=None,\n",
    "                                     num_parallel_tree=None, random_state=None,\n",
    "                                     reg_alpha=None, reg_lambda=None,\n",
    "                                     scale_pos_weight=None, subsample=None,\n",
    "                                     tree_method=None, validate_parameters=None,\n",
    "                                     verbosity=None),\n",
    "             n_jobs=-1,\n",
    "             param_grid={'learning_rate': [0.01], 'max_depth': [4],\n",
    "                         'n_estimators': [500]},\n",
    "             scoring='accuracy', verbose=3)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[20:57:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8359875737021389"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, stratify=train_y, test_size=0.25, random_state=10086)\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=500, learning_rate=0.01, max_depth = 4, use_label_encoder=False)\n",
    "xgb.fit(X_train, Y_train)\n",
    "predictions = xgb.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "\n",
    "log_loss(y_val_onehot, predictions)"
   ]
  },
  {
   "source": [
    "## 5) LightGBM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8248750259707588"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier, plot_importance\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, stratify=train_y, test_size=0.25, random_state=10086)\n",
    "\n",
    "lgb = LGBMClassifier(n_estimators=400)\n",
    "lgb.fit(X_train, Y_train)\n",
    "predict = lgb.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "log_loss(y_val_onehot, predict)"
   ]
  },
  {
   "source": [
    "### 5_1) LGBM Early stopping 적용"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1]\tvalid_0's multi_logloss: 0.871384\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tvalid_0's multi_logloss: 0.863215\n",
      "[3]\tvalid_0's multi_logloss: 0.857059\n",
      "[4]\tvalid_0's multi_logloss: 0.852325\n",
      "[5]\tvalid_0's multi_logloss: 0.848258\n",
      "[6]\tvalid_0's multi_logloss: 0.84532\n",
      "[7]\tvalid_0's multi_logloss: 0.842952\n",
      "[8]\tvalid_0's multi_logloss: 0.840694\n",
      "[9]\tvalid_0's multi_logloss: 0.83901\n",
      "[10]\tvalid_0's multi_logloss: 0.837603\n",
      "[11]\tvalid_0's multi_logloss: 0.836505\n",
      "[12]\tvalid_0's multi_logloss: 0.835527\n",
      "[13]\tvalid_0's multi_logloss: 0.834671\n",
      "[14]\tvalid_0's multi_logloss: 0.833793\n",
      "[15]\tvalid_0's multi_logloss: 0.833183\n",
      "[16]\tvalid_0's multi_logloss: 0.83234\n",
      "[17]\tvalid_0's multi_logloss: 0.831761\n",
      "[18]\tvalid_0's multi_logloss: 0.831425\n",
      "[19]\tvalid_0's multi_logloss: 0.830885\n",
      "[20]\tvalid_0's multi_logloss: 0.830542\n",
      "[21]\tvalid_0's multi_logloss: 0.830351\n",
      "[22]\tvalid_0's multi_logloss: 0.830166\n",
      "[23]\tvalid_0's multi_logloss: 0.830036\n",
      "[24]\tvalid_0's multi_logloss: 0.829671\n",
      "[25]\tvalid_0's multi_logloss: 0.829519\n",
      "[26]\tvalid_0's multi_logloss: 0.829229\n",
      "[27]\tvalid_0's multi_logloss: 0.829262\n",
      "[28]\tvalid_0's multi_logloss: 0.829037\n",
      "[29]\tvalid_0's multi_logloss: 0.828849\n",
      "[30]\tvalid_0's multi_logloss: 0.828643\n",
      "[31]\tvalid_0's multi_logloss: 0.828665\n",
      "[32]\tvalid_0's multi_logloss: 0.828359\n",
      "[33]\tvalid_0's multi_logloss: 0.828262\n",
      "[34]\tvalid_0's multi_logloss: 0.827804\n",
      "[35]\tvalid_0's multi_logloss: 0.827619\n",
      "[36]\tvalid_0's multi_logloss: 0.827389\n",
      "[37]\tvalid_0's multi_logloss: 0.827356\n",
      "[38]\tvalid_0's multi_logloss: 0.827293\n",
      "[39]\tvalid_0's multi_logloss: 0.826958\n",
      "[40]\tvalid_0's multi_logloss: 0.826816\n",
      "[41]\tvalid_0's multi_logloss: 0.826786\n",
      "[42]\tvalid_0's multi_logloss: 0.826573\n",
      "[43]\tvalid_0's multi_logloss: 0.82634\n",
      "[44]\tvalid_0's multi_logloss: 0.826291\n",
      "[45]\tvalid_0's multi_logloss: 0.826135\n",
      "[46]\tvalid_0's multi_logloss: 0.826101\n",
      "[47]\tvalid_0's multi_logloss: 0.825966\n",
      "[48]\tvalid_0's multi_logloss: 0.825712\n",
      "[49]\tvalid_0's multi_logloss: 0.825559\n",
      "[50]\tvalid_0's multi_logloss: 0.825187\n",
      "[51]\tvalid_0's multi_logloss: 0.825019\n",
      "[52]\tvalid_0's multi_logloss: 0.825123\n",
      "[53]\tvalid_0's multi_logloss: 0.825115\n",
      "[54]\tvalid_0's multi_logloss: 0.824682\n",
      "[55]\tvalid_0's multi_logloss: 0.824667\n",
      "[56]\tvalid_0's multi_logloss: 0.824501\n",
      "[57]\tvalid_0's multi_logloss: 0.824404\n",
      "[58]\tvalid_0's multi_logloss: 0.824241\n",
      "[59]\tvalid_0's multi_logloss: 0.824274\n",
      "[60]\tvalid_0's multi_logloss: 0.824144\n",
      "[61]\tvalid_0's multi_logloss: 0.823991\n",
      "[62]\tvalid_0's multi_logloss: 0.823945\n",
      "[63]\tvalid_0's multi_logloss: 0.823793\n",
      "[64]\tvalid_0's multi_logloss: 0.82363\n",
      "[65]\tvalid_0's multi_logloss: 0.823577\n",
      "[66]\tvalid_0's multi_logloss: 0.823364\n",
      "[67]\tvalid_0's multi_logloss: 0.823236\n",
      "[68]\tvalid_0's multi_logloss: 0.823105\n",
      "[69]\tvalid_0's multi_logloss: 0.823008\n",
      "[70]\tvalid_0's multi_logloss: 0.823012\n",
      "[71]\tvalid_0's multi_logloss: 0.823018\n",
      "[72]\tvalid_0's multi_logloss: 0.822906\n",
      "[73]\tvalid_0's multi_logloss: 0.822797\n",
      "[74]\tvalid_0's multi_logloss: 0.822914\n",
      "[75]\tvalid_0's multi_logloss: 0.822859\n",
      "[76]\tvalid_0's multi_logloss: 0.822457\n",
      "[77]\tvalid_0's multi_logloss: 0.822447\n",
      "[78]\tvalid_0's multi_logloss: 0.822259\n",
      "[79]\tvalid_0's multi_logloss: 0.822138\n",
      "[80]\tvalid_0's multi_logloss: 0.822083\n",
      "[81]\tvalid_0's multi_logloss: 0.822013\n",
      "[82]\tvalid_0's multi_logloss: 0.822043\n",
      "[83]\tvalid_0's multi_logloss: 0.822145\n",
      "[84]\tvalid_0's multi_logloss: 0.822083\n",
      "[85]\tvalid_0's multi_logloss: 0.821986\n",
      "[86]\tvalid_0's multi_logloss: 0.821953\n",
      "[87]\tvalid_0's multi_logloss: 0.821722\n",
      "[88]\tvalid_0's multi_logloss: 0.821543\n",
      "[89]\tvalid_0's multi_logloss: 0.821408\n",
      "[90]\tvalid_0's multi_logloss: 0.821313\n",
      "[91]\tvalid_0's multi_logloss: 0.821416\n",
      "[92]\tvalid_0's multi_logloss: 0.820971\n",
      "[93]\tvalid_0's multi_logloss: 0.820791\n",
      "[94]\tvalid_0's multi_logloss: 0.820557\n",
      "[95]\tvalid_0's multi_logloss: 0.820226\n",
      "[96]\tvalid_0's multi_logloss: 0.820044\n",
      "[97]\tvalid_0's multi_logloss: 0.819827\n",
      "[98]\tvalid_0's multi_logloss: 0.819404\n",
      "[99]\tvalid_0's multi_logloss: 0.819393\n",
      "[100]\tvalid_0's multi_logloss: 0.819325\n",
      "[101]\tvalid_0's multi_logloss: 0.81921\n",
      "[102]\tvalid_0's multi_logloss: 0.819381\n",
      "[103]\tvalid_0's multi_logloss: 0.819383\n",
      "[104]\tvalid_0's multi_logloss: 0.819118\n",
      "[105]\tvalid_0's multi_logloss: 0.818937\n",
      "[106]\tvalid_0's multi_logloss: 0.818774\n",
      "[107]\tvalid_0's multi_logloss: 0.818807\n",
      "[108]\tvalid_0's multi_logloss: 0.818723\n",
      "[109]\tvalid_0's multi_logloss: 0.818886\n",
      "[110]\tvalid_0's multi_logloss: 0.818767\n",
      "[111]\tvalid_0's multi_logloss: 0.818832\n",
      "[112]\tvalid_0's multi_logloss: 0.818768\n",
      "[113]\tvalid_0's multi_logloss: 0.818561\n",
      "[114]\tvalid_0's multi_logloss: 0.818637\n",
      "[115]\tvalid_0's multi_logloss: 0.8185\n",
      "[116]\tvalid_0's multi_logloss: 0.818314\n",
      "[117]\tvalid_0's multi_logloss: 0.818235\n",
      "[118]\tvalid_0's multi_logloss: 0.818215\n",
      "[119]\tvalid_0's multi_logloss: 0.818015\n",
      "[120]\tvalid_0's multi_logloss: 0.818105\n",
      "[121]\tvalid_0's multi_logloss: 0.817966\n",
      "[122]\tvalid_0's multi_logloss: 0.817894\n",
      "[123]\tvalid_0's multi_logloss: 0.817587\n",
      "[124]\tvalid_0's multi_logloss: 0.817499\n",
      "[125]\tvalid_0's multi_logloss: 0.817386\n",
      "[126]\tvalid_0's multi_logloss: 0.817303\n",
      "[127]\tvalid_0's multi_logloss: 0.81739\n",
      "[128]\tvalid_0's multi_logloss: 0.817411\n",
      "[129]\tvalid_0's multi_logloss: 0.817387\n",
      "[130]\tvalid_0's multi_logloss: 0.817349\n",
      "[131]\tvalid_0's multi_logloss: 0.817435\n",
      "[132]\tvalid_0's multi_logloss: 0.817424\n",
      "[133]\tvalid_0's multi_logloss: 0.817413\n",
      "[134]\tvalid_0's multi_logloss: 0.817493\n",
      "[135]\tvalid_0's multi_logloss: 0.817377\n",
      "[136]\tvalid_0's multi_logloss: 0.81738\n",
      "[137]\tvalid_0's multi_logloss: 0.817454\n",
      "[138]\tvalid_0's multi_logloss: 0.817259\n",
      "[139]\tvalid_0's multi_logloss: 0.81714\n",
      "[140]\tvalid_0's multi_logloss: 0.816845\n",
      "[141]\tvalid_0's multi_logloss: 0.816691\n",
      "[142]\tvalid_0's multi_logloss: 0.816643\n",
      "[143]\tvalid_0's multi_logloss: 0.816532\n",
      "[144]\tvalid_0's multi_logloss: 0.81637\n",
      "[145]\tvalid_0's multi_logloss: 0.81613\n",
      "[146]\tvalid_0's multi_logloss: 0.815985\n",
      "[147]\tvalid_0's multi_logloss: 0.816012\n",
      "[148]\tvalid_0's multi_logloss: 0.816032\n",
      "[149]\tvalid_0's multi_logloss: 0.81607\n",
      "[150]\tvalid_0's multi_logloss: 0.815985\n",
      "[151]\tvalid_0's multi_logloss: 0.816081\n",
      "[152]\tvalid_0's multi_logloss: 0.815991\n",
      "[153]\tvalid_0's multi_logloss: 0.815889\n",
      "[154]\tvalid_0's multi_logloss: 0.815904\n",
      "[155]\tvalid_0's multi_logloss: 0.815886\n",
      "[156]\tvalid_0's multi_logloss: 0.815752\n",
      "[157]\tvalid_0's multi_logloss: 0.815806\n",
      "[158]\tvalid_0's multi_logloss: 0.815742\n",
      "[159]\tvalid_0's multi_logloss: 0.815868\n",
      "[160]\tvalid_0's multi_logloss: 0.815914\n",
      "[161]\tvalid_0's multi_logloss: 0.815892\n",
      "[162]\tvalid_0's multi_logloss: 0.815838\n",
      "[163]\tvalid_0's multi_logloss: 0.815685\n",
      "[164]\tvalid_0's multi_logloss: 0.815754\n",
      "[165]\tvalid_0's multi_logloss: 0.815742\n",
      "[166]\tvalid_0's multi_logloss: 0.815818\n",
      "[167]\tvalid_0's multi_logloss: 0.815877\n",
      "[168]\tvalid_0's multi_logloss: 0.815856\n",
      "[169]\tvalid_0's multi_logloss: 0.816004\n",
      "[170]\tvalid_0's multi_logloss: 0.816022\n",
      "[171]\tvalid_0's multi_logloss: 0.816048\n",
      "[172]\tvalid_0's multi_logloss: 0.8161\n",
      "[173]\tvalid_0's multi_logloss: 0.816062\n",
      "[174]\tvalid_0's multi_logloss: 0.816022\n",
      "[175]\tvalid_0's multi_logloss: 0.815977\n",
      "[176]\tvalid_0's multi_logloss: 0.81602\n",
      "[177]\tvalid_0's multi_logloss: 0.816042\n",
      "[178]\tvalid_0's multi_logloss: 0.816101\n",
      "[179]\tvalid_0's multi_logloss: 0.816191\n",
      "[180]\tvalid_0's multi_logloss: 0.816228\n",
      "[181]\tvalid_0's multi_logloss: 0.8162\n",
      "[182]\tvalid_0's multi_logloss: 0.816205\n",
      "[183]\tvalid_0's multi_logloss: 0.816312\n",
      "[184]\tvalid_0's multi_logloss: 0.816287\n",
      "[185]\tvalid_0's multi_logloss: 0.816403\n",
      "[186]\tvalid_0's multi_logloss: 0.816519\n",
      "[187]\tvalid_0's multi_logloss: 0.816444\n",
      "[188]\tvalid_0's multi_logloss: 0.816421\n",
      "[189]\tvalid_0's multi_logloss: 0.816298\n",
      "[190]\tvalid_0's multi_logloss: 0.816265\n",
      "[191]\tvalid_0's multi_logloss: 0.816343\n",
      "[192]\tvalid_0's multi_logloss: 0.816423\n",
      "[193]\tvalid_0's multi_logloss: 0.816623\n",
      "[194]\tvalid_0's multi_logloss: 0.81658\n",
      "[195]\tvalid_0's multi_logloss: 0.816574\n",
      "[196]\tvalid_0's multi_logloss: 0.816605\n",
      "[197]\tvalid_0's multi_logloss: 0.816762\n",
      "[198]\tvalid_0's multi_logloss: 0.816645\n",
      "[199]\tvalid_0's multi_logloss: 0.816797\n",
      "[200]\tvalid_0's multi_logloss: 0.816738\n",
      "[201]\tvalid_0's multi_logloss: 0.816695\n",
      "[202]\tvalid_0's multi_logloss: 0.816749\n",
      "[203]\tvalid_0's multi_logloss: 0.816764\n",
      "[204]\tvalid_0's multi_logloss: 0.816815\n",
      "[205]\tvalid_0's multi_logloss: 0.816856\n",
      "[206]\tvalid_0's multi_logloss: 0.816949\n",
      "[207]\tvalid_0's multi_logloss: 0.816955\n",
      "[208]\tvalid_0's multi_logloss: 0.816852\n",
      "[209]\tvalid_0's multi_logloss: 0.816964\n",
      "[210]\tvalid_0's multi_logloss: 0.817072\n",
      "[211]\tvalid_0's multi_logloss: 0.817013\n",
      "[212]\tvalid_0's multi_logloss: 0.817144\n",
      "[213]\tvalid_0's multi_logloss: 0.817175\n",
      "[214]\tvalid_0's multi_logloss: 0.817195\n",
      "[215]\tvalid_0's multi_logloss: 0.817245\n",
      "[216]\tvalid_0's multi_logloss: 0.817418\n",
      "[217]\tvalid_0's multi_logloss: 0.817516\n",
      "[218]\tvalid_0's multi_logloss: 0.817628\n",
      "[219]\tvalid_0's multi_logloss: 0.817768\n",
      "[220]\tvalid_0's multi_logloss: 0.817828\n",
      "[221]\tvalid_0's multi_logloss: 0.81796\n",
      "[222]\tvalid_0's multi_logloss: 0.818176\n",
      "[223]\tvalid_0's multi_logloss: 0.8182\n",
      "[224]\tvalid_0's multi_logloss: 0.818271\n",
      "[225]\tvalid_0's multi_logloss: 0.818258\n",
      "[226]\tvalid_0's multi_logloss: 0.818392\n",
      "[227]\tvalid_0's multi_logloss: 0.818549\n",
      "[228]\tvalid_0's multi_logloss: 0.818574\n",
      "[229]\tvalid_0's multi_logloss: 0.818664\n",
      "[230]\tvalid_0's multi_logloss: 0.818693\n",
      "[231]\tvalid_0's multi_logloss: 0.818632\n",
      "[232]\tvalid_0's multi_logloss: 0.818626\n",
      "[233]\tvalid_0's multi_logloss: 0.818638\n",
      "[234]\tvalid_0's multi_logloss: 0.818587\n",
      "[235]\tvalid_0's multi_logloss: 0.818552\n",
      "[236]\tvalid_0's multi_logloss: 0.818431\n",
      "[237]\tvalid_0's multi_logloss: 0.818532\n",
      "[238]\tvalid_0's multi_logloss: 0.818426\n",
      "[239]\tvalid_0's multi_logloss: 0.818505\n",
      "[240]\tvalid_0's multi_logloss: 0.818419\n",
      "[241]\tvalid_0's multi_logloss: 0.81839\n",
      "[242]\tvalid_0's multi_logloss: 0.818433\n",
      "[243]\tvalid_0's multi_logloss: 0.818293\n",
      "[244]\tvalid_0's multi_logloss: 0.818186\n",
      "[245]\tvalid_0's multi_logloss: 0.818149\n",
      "[246]\tvalid_0's multi_logloss: 0.818069\n",
      "[247]\tvalid_0's multi_logloss: 0.81819\n",
      "[248]\tvalid_0's multi_logloss: 0.818154\n",
      "[249]\tvalid_0's multi_logloss: 0.818171\n",
      "[250]\tvalid_0's multi_logloss: 0.818223\n",
      "[251]\tvalid_0's multi_logloss: 0.818296\n",
      "[252]\tvalid_0's multi_logloss: 0.818262\n",
      "[253]\tvalid_0's multi_logloss: 0.818232\n",
      "[254]\tvalid_0's multi_logloss: 0.818197\n",
      "[255]\tvalid_0's multi_logloss: 0.818272\n",
      "[256]\tvalid_0's multi_logloss: 0.818405\n",
      "[257]\tvalid_0's multi_logloss: 0.818496\n",
      "[258]\tvalid_0's multi_logloss: 0.818531\n",
      "[259]\tvalid_0's multi_logloss: 0.818558\n",
      "[260]\tvalid_0's multi_logloss: 0.818767\n",
      "[261]\tvalid_0's multi_logloss: 0.81879\n",
      "[262]\tvalid_0's multi_logloss: 0.818777\n",
      "[263]\tvalid_0's multi_logloss: 0.818814\n",
      "Early stopping, best iteration is:\n",
      "[163]\tvalid_0's multi_logloss: 0.815685\n"
     ]
    }
   ],
   "source": [
    "lgb = LGBMClassifier(n_estimators=1000)\n",
    "evals = [(X_val, Y_val)]\n",
    "lgb.fit(X_train, Y_train, early_stopping_rounds=100, eval_metric=\"logloss\", eval_set=evals, verbose=True)\n",
    "predict = lgb.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "logloss = log_loss(y_val_onehot, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8156847808255132"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "logloss"
   ]
  },
  {
   "source": [
    "## 6) TabNet"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, stratify=train_y, test_size=0.18, random_state=10086)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/17 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "340eef1d072d4db19b4dd8035eb6b259"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "'''\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "features = [col for col in train_x.columns]\n",
    "\n",
    "categorical_columns = []\n",
    "categorical_dims = {}\n",
    "\n",
    "for col in tqdm(train_x.columns):\n",
    "    l_enc = preprocessing.LabelEncoder()\n",
    "    l_enc.fit_transform(train_x[col].values)\n",
    "    categorical_columns.append(col)\n",
    "    categorical_dims[col] = len(l_enc.classes_)\n",
    "\n",
    "cat_idxs = [i for i, f in enumerate(features) if f in categorical_columns]\n",
    "cat_dims = [categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]\n",
    "'''"
   ]
  },
  {
   "source": [
    "### Hyperparameter 정리\n",
    "#### * https://github.com/dreamquark-ai/tabnet 및 https://arxiv.org/pdf/1908.07442.pdf 를 참고함. 더 자세한 내용이나 추가 Hyperparameter는 여기로\n",
    "- 모델 파라미터\n",
    "* <code>n_d</code> = decision prediction layer의 층. 값이 클수록 featrue특징을 더 잘 잡아내지만, 오버피팅될 확률이 높음. 8~64값\n",
    "* <code>n_a</code> = Width of the attention embedding for each mask. 논문에서는 n_d == n_a가 되도록 추천함.\n",
    "* <code>n_steps</code> = decision step. 정보를 담은 feature들이 많다면 높은 값을 주는 것이 좋지만, 너무 높으면 성능을 떨어뜨리고 오버피팅될 확률이 높음. 3~10\n",
    "* <code>gamma</code> = This is the coefficient for feature reusage in the masks(?). 한번 선택된 feature가 다시 사용될지를 결정하는 hyperparameter. 논문에서는 성능에 큰 역할을 하고, n_steps가 높을수록 gamma값도 높이는 것을 추천. 1.0~2.0\n",
    "* <code>momentum</code> = 배치 정규화 모멘텀 값. 0.01~0.4\n",
    "* <code>lambda_sparse</code> = sparsity loss coefficient. 이것도 잘... 적당히 높이면 학습에 도움을 준다고 나와있음\n",
    "* <code>optimizer_fn</code> = 신경망 최적화 함수. Adam을 주로 사용\n",
    "* <code>optimizer_params</code> = optimizer_fn에 넣어줄 parameter. optimizer_fn이 Adam이라면 초기 학습률을 parameter로 받음. 기본값이 dict(lr=2e-2)\n",
    "* <code>scheduler_fn</code> = 학습률 scheduler. 여기서는 단계별로 학습률을 감쇠시키는 StepLR사용\n",
    "* <code>scheduler_params</code> = scheduler_fn에 넣어줄 parameter. 여기서는 감쇠율 gamma와 몇 단계 후 감쇠시킬지 정하는 step_size를 설정\n",
    "* <code>device_name</code> = 'cpu'로 설정하면 cpu, 'cuda'로 설정하면 nvidia gpu사용. gpu 사용하려면 cuda 11.1버전 필요(그 이상 버전은 아직 지원 안하는듯)\n",
    "* <code>mask_type</code> = featrue선택에 사용하는 masking function(?). sparsemax 또는 entmax 둘 중 하나 사용.\n",
    "- Fit 파라미터\n",
    "* <code>max_epochs</code> = 최대 epoch\n",
    "* <code>patience</code> = 지정해준 patience만큼의 epoch동안, 학습의 향상이 이루어지지 않으면, 학습을 중단하고 제일 결과가 잘 나온 것의 가중치로 설정. 0으로 하면 조기 종료 없이 계속 진행\n",
    "* <code>loss_fn</code> = loss function 지정\n",
    "* <code>batch_size</code> = 메모리 크기가 되는 한 크게 잡아주는게 좋다고 하는데 batch 크기도 logloss에 영향을 주니 잘 조절하는것이 좋을듯\n",
    "* <code>virtual_batch_size</code> = ghost batch normalization(?)에 사용된다고 함. 작은 값이 좋고, batch_size값을 나눌 수 있어야 함.\n",
    "* <code>num_workers</code> = GPU연산 시 사용할 cpu코어 개수를 설정한다는데... 오히려 설정하니까 더 느림. CPU보다 GPU가 훨씬 좋으면 어느정도 설정해주는 게 좋아보임\n",
    "* <code>drop_last</code> = 맨 마지막, 남는 배치를 쓸지 안쓸지 결정"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* **주의. 한번 실행에 시간 소요가 너무 김(약 1시간)**\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device used : cuda\n",
      "epoch 0  | loss: 3.81459 | val_0_logloss: 5.64735 |  0:00:01s\n",
      "epoch 1  | loss: 1.28557 | val_0_logloss: 2.12134 |  0:00:03s\n",
      "epoch 2  | loss: 1.03773 | val_0_logloss: 1.38402 |  0:00:05s\n",
      "epoch 3  | loss: 0.98534 | val_0_logloss: 1.11979 |  0:00:07s\n",
      "epoch 4  | loss: 0.93837 | val_0_logloss: 1.02961 |  0:00:09s\n",
      "epoch 5  | loss: 0.9175  | val_0_logloss: 0.98219 |  0:00:11s\n",
      "epoch 6  | loss: 0.90652 | val_0_logloss: 0.99071 |  0:00:13s\n",
      "epoch 7  | loss: 0.89724 | val_0_logloss: 0.99378 |  0:00:15s\n",
      "epoch 8  | loss: 0.89369 | val_0_logloss: 0.98137 |  0:00:17s\n",
      "epoch 9  | loss: 0.89046 | val_0_logloss: 0.96116 |  0:00:19s\n",
      "epoch 10 | loss: 0.88661 | val_0_logloss: 0.94896 |  0:00:21s\n",
      "epoch 11 | loss: 0.8849  | val_0_logloss: 0.93632 |  0:00:23s\n",
      "epoch 12 | loss: 0.88414 | val_0_logloss: 0.92281 |  0:00:25s\n",
      "epoch 13 | loss: 0.8811  | val_0_logloss: 0.91622 |  0:00:27s\n",
      "epoch 14 | loss: 0.87958 | val_0_logloss: 0.91203 |  0:00:29s\n",
      "epoch 15 | loss: 0.88089 | val_0_logloss: 0.9036  |  0:00:30s\n",
      "epoch 16 | loss: 0.87841 | val_0_logloss: 0.89869 |  0:00:32s\n",
      "epoch 17 | loss: 0.8763  | val_0_logloss: 0.89261 |  0:00:34s\n",
      "epoch 18 | loss: 0.87468 | val_0_logloss: 0.88652 |  0:00:35s\n",
      "epoch 19 | loss: 0.87431 | val_0_logloss: 0.8823  |  0:00:37s\n",
      "epoch 20 | loss: 0.87203 | val_0_logloss: 0.88086 |  0:00:38s\n",
      "epoch 21 | loss: 0.87203 | val_0_logloss: 0.87962 |  0:00:40s\n",
      "epoch 22 | loss: 0.86825 | val_0_logloss: 0.87701 |  0:00:41s\n",
      "epoch 23 | loss: 0.86756 | val_0_logloss: 0.8721  |  0:00:43s\n",
      "epoch 24 | loss: 0.86673 | val_0_logloss: 0.87102 |  0:00:44s\n",
      "epoch 25 | loss: 0.86476 | val_0_logloss: 0.87053 |  0:00:46s\n",
      "epoch 26 | loss: 0.86531 | val_0_logloss: 0.86902 |  0:00:47s\n",
      "epoch 27 | loss: 0.86207 | val_0_logloss: 0.8693  |  0:00:49s\n",
      "epoch 28 | loss: 0.86175 | val_0_logloss: 0.86791 |  0:00:51s\n",
      "epoch 29 | loss: 0.86089 | val_0_logloss: 0.86632 |  0:00:52s\n",
      "epoch 30 | loss: 0.86053 | val_0_logloss: 0.8715  |  0:00:54s\n",
      "epoch 31 | loss: 0.85931 | val_0_logloss: 0.86281 |  0:00:56s\n",
      "epoch 32 | loss: 0.85617 | val_0_logloss: 0.86289 |  0:00:57s\n",
      "epoch 33 | loss: 0.85773 | val_0_logloss: 0.86257 |  0:00:59s\n",
      "epoch 34 | loss: 0.85621 | val_0_logloss: 0.86101 |  0:01:01s\n",
      "epoch 35 | loss: 0.85491 | val_0_logloss: 0.86181 |  0:01:02s\n",
      "epoch 36 | loss: 0.85406 | val_0_logloss: 0.85867 |  0:01:05s\n",
      "epoch 37 | loss: 0.85071 | val_0_logloss: 0.86214 |  0:01:07s\n",
      "epoch 38 | loss: 0.85183 | val_0_logloss: 0.8626  |  0:01:09s\n",
      "epoch 39 | loss: 0.85033 | val_0_logloss: 0.86298 |  0:01:12s\n",
      "epoch 40 | loss: 0.85013 | val_0_logloss: 0.86835 |  0:01:14s\n",
      "epoch 41 | loss: 0.85094 | val_0_logloss: 0.86579 |  0:01:16s\n",
      "epoch 42 | loss: 0.85037 | val_0_logloss: 0.8602  |  0:01:18s\n",
      "epoch 43 | loss: 0.84864 | val_0_logloss: 0.8635  |  0:01:21s\n",
      "epoch 44 | loss: 0.84882 | val_0_logloss: 0.86394 |  0:01:23s\n",
      "epoch 45 | loss: 0.84797 | val_0_logloss: 0.85897 |  0:01:25s\n",
      "epoch 46 | loss: 0.8482  | val_0_logloss: 0.86516 |  0:01:27s\n",
      "epoch 47 | loss: 0.84893 | val_0_logloss: 0.86593 |  0:01:28s\n",
      "epoch 48 | loss: 0.84823 | val_0_logloss: 0.8641  |  0:01:30s\n",
      "epoch 49 | loss: 0.84807 | val_0_logloss: 0.8597  |  0:01:31s\n",
      "epoch 50 | loss: 0.84752 | val_0_logloss: 0.86177 |  0:01:33s\n",
      "epoch 51 | loss: 0.84704 | val_0_logloss: 0.86472 |  0:01:35s\n",
      "epoch 52 | loss: 0.84658 | val_0_logloss: 0.85726 |  0:01:36s\n",
      "epoch 53 | loss: 0.84573 | val_0_logloss: 0.85575 |  0:01:38s\n",
      "epoch 54 | loss: 0.84557 | val_0_logloss: 0.85605 |  0:01:40s\n",
      "epoch 55 | loss: 0.845   | val_0_logloss: 0.85234 |  0:01:42s\n",
      "epoch 56 | loss: 0.84355 | val_0_logloss: 0.84988 |  0:01:43s\n",
      "epoch 57 | loss: 0.84549 | val_0_logloss: 0.85491 |  0:01:45s\n",
      "epoch 58 | loss: 0.84499 | val_0_logloss: 0.8545  |  0:01:47s\n",
      "epoch 59 | loss: 0.84451 | val_0_logloss: 0.84793 |  0:01:49s\n",
      "epoch 60 | loss: 0.84364 | val_0_logloss: 0.85127 |  0:01:50s\n",
      "epoch 61 | loss: 0.8438  | val_0_logloss: 0.85133 |  0:01:52s\n",
      "epoch 62 | loss: 0.84289 | val_0_logloss: 0.85059 |  0:01:54s\n",
      "epoch 63 | loss: 0.84394 | val_0_logloss: 0.84632 |  0:01:56s\n",
      "epoch 64 | loss: 0.84319 | val_0_logloss: 0.84528 |  0:01:58s\n",
      "epoch 65 | loss: 0.84291 | val_0_logloss: 0.84585 |  0:02:00s\n",
      "epoch 66 | loss: 0.84164 | val_0_logloss: 0.84646 |  0:02:01s\n",
      "epoch 67 | loss: 0.8427  | val_0_logloss: 0.84645 |  0:02:03s\n",
      "epoch 68 | loss: 0.8407  | val_0_logloss: 0.84804 |  0:02:04s\n",
      "epoch 69 | loss: 0.84338 | val_0_logloss: 0.84693 |  0:02:06s\n",
      "epoch 70 | loss: 0.84207 | val_0_logloss: 0.8436  |  0:02:07s\n",
      "epoch 71 | loss: 0.84353 | val_0_logloss: 0.84663 |  0:02:09s\n",
      "epoch 72 | loss: 0.84179 | val_0_logloss: 0.8465  |  0:02:10s\n",
      "epoch 73 | loss: 0.84136 | val_0_logloss: 0.84516 |  0:02:12s\n",
      "epoch 74 | loss: 0.84098 | val_0_logloss: 0.84442 |  0:02:14s\n",
      "epoch 75 | loss: 0.84187 | val_0_logloss: 0.84676 |  0:02:15s\n",
      "epoch 76 | loss: 0.84009 | val_0_logloss: 0.84347 |  0:02:17s\n",
      "epoch 77 | loss: 0.84123 | val_0_logloss: 0.84499 |  0:02:18s\n",
      "epoch 78 | loss: 0.84034 | val_0_logloss: 0.84424 |  0:02:20s\n",
      "epoch 79 | loss: 0.83956 | val_0_logloss: 0.84537 |  0:02:21s\n",
      "epoch 80 | loss: 0.84032 | val_0_logloss: 0.84811 |  0:02:23s\n",
      "epoch 81 | loss: 0.83898 | val_0_logloss: 0.84316 |  0:02:24s\n",
      "epoch 82 | loss: 0.83793 | val_0_logloss: 0.84097 |  0:02:26s\n",
      "epoch 83 | loss: 0.83827 | val_0_logloss: 0.84467 |  0:02:27s\n",
      "epoch 84 | loss: 0.83739 | val_0_logloss: 0.84347 |  0:02:29s\n",
      "epoch 85 | loss: 0.83823 | val_0_logloss: 0.84195 |  0:02:30s\n",
      "epoch 86 | loss: 0.83872 | val_0_logloss: 0.84272 |  0:02:32s\n",
      "epoch 87 | loss: 0.83793 | val_0_logloss: 0.84495 |  0:02:34s\n",
      "epoch 88 | loss: 0.83777 | val_0_logloss: 0.84223 |  0:02:35s\n",
      "epoch 89 | loss: 0.83759 | val_0_logloss: 0.8415  |  0:02:37s\n",
      "epoch 90 | loss: 0.83682 | val_0_logloss: 0.84203 |  0:02:38s\n",
      "epoch 91 | loss: 0.83555 | val_0_logloss: 0.84181 |  0:02:40s\n",
      "epoch 92 | loss: 0.83576 | val_0_logloss: 0.84281 |  0:02:41s\n",
      "epoch 93 | loss: 0.83738 | val_0_logloss: 0.84066 |  0:02:43s\n",
      "epoch 94 | loss: 0.83581 | val_0_logloss: 0.84035 |  0:02:44s\n",
      "epoch 95 | loss: 0.83539 | val_0_logloss: 0.84392 |  0:02:46s\n",
      "epoch 96 | loss: 0.83555 | val_0_logloss: 0.84247 |  0:02:48s\n",
      "epoch 97 | loss: 0.83583 | val_0_logloss: 0.84265 |  0:02:50s\n",
      "epoch 98 | loss: 0.83475 | val_0_logloss: 0.84055 |  0:02:52s\n",
      "epoch 99 | loss: 0.83394 | val_0_logloss: 0.84037 |  0:02:53s\n",
      "epoch 100| loss: 0.83386 | val_0_logloss: 0.84207 |  0:02:55s\n",
      "epoch 101| loss: 0.83314 | val_0_logloss: 0.84021 |  0:02:57s\n",
      "epoch 102| loss: 0.83435 | val_0_logloss: 0.83913 |  0:02:59s\n",
      "epoch 103| loss: 0.8323  | val_0_logloss: 0.841   |  0:03:01s\n",
      "epoch 104| loss: 0.83328 | val_0_logloss: 0.83857 |  0:03:03s\n",
      "epoch 105| loss: 0.83322 | val_0_logloss: 0.83579 |  0:03:04s\n",
      "epoch 106| loss: 0.83328 | val_0_logloss: 0.83673 |  0:03:06s\n",
      "epoch 107| loss: 0.83359 | val_0_logloss: 0.83949 |  0:03:08s\n",
      "epoch 108| loss: 0.83277 | val_0_logloss: 0.83802 |  0:03:09s\n",
      "epoch 109| loss: 0.83138 | val_0_logloss: 0.83872 |  0:03:11s\n",
      "epoch 110| loss: 0.83121 | val_0_logloss: 0.83847 |  0:03:13s\n",
      "epoch 111| loss: 0.83087 | val_0_logloss: 0.83699 |  0:03:15s\n",
      "epoch 112| loss: 0.83116 | val_0_logloss: 0.83675 |  0:03:16s\n",
      "epoch 113| loss: 0.83045 | val_0_logloss: 0.83684 |  0:03:18s\n",
      "epoch 114| loss: 0.82946 | val_0_logloss: 0.8404  |  0:03:20s\n",
      "epoch 115| loss: 0.8301  | val_0_logloss: 0.84221 |  0:03:21s\n",
      "epoch 116| loss: 0.82973 | val_0_logloss: 0.83781 |  0:03:23s\n",
      "epoch 117| loss: 0.82954 | val_0_logloss: 0.83797 |  0:03:25s\n",
      "epoch 118| loss: 0.83004 | val_0_logloss: 0.83733 |  0:03:26s\n",
      "epoch 119| loss: 0.82959 | val_0_logloss: 0.83732 |  0:03:28s\n",
      "epoch 120| loss: 0.82935 | val_0_logloss: 0.83771 |  0:03:29s\n",
      "epoch 121| loss: 0.82914 | val_0_logloss: 0.83835 |  0:03:31s\n",
      "epoch 122| loss: 0.82888 | val_0_logloss: 0.83918 |  0:03:33s\n",
      "epoch 123| loss: 0.83026 | val_0_logloss: 0.84004 |  0:03:34s\n",
      "epoch 124| loss: 0.82899 | val_0_logloss: 0.84024 |  0:03:36s\n",
      "epoch 125| loss: 0.83004 | val_0_logloss: 0.84063 |  0:03:37s\n",
      "epoch 126| loss: 0.82963 | val_0_logloss: 0.83943 |  0:03:39s\n",
      "epoch 127| loss: 0.82874 | val_0_logloss: 0.83865 |  0:03:41s\n",
      "epoch 128| loss: 0.82802 | val_0_logloss: 0.83726 |  0:03:42s\n",
      "epoch 129| loss: 0.82846 | val_0_logloss: 0.8374  |  0:03:44s\n",
      "epoch 130| loss: 0.82703 | val_0_logloss: 0.83719 |  0:03:46s\n",
      "epoch 131| loss: 0.8265  | val_0_logloss: 0.83718 |  0:03:48s\n",
      "epoch 132| loss: 0.82613 | val_0_logloss: 0.83556 |  0:03:50s\n",
      "epoch 133| loss: 0.82455 | val_0_logloss: 0.83671 |  0:03:52s\n",
      "epoch 134| loss: 0.82545 | val_0_logloss: 0.83788 |  0:03:54s\n",
      "epoch 135| loss: 0.82561 | val_0_logloss: 0.83925 |  0:03:55s\n",
      "epoch 136| loss: 0.8251  | val_0_logloss: 0.8387  |  0:03:57s\n",
      "epoch 137| loss: 0.82443 | val_0_logloss: 0.83843 |  0:03:58s\n",
      "epoch 138| loss: 0.82503 | val_0_logloss: 0.83856 |  0:04:00s\n",
      "epoch 139| loss: 0.82498 | val_0_logloss: 0.83862 |  0:04:02s\n",
      "epoch 140| loss: 0.82553 | val_0_logloss: 0.83766 |  0:04:04s\n",
      "epoch 141| loss: 0.82473 | val_0_logloss: 0.83565 |  0:04:06s\n",
      "epoch 142| loss: 0.82519 | val_0_logloss: 0.83662 |  0:04:08s\n",
      "epoch 143| loss: 0.82545 | val_0_logloss: 0.83692 |  0:04:10s\n",
      "epoch 144| loss: 0.82594 | val_0_logloss: 0.83762 |  0:04:12s\n",
      "epoch 145| loss: 0.82623 | val_0_logloss: 0.83776 |  0:04:14s\n",
      "epoch 146| loss: 0.82554 | val_0_logloss: 0.83765 |  0:04:16s\n",
      "epoch 147| loss: 0.82569 | val_0_logloss: 0.83844 |  0:04:18s\n",
      "epoch 148| loss: 0.82363 | val_0_logloss: 0.83664 |  0:04:20s\n",
      "epoch 149| loss: 0.82345 | val_0_logloss: 0.83501 |  0:04:21s\n",
      "epoch 150| loss: 0.82401 | val_0_logloss: 0.83528 |  0:04:23s\n",
      "epoch 151| loss: 0.82354 | val_0_logloss: 0.83562 |  0:04:25s\n",
      "epoch 152| loss: 0.82248 | val_0_logloss: 0.83565 |  0:04:27s\n",
      "epoch 153| loss: 0.82235 | val_0_logloss: 0.8357  |  0:04:29s\n",
      "epoch 154| loss: 0.82355 | val_0_logloss: 0.83485 |  0:04:31s\n",
      "epoch 155| loss: 0.82443 | val_0_logloss: 0.83545 |  0:04:33s\n",
      "epoch 156| loss: 0.82277 | val_0_logloss: 0.83604 |  0:04:35s\n",
      "epoch 157| loss: 0.82336 | val_0_logloss: 0.83287 |  0:04:37s\n",
      "epoch 158| loss: 0.82111 | val_0_logloss: 0.83263 |  0:04:38s\n",
      "epoch 159| loss: 0.82243 | val_0_logloss: 0.83612 |  0:04:40s\n",
      "epoch 160| loss: 0.82069 | val_0_logloss: 0.83444 |  0:04:42s\n",
      "epoch 161| loss: 0.82119 | val_0_logloss: 0.83339 |  0:04:44s\n",
      "epoch 162| loss: 0.82072 | val_0_logloss: 0.83608 |  0:04:46s\n",
      "epoch 163| loss: 0.8215  | val_0_logloss: 0.83723 |  0:04:48s\n",
      "epoch 164| loss: 0.82073 | val_0_logloss: 0.83649 |  0:04:50s\n",
      "epoch 165| loss: 0.82058 | val_0_logloss: 0.83563 |  0:04:52s\n",
      "epoch 166| loss: 0.82051 | val_0_logloss: 0.8347  |  0:04:54s\n",
      "epoch 167| loss: 0.82075 | val_0_logloss: 0.83393 |  0:04:55s\n",
      "epoch 168| loss: 0.82262 | val_0_logloss: 0.8345  |  0:04:57s\n",
      "epoch 169| loss: 0.82255 | val_0_logloss: 0.83493 |  0:04:59s\n",
      "epoch 170| loss: 0.8232  | val_0_logloss: 0.83406 |  0:05:01s\n",
      "epoch 171| loss: 0.82332 | val_0_logloss: 0.83263 |  0:05:03s\n",
      "epoch 172| loss: 0.82077 | val_0_logloss: 0.83187 |  0:05:04s\n",
      "epoch 173| loss: 0.82172 | val_0_logloss: 0.83062 |  0:05:07s\n",
      "epoch 174| loss: 0.81949 | val_0_logloss: 0.83103 |  0:05:09s\n",
      "epoch 175| loss: 0.81979 | val_0_logloss: 0.83042 |  0:05:11s\n",
      "epoch 176| loss: 0.81846 | val_0_logloss: 0.83069 |  0:05:13s\n",
      "epoch 177| loss: 0.81941 | val_0_logloss: 0.83015 |  0:05:15s\n",
      "epoch 178| loss: 0.81726 | val_0_logloss: 0.83161 |  0:05:16s\n",
      "epoch 179| loss: 0.81798 | val_0_logloss: 0.83156 |  0:05:19s\n",
      "epoch 180| loss: 0.81509 | val_0_logloss: 0.83269 |  0:05:22s\n",
      "epoch 181| loss: 0.81653 | val_0_logloss: 0.83316 |  0:05:24s\n",
      "epoch 182| loss: 0.81459 | val_0_logloss: 0.83333 |  0:05:25s\n",
      "epoch 183| loss: 0.81327 | val_0_logloss: 0.83419 |  0:05:27s\n",
      "epoch 184| loss: 0.81411 | val_0_logloss: 0.83364 |  0:05:29s\n",
      "epoch 185| loss: 0.81468 | val_0_logloss: 0.83559 |  0:05:31s\n",
      "epoch 186| loss: 0.81479 | val_0_logloss: 0.83611 |  0:05:32s\n",
      "epoch 187| loss: 0.81461 | val_0_logloss: 0.83537 |  0:05:34s\n",
      "epoch 188| loss: 0.81425 | val_0_logloss: 0.83308 |  0:05:35s\n",
      "epoch 189| loss: 0.81355 | val_0_logloss: 0.83211 |  0:05:37s\n",
      "epoch 190| loss: 0.8142  | val_0_logloss: 0.83127 |  0:05:39s\n",
      "epoch 191| loss: 0.8142  | val_0_logloss: 0.83209 |  0:05:41s\n",
      "epoch 192| loss: 0.81446 | val_0_logloss: 0.83233 |  0:05:42s\n",
      "epoch 193| loss: 0.81492 | val_0_logloss: 0.8339  |  0:05:44s\n",
      "epoch 194| loss: 0.81285 | val_0_logloss: 0.83596 |  0:05:47s\n",
      "epoch 195| loss: 0.81361 | val_0_logloss: 0.83667 |  0:05:48s\n",
      "epoch 196| loss: 0.81581 | val_0_logloss: 0.83613 |  0:05:50s\n",
      "epoch 197| loss: 0.81748 | val_0_logloss: 0.83373 |  0:05:52s\n",
      "epoch 198| loss: 0.81409 | val_0_logloss: 0.83172 |  0:05:54s\n",
      "epoch 199| loss: 0.81485 | val_0_logloss: 0.83159 |  0:05:56s\n",
      "epoch 200| loss: 0.81399 | val_0_logloss: 0.83113 |  0:05:58s\n",
      "epoch 201| loss: 0.81406 | val_0_logloss: 0.83202 |  0:05:59s\n",
      "epoch 202| loss: 0.81707 | val_0_logloss: 0.8318  |  0:06:01s\n",
      "epoch 203| loss: 0.8129  | val_0_logloss: 0.82945 |  0:06:03s\n",
      "epoch 204| loss: 0.81235 | val_0_logloss: 0.83074 |  0:06:05s\n",
      "epoch 205| loss: 0.81121 | val_0_logloss: 0.83162 |  0:06:06s\n",
      "epoch 206| loss: 0.81173 | val_0_logloss: 0.83267 |  0:06:08s\n",
      "epoch 207| loss: 0.81054 | val_0_logloss: 0.83348 |  0:06:10s\n",
      "epoch 208| loss: 0.81001 | val_0_logloss: 0.83439 |  0:06:11s\n",
      "epoch 209| loss: 0.80958 | val_0_logloss: 0.83479 |  0:06:13s\n",
      "epoch 210| loss: 0.80963 | val_0_logloss: 0.83406 |  0:06:15s\n",
      "epoch 211| loss: 0.80909 | val_0_logloss: 0.8337  |  0:06:17s\n",
      "epoch 212| loss: 0.80825 | val_0_logloss: 0.83286 |  0:06:19s\n",
      "epoch 213| loss: 0.80788 | val_0_logloss: 0.83183 |  0:06:21s\n",
      "epoch 214| loss: 0.80786 | val_0_logloss: 0.8316  |  0:06:23s\n",
      "epoch 215| loss: 0.80874 | val_0_logloss: 0.83376 |  0:06:25s\n",
      "epoch 216| loss: 0.8099  | val_0_logloss: 0.83573 |  0:06:27s\n",
      "epoch 217| loss: 0.81005 | val_0_logloss: 0.83578 |  0:06:29s\n",
      "epoch 218| loss: 0.80854 | val_0_logloss: 0.83421 |  0:06:31s\n",
      "epoch 219| loss: 0.80972 | val_0_logloss: 0.83469 |  0:06:33s\n",
      "epoch 220| loss: 0.80941 | val_0_logloss: 0.83307 |  0:06:35s\n",
      "epoch 221| loss: 0.80843 | val_0_logloss: 0.83118 |  0:06:36s\n",
      "epoch 222| loss: 0.80686 | val_0_logloss: 0.82963 |  0:06:39s\n",
      "epoch 223| loss: 0.80715 | val_0_logloss: 0.8293  |  0:06:41s\n",
      "epoch 224| loss: 0.80599 | val_0_logloss: 0.82955 |  0:06:43s\n",
      "epoch 225| loss: 0.80614 | val_0_logloss: 0.83039 |  0:06:45s\n",
      "epoch 226| loss: 0.80351 | val_0_logloss: 0.83003 |  0:06:47s\n",
      "epoch 227| loss: 0.80587 | val_0_logloss: 0.83031 |  0:06:48s\n",
      "epoch 228| loss: 0.80555 | val_0_logloss: 0.83062 |  0:06:50s\n",
      "epoch 229| loss: 0.80328 | val_0_logloss: 0.83121 |  0:06:51s\n",
      "epoch 230| loss: 0.80322 | val_0_logloss: 0.8316  |  0:06:54s\n",
      "epoch 231| loss: 0.80262 | val_0_logloss: 0.83273 |  0:06:56s\n",
      "epoch 232| loss: 0.80273 | val_0_logloss: 0.83086 |  0:06:58s\n",
      "epoch 233| loss: 0.8025  | val_0_logloss: 0.83133 |  0:07:00s\n",
      "epoch 234| loss: 0.80358 | val_0_logloss: 0.83168 |  0:07:02s\n",
      "epoch 235| loss: 0.80011 | val_0_logloss: 0.83263 |  0:07:03s\n",
      "epoch 236| loss: 0.80229 | val_0_logloss: 0.83411 |  0:07:05s\n",
      "epoch 237| loss: 0.79933 | val_0_logloss: 0.83495 |  0:07:07s\n",
      "epoch 238| loss: 0.79899 | val_0_logloss: 0.83564 |  0:07:09s\n",
      "epoch 239| loss: 0.79793 | val_0_logloss: 0.83461 |  0:07:10s\n",
      "epoch 240| loss: 0.79778 | val_0_logloss: 0.8342  |  0:07:12s\n",
      "epoch 241| loss: 0.79797 | val_0_logloss: 0.83406 |  0:07:14s\n",
      "epoch 242| loss: 0.7968  | val_0_logloss: 0.83445 |  0:07:15s\n",
      "epoch 243| loss: 0.79579 | val_0_logloss: 0.83423 |  0:07:17s\n",
      "epoch 244| loss: 0.79426 | val_0_logloss: 0.83294 |  0:07:18s\n",
      "epoch 245| loss: 0.79441 | val_0_logloss: 0.8302  |  0:07:20s\n",
      "epoch 246| loss: 0.7938  | val_0_logloss: 0.83049 |  0:07:22s\n",
      "epoch 247| loss: 0.79489 | val_0_logloss: 0.83022 |  0:07:23s\n",
      "epoch 248| loss: 0.79418 | val_0_logloss: 0.83135 |  0:07:25s\n",
      "epoch 249| loss: 0.7933  | val_0_logloss: 0.83059 |  0:07:27s\n",
      "epoch 250| loss: 0.79331 | val_0_logloss: 0.83206 |  0:07:28s\n",
      "epoch 251| loss: 0.79151 | val_0_logloss: 0.83325 |  0:07:30s\n",
      "epoch 252| loss: 0.79164 | val_0_logloss: 0.83431 |  0:07:33s\n",
      "epoch 253| loss: 0.79137 | val_0_logloss: 0.83422 |  0:07:35s\n",
      "epoch 254| loss: 0.79156 | val_0_logloss: 0.8329  |  0:07:37s\n",
      "epoch 255| loss: 0.79198 | val_0_logloss: 0.83231 |  0:07:39s\n",
      "epoch 256| loss: 0.78928 | val_0_logloss: 0.83175 |  0:07:41s\n",
      "epoch 257| loss: 0.79067 | val_0_logloss: 0.83366 |  0:07:43s\n",
      "epoch 258| loss: 0.79048 | val_0_logloss: 0.83617 |  0:07:45s\n",
      "epoch 259| loss: 0.79087 | val_0_logloss: 0.83634 |  0:07:47s\n",
      "epoch 260| loss: 0.79208 | val_0_logloss: 0.83487 |  0:07:48s\n",
      "epoch 261| loss: 0.7894  | val_0_logloss: 0.83249 |  0:07:50s\n",
      "epoch 262| loss: 0.78994 | val_0_logloss: 0.83371 |  0:07:52s\n",
      "epoch 263| loss: 0.79015 | val_0_logloss: 0.83044 |  0:07:54s\n",
      "epoch 264| loss: 0.78844 | val_0_logloss: 0.82991 |  0:07:56s\n",
      "epoch 265| loss: 0.78778 | val_0_logloss: 0.83058 |  0:07:58s\n",
      "epoch 266| loss: 0.78808 | val_0_logloss: 0.83272 |  0:08:00s\n",
      "epoch 267| loss: 0.78797 | val_0_logloss: 0.83351 |  0:08:02s\n",
      "epoch 268| loss: 0.78568 | val_0_logloss: 0.83464 |  0:08:04s\n",
      "epoch 269| loss: 0.78489 | val_0_logloss: 0.83314 |  0:08:05s\n",
      "epoch 270| loss: 0.78497 | val_0_logloss: 0.83079 |  0:08:08s\n",
      "epoch 271| loss: 0.78441 | val_0_logloss: 0.83068 |  0:08:11s\n",
      "epoch 272| loss: 0.78233 | val_0_logloss: 0.83185 |  0:08:14s\n",
      "epoch 273| loss: 0.7867  | val_0_logloss: 0.83034 |  0:08:17s\n",
      "epoch 274| loss: 0.78569 | val_0_logloss: 0.83229 |  0:08:20s\n",
      "epoch 275| loss: 0.78385 | val_0_logloss: 0.8319  |  0:08:22s\n",
      "epoch 276| loss: 0.78353 | val_0_logloss: 0.8313  |  0:08:25s\n",
      "epoch 277| loss: 0.78464 | val_0_logloss: 0.83314 |  0:08:28s\n",
      "epoch 278| loss: 0.78309 | val_0_logloss: 0.83316 |  0:08:30s\n",
      "epoch 279| loss: 0.78236 | val_0_logloss: 0.83085 |  0:08:32s\n",
      "epoch 280| loss: 0.78442 | val_0_logloss: 0.83119 |  0:08:33s\n",
      "epoch 281| loss: 0.78083 | val_0_logloss: 0.83079 |  0:08:35s\n",
      "epoch 282| loss: 0.7822  | val_0_logloss: 0.8308  |  0:08:37s\n",
      "epoch 283| loss: 0.78339 | val_0_logloss: 0.83052 |  0:08:39s\n",
      "\n",
      "Early stopping occurred at epoch 283 with best_epoch = 223 and best_val_0_logloss = 0.8293\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.multitask import TabNetMultiTaskClassifier\n",
    "\n",
    "clf = TabNetMultiTaskClassifier(\n",
    "    n_d=16, n_a=16, n_steps=5,\n",
    "    lambda_sparse=1e-2,\n",
    "    gamma = 1.3,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    scheduler_params = {\"gamma\": 0.95, \"step_size\": 20},\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    mask_type='entmax',\n",
    "    device_name='cuda'\n",
    ")\n",
    "clf.fit(\n",
    "    X_train = X_train.values, y_train = np.array(Y_train).reshape(Y_train.shape[0],1),\n",
    "    eval_set = [(X_val.values, np.array(Y_val).reshape(Y_val.shape[0],1))],\n",
    "    max_epochs=300,\n",
    "    patience=60,\n",
    "    batch_size=8192,\n",
    "    virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x275398d7460>]"
      ]
     },
     "metadata": {},
     "execution_count": 51
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 378.465625 248.518125\" width=\"378.465625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-05-11T01:13:41.756956</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.4.1, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 378.465625 248.518125 \r\nL 378.465625 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 371.265625 224.64 \r\nL 371.265625 7.2 \r\nL 36.465625 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m6e8954b22a\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.683807\" xlink:href=\"#m6e8954b22a\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(48.502557 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 4250 \r\nQ 1547 4250 1301 3770 \r\nQ 1056 3291 1056 2328 \r\nQ 1056 1369 1301 889 \r\nQ 1547 409 2034 409 \r\nQ 2525 409 2770 889 \r\nQ 3016 1369 3016 2328 \r\nQ 3016 3291 2770 3770 \r\nQ 2525 4250 2034 4250 \r\nz\r\nM 2034 4750 \r\nQ 2819 4750 3233 4129 \r\nQ 3647 3509 3647 2328 \r\nQ 3647 1150 3233 529 \r\nQ 2819 -91 2034 -91 \r\nQ 1250 -91 836 529 \r\nQ 422 1150 422 2328 \r\nQ 422 3509 836 4129 \r\nQ 1250 4750 2034 4750 \r\nz\r\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"106.425468\" xlink:href=\"#m6e8954b22a\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 50 -->\r\n      <g transform=\"translate(100.062968 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 691 4666 \r\nL 3169 4666 \r\nL 3169 4134 \r\nL 1269 4134 \r\nL 1269 2991 \r\nQ 1406 3038 1543 3061 \r\nQ 1681 3084 1819 3084 \r\nQ 2600 3084 3056 2656 \r\nQ 3513 2228 3513 1497 \r\nQ 3513 744 3044 326 \r\nQ 2575 -91 1722 -91 \r\nQ 1428 -91 1123 -41 \r\nQ 819 9 494 109 \r\nL 494 744 \r\nQ 775 591 1075 516 \r\nQ 1375 441 1709 441 \r\nQ 2250 441 2565 725 \r\nQ 2881 1009 2881 1497 \r\nQ 2881 1984 2565 2268 \r\nQ 2250 2553 1709 2553 \r\nQ 1456 2553 1204 2497 \r\nQ 953 2441 691 2322 \r\nL 691 4666 \r\nz\r\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"161.167129\" xlink:href=\"#m6e8954b22a\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 100 -->\r\n      <g transform=\"translate(151.623379 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 794 531 \r\nL 1825 531 \r\nL 1825 4091 \r\nL 703 3866 \r\nL 703 4441 \r\nL 1819 4666 \r\nL 2450 4666 \r\nL 2450 531 \r\nL 3481 531 \r\nL 3481 0 \r\nL 794 0 \r\nL 794 531 \r\nz\r\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"215.90879\" xlink:href=\"#m6e8954b22a\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 150 -->\r\n      <g transform=\"translate(206.36504 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"270.650452\" xlink:href=\"#m6e8954b22a\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 200 -->\r\n      <g transform=\"translate(261.106702 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 1228 531 \r\nL 3431 531 \r\nL 3431 0 \r\nL 469 0 \r\nL 469 531 \r\nQ 828 903 1448 1529 \r\nQ 2069 2156 2228 2338 \r\nQ 2531 2678 2651 2914 \r\nQ 2772 3150 2772 3378 \r\nQ 2772 3750 2511 3984 \r\nQ 2250 4219 1831 4219 \r\nQ 1534 4219 1204 4116 \r\nQ 875 4013 500 3803 \r\nL 500 4441 \r\nQ 881 4594 1212 4672 \r\nQ 1544 4750 1819 4750 \r\nQ 2544 4750 2975 4387 \r\nQ 3406 4025 3406 3419 \r\nQ 3406 3131 3298 2873 \r\nQ 3191 2616 2906 2266 \r\nQ 2828 2175 2409 1742 \r\nQ 1991 1309 1228 531 \r\nz\r\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"325.392113\" xlink:href=\"#m6e8954b22a\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 250 -->\r\n      <g transform=\"translate(315.848363 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m22428ae0b9\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m22428ae0b9\" y=\"196.958514\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0.80 -->\r\n      <g transform=\"translate(7.2 200.757733)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 684 794 \r\nL 1344 794 \r\nL 1344 0 \r\nL 684 0 \r\nL 684 794 \r\nz\r\n\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\r\n        <path d=\"M 2034 2216 \r\nQ 1584 2216 1326 1975 \r\nQ 1069 1734 1069 1313 \r\nQ 1069 891 1326 650 \r\nQ 1584 409 2034 409 \r\nQ 2484 409 2743 651 \r\nQ 3003 894 3003 1313 \r\nQ 3003 1734 2745 1975 \r\nQ 2488 2216 2034 2216 \r\nz\r\nM 1403 2484 \r\nQ 997 2584 770 2862 \r\nQ 544 3141 544 3541 \r\nQ 544 4100 942 4425 \r\nQ 1341 4750 2034 4750 \r\nQ 2731 4750 3128 4425 \r\nQ 3525 4100 3525 3541 \r\nQ 3525 3141 3298 2862 \r\nQ 3072 2584 2669 2484 \r\nQ 3125 2378 3379 2068 \r\nQ 3634 1759 3634 1313 \r\nQ 3634 634 3220 271 \r\nQ 2806 -91 2034 -91 \r\nQ 1263 -91 848 271 \r\nQ 434 634 434 1313 \r\nQ 434 1759 690 2068 \r\nQ 947 2378 1403 2484 \r\nz\r\nM 1172 3481 \r\nQ 1172 3119 1398 2916 \r\nQ 1625 2713 2034 2713 \r\nQ 2441 2713 2670 2916 \r\nQ 2900 3119 2900 3481 \r\nQ 2900 3844 2670 4047 \r\nQ 2441 4250 2034 4250 \r\nQ 1625 4250 1398 4047 \r\nQ 1172 3844 1172 3481 \r\nz\r\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m22428ae0b9\" y=\"150.545599\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.85 -->\r\n      <g transform=\"translate(7.2 154.344818)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m22428ae0b9\" y=\"104.132684\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.90 -->\r\n      <g transform=\"translate(7.2 107.931903)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 703 97 \r\nL 703 672 \r\nQ 941 559 1184 500 \r\nQ 1428 441 1663 441 \r\nQ 2288 441 2617 861 \r\nQ 2947 1281 2994 2138 \r\nQ 2813 1869 2534 1725 \r\nQ 2256 1581 1919 1581 \r\nQ 1219 1581 811 2004 \r\nQ 403 2428 403 3163 \r\nQ 403 3881 828 4315 \r\nQ 1253 4750 1959 4750 \r\nQ 2769 4750 3195 4129 \r\nQ 3622 3509 3622 2328 \r\nQ 3622 1225 3098 567 \r\nQ 2575 -91 1691 -91 \r\nQ 1453 -91 1209 -44 \r\nQ 966 3 703 97 \r\nz\r\nM 1959 2075 \r\nQ 2384 2075 2632 2365 \r\nQ 2881 2656 2881 3163 \r\nQ 2881 3666 2632 3958 \r\nQ 2384 4250 1959 4250 \r\nQ 1534 4250 1286 3958 \r\nQ 1038 3666 1038 3163 \r\nQ 1038 2656 1286 2365 \r\nQ 1534 2075 1959 2075 \r\nz\r\n\" id=\"DejaVuSans-39\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-39\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m22428ae0b9\" y=\"57.71977\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.95 -->\r\n      <g transform=\"translate(7.2 61.518988)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-39\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m22428ae0b9\" y=\"11.306855\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 1.00 -->\r\n      <g transform=\"translate(7.2 15.106074)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_12\">\r\n    <path clip-path=\"url(#paca74df3fa)\" d=\"M 51.683807 87.888312 \r\nL 52.77864 98.08216 \r\nL 53.873473 106.691575 \r\nL 57.157973 116.559969 \r\nL 58.252806 118.151891 \r\nL 59.347639 118.851791 \r\nL 60.442473 121.676411 \r\nL 61.537306 123.085144 \r\nL 62.632139 121.867924 \r\nL 63.726972 124.171822 \r\nL 64.821806 126.128344 \r\nL 65.916639 127.634106 \r\nL 67.011472 127.981881 \r\nL 68.106305 130.096808 \r\nL 69.201138 130.098405 \r\nL 70.295972 133.606555 \r\nL 72.485638 135.019338 \r\nL 73.580471 136.845268 \r\nL 74.675305 136.330347 \r\nL 75.770138 139.342518 \r\nL 76.864971 139.634268 \r\nL 77.959804 140.440129 \r\nL 79.054637 140.773241 \r\nL 80.149471 141.901286 \r\nL 81.244304 144.81769 \r\nL 82.339137 143.365592 \r\nL 83.43397 144.784432 \r\nL 84.528804 145.986261 \r\nL 85.623637 146.774718 \r\nL 86.71847 149.886625 \r\nL 87.813303 148.843271 \r\nL 88.908136 150.242682 \r\nL 90.00297 150.421028 \r\nL 91.097803 149.673851 \r\nL 92.192636 150.20659 \r\nL 93.287469 151.809869 \r\nL 94.382303 151.645357 \r\nL 95.477136 152.42926 \r\nL 96.571969 152.21668 \r\nL 97.666802 151.537409 \r\nL 98.761635 152.18913 \r\nL 99.856469 152.33632 \r\nL 102.046135 153.291437 \r\nL 103.140968 153.718057 \r\nL 104.235802 154.509541 \r\nL 105.330635 154.657943 \r\nL 106.425468 155.190399 \r\nL 107.520301 156.532439 \r\nL 108.615134 154.728755 \r\nL 110.804801 155.642619 \r\nL 111.899634 156.449474 \r\nL 112.994467 156.303642 \r\nL 114.089301 157.147751 \r\nL 115.184134 156.167533 \r\nL 116.278967 156.863478 \r\nL 117.3738 157.130905 \r\nL 118.468634 158.304263 \r\nL 119.563467 157.324176 \r\nL 120.6583 159.174253 \r\nL 121.753133 156.688976 \r\nL 122.847966 157.906212 \r\nL 123.9428 156.554299 \r\nL 125.037633 158.165719 \r\nL 127.227299 158.915862 \r\nL 128.322133 158.092216 \r\nL 129.416966 159.742124 \r\nL 130.511799 158.686731 \r\nL 132.701465 160.236369 \r\nL 133.796299 159.529839 \r\nL 134.891132 160.774514 \r\nL 135.985965 161.749499 \r\nL 137.080798 161.429994 \r\nL 138.175632 162.249767 \r\nL 139.270465 161.469046 \r\nL 140.365298 161.016003 \r\nL 141.460131 161.752817 \r\nL 143.649798 162.060912 \r\nL 144.744631 162.783192 \r\nL 145.839464 163.963253 \r\nL 146.934297 163.767316 \r\nL 148.029131 162.255908 \r\nL 149.123964 163.713591 \r\nL 150.218797 164.107884 \r\nL 151.31363 163.958372 \r\nL 152.408463 163.696451 \r\nL 153.503297 164.701836 \r\nL 154.59813 165.457141 \r\nL 155.692963 165.524966 \r\nL 156.787796 166.198795 \r\nL 157.88263 165.074711 \r\nL 158.977463 166.980229 \r\nL 160.072296 166.066455 \r\nL 161.167129 166.125007 \r\nL 162.261962 166.065931 \r\nL 163.356796 165.775405 \r\nL 164.451629 166.541704 \r\nL 165.546462 167.826194 \r\nL 166.641295 167.987587 \r\nL 167.736129 168.304095 \r\nL 168.830962 168.03124 \r\nL 169.925795 168.697047 \r\nL 171.020628 169.60977 \r\nL 172.115461 169.018652 \r\nL 173.210295 169.361881 \r\nL 174.305128 169.536021 \r\nL 175.399961 169.076605 \r\nL 176.494794 169.491261 \r\nL 179.779294 170.151303 \r\nL 180.874127 168.869122 \r\nL 181.968961 170.048387 \r\nL 183.063794 169.070573 \r\nL 184.158627 169.456533 \r\nL 185.25346 170.276036 \r\nL 186.348293 170.952428 \r\nL 187.443127 170.543345 \r\nL 188.53796 171.871558 \r\nL 189.632793 172.361501 \r\nL 190.727626 172.700626 \r\nL 191.82246 174.168211 \r\nL 192.917293 173.332302 \r\nL 194.012126 173.183943 \r\nL 195.106959 173.657258 \r\nL 196.201792 174.281816 \r\nL 197.296626 173.727275 \r\nL 198.391459 173.774027 \r\nL 199.486292 173.256302 \r\nL 200.581125 174.007046 \r\nL 201.675959 173.573347 \r\nL 202.770792 173.330083 \r\nL 203.865625 172.8774 \r\nL 204.960458 172.609516 \r\nL 206.055291 173.248216 \r\nL 207.150125 173.109417 \r\nL 208.244958 175.023662 \r\nL 209.339791 175.193613 \r\nL 210.434624 174.670001 \r\nL 211.529458 175.109341 \r\nL 212.624291 176.094711 \r\nL 213.719124 176.20763 \r\nL 214.813957 175.099296 \r\nL 215.90879 174.282493 \r\nL 217.003624 175.820368 \r\nL 218.098457 175.273241 \r\nL 219.19329 177.367521 \r\nL 220.288123 176.141948 \r\nL 221.382957 177.753365 \r\nL 222.47779 177.291247 \r\nL 223.572623 177.723324 \r\nL 224.667456 177.002499 \r\nL 225.762289 177.713822 \r\nL 227.951956 177.920558 \r\nL 229.046789 177.698711 \r\nL 230.141622 175.964137 \r\nL 231.236456 176.023786 \r\nL 232.331289 175.425189 \r\nL 233.426122 175.308945 \r\nL 234.520955 177.677791 \r\nL 235.615789 176.800053 \r\nL 236.710622 178.867945 \r\nL 237.805455 178.591695 \r\nL 238.900288 179.820303 \r\nL 239.995121 178.942132 \r\nL 241.089955 180.94064 \r\nL 242.184788 180.270117 \r\nL 243.279621 182.952095 \r\nL 244.374454 181.613652 \r\nL 245.469288 183.412286 \r\nL 246.564121 184.644599 \r\nL 247.658954 183.863122 \r\nL 248.753787 183.335848 \r\nL 249.84862 183.231657 \r\nL 250.943454 183.39877 \r\nL 252.038287 183.731172 \r\nL 253.13312 184.385035 \r\nL 254.227953 183.779859 \r\nL 255.322787 183.78028 \r\nL 256.41762 183.535486 \r\nL 257.512453 183.108064 \r\nL 258.607286 185.026716 \r\nL 259.702119 184.325155 \r\nL 260.796953 182.285648 \r\nL 261.891786 180.733537 \r\nL 262.986619 183.879666 \r\nL 264.081452 183.178006 \r\nL 265.176286 183.974556 \r\nL 266.271119 183.90878 \r\nL 267.365952 181.115021 \r\nL 268.460785 184.988162 \r\nL 269.555618 185.496421 \r\nL 270.650452 186.550892 \r\nL 271.745285 186.071252 \r\nL 272.840118 187.178174 \r\nL 275.029785 188.065355 \r\nL 276.124618 188.021097 \r\nL 277.219451 188.522361 \r\nL 278.314284 189.29641 \r\nL 279.409117 189.640414 \r\nL 280.503951 189.662665 \r\nL 281.598784 188.850082 \r\nL 282.693617 187.766467 \r\nL 283.78845 187.628204 \r\nL 284.883284 189.034958 \r\nL 285.978117 187.936509 \r\nL 287.07295 188.224966 \r\nL 288.167783 189.131331 \r\nL 289.262616 190.591292 \r\nL 290.35745 190.325326 \r\nL 291.452283 191.402347 \r\nL 292.547116 191.261458 \r\nL 293.641949 193.698908 \r\nL 294.736783 191.512039 \r\nL 295.831616 191.802061 \r\nL 296.926449 193.912826 \r\nL 298.021282 193.973577 \r\nL 299.116116 194.522948 \r\nL 300.210949 194.421447 \r\nL 301.305782 194.640981 \r\nL 302.400615 193.633724 \r\nL 303.495448 196.852434 \r\nL 304.590282 194.829278 \r\nL 305.685115 197.583472 \r\nL 306.779948 197.898642 \r\nL 307.874781 198.880873 \r\nL 308.969615 199.015201 \r\nL 310.064448 198.84388 \r\nL 312.254114 200.864485 \r\nL 313.348947 202.28439 \r\nL 314.443781 202.147009 \r\nL 315.538614 202.709372 \r\nL 316.633447 201.703829 \r\nL 317.72828 202.364678 \r\nL 318.823114 203.17533 \r\nL 319.917947 203.166223 \r\nL 321.01278 204.83691 \r\nL 322.107613 204.722777 \r\nL 323.202446 204.973082 \r\nL 324.29728 204.793031 \r\nL 325.392113 204.400702 \r\nL 326.486946 206.909648 \r\nL 327.581779 205.621426 \r\nL 328.676613 205.796352 \r\nL 329.771446 205.437397 \r\nL 330.866279 204.308484 \r\nL 331.961112 206.79621 \r\nL 333.055945 206.292386 \r\nL 334.150779 206.097254 \r\nL 335.245612 207.691559 \r\nL 336.340445 208.302673 \r\nL 337.435278 208.026559 \r\nL 338.530112 208.127135 \r\nL 339.624945 210.247944 \r\nL 340.719778 210.984491 \r\nL 341.814611 210.914231 \r\nL 342.909444 211.42559 \r\nL 344.004278 213.359638 \r\nL 345.099111 209.302874 \r\nL 346.193944 210.240487 \r\nL 347.288777 211.948658 \r\nL 348.383611 212.249334 \r\nL 349.478444 211.217316 \r\nL 350.573277 212.652475 \r\nL 351.66811 213.332056 \r\nL 352.762944 211.4185 \r\nL 353.857777 214.756364 \r\nL 356.047443 212.372477 \r\nL 356.047443 212.372477 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_13\">\r\n    <path clip-path=\"url(#paca74df3fa)\" d=\"M 51.683807 27.841103 \r\nL 52.77864 19.925871 \r\nL 53.873473 17.083636 \r\nL 54.968306 28.595824 \r\nL 56.06314 47.360313 \r\nL 58.252806 70.414868 \r\nL 59.347639 82.95539 \r\nL 60.442473 89.077773 \r\nL 61.537306 92.964938 \r\nL 62.632139 100.791577 \r\nL 63.726972 105.348417 \r\nL 65.916639 116.64266 \r\nL 67.011472 120.565731 \r\nL 68.106305 121.902583 \r\nL 69.201138 123.049513 \r\nL 70.295972 125.477027 \r\nL 71.390805 130.027129 \r\nL 72.485638 131.03447 \r\nL 73.580471 131.49295 \r\nL 74.675305 132.89139 \r\nL 75.770138 132.633366 \r\nL 76.864971 133.92327 \r\nL 77.959804 135.396733 \r\nL 79.054637 130.592049 \r\nL 80.149471 138.65087 \r\nL 81.244304 138.57677 \r\nL 82.339137 138.874857 \r\nL 83.43397 140.327647 \r\nL 84.528804 139.579332 \r\nL 85.623637 142.499114 \r\nL 86.71847 139.272491 \r\nL 88.908136 138.498579 \r\nL 90.00297 133.508366 \r\nL 91.097803 135.889637 \r\nL 92.192636 141.074885 \r\nL 93.287469 138.017966 \r\nL 94.382303 137.604263 \r\nL 95.477136 142.220555 \r\nL 96.571969 136.470882 \r\nL 97.666802 135.757939 \r\nL 98.761635 137.458087 \r\nL 99.856469 141.545281 \r\nL 100.951302 139.619884 \r\nL 102.046135 136.881059 \r\nL 103.140968 143.803477 \r\nL 104.235802 145.209729 \r\nL 105.330635 144.932199 \r\nL 106.425468 148.373642 \r\nL 107.520301 150.659068 \r\nL 108.615134 145.987765 \r\nL 109.709968 146.367993 \r\nL 110.804801 152.468862 \r\nL 111.899634 149.368354 \r\nL 112.994467 149.3112 \r\nL 114.089301 150.001753 \r\nL 115.184134 153.961531 \r\nL 116.278967 154.93065 \r\nL 118.468634 153.831145 \r\nL 119.563467 153.840925 \r\nL 120.6583 152.360505 \r\nL 121.753133 153.395401 \r\nL 122.847966 156.484509 \r\nL 123.9428 153.669787 \r\nL 125.037633 153.793433 \r\nL 126.132466 155.03826 \r\nL 127.227299 155.722543 \r\nL 128.322133 153.551393 \r\nL 129.416966 156.60325 \r\nL 130.511799 155.194606 \r\nL 131.606632 155.893059 \r\nL 132.701465 154.847011 \r\nL 133.796299 152.297164 \r\nL 134.891132 156.892458 \r\nL 135.985965 158.928722 \r\nL 137.080798 155.496314 \r\nL 138.175632 156.603219 \r\nL 139.270465 158.020704 \r\nL 140.365298 157.301503 \r\nL 141.460131 155.232771 \r\nL 142.554964 157.756246 \r\nL 143.649798 158.43456 \r\nL 144.744631 157.945829 \r\nL 145.839464 158.149526 \r\nL 146.934297 157.216795 \r\nL 148.029131 159.21134 \r\nL 149.123964 159.499997 \r\nL 150.218797 156.191973 \r\nL 151.31363 157.537323 \r\nL 152.408463 157.367274 \r\nL 153.503297 159.316997 \r\nL 154.59813 159.484967 \r\nL 155.692963 157.908579 \r\nL 156.787796 159.633208 \r\nL 157.88263 160.634116 \r\nL 158.977463 158.898535 \r\nL 160.072296 161.158842 \r\nL 161.167129 163.739033 \r\nL 162.261962 162.864419 \r\nL 163.356796 160.300999 \r\nL 164.451629 161.668756 \r\nL 165.546462 161.020405 \r\nL 166.641295 161.252391 \r\nL 167.736129 162.620433 \r\nL 168.830962 162.84914 \r\nL 169.925795 162.760669 \r\nL 171.020628 159.460988 \r\nL 172.115461 157.778868 \r\nL 173.210295 161.856946 \r\nL 174.305128 161.710711 \r\nL 175.399961 162.304287 \r\nL 176.494794 162.311993 \r\nL 177.589628 161.955948 \r\nL 178.684461 161.363981 \r\nL 180.874127 159.788284 \r\nL 181.968961 159.602862 \r\nL 183.063794 159.244747 \r\nL 184.158627 160.360964 \r\nL 185.25346 161.081435 \r\nL 186.348293 162.376133 \r\nL 187.443127 162.246082 \r\nL 188.53796 162.437463 \r\nL 189.632793 162.441267 \r\nL 190.727626 163.947826 \r\nL 192.917293 161.794793 \r\nL 194.012126 160.528862 \r\nL 195.106959 161.033928 \r\nL 196.201792 161.281951 \r\nL 198.391459 161.111474 \r\nL 199.486292 161.995965 \r\nL 200.581125 163.869592 \r\nL 201.675959 162.969308 \r\nL 202.770792 162.691435 \r\nL 203.865625 162.038288 \r\nL 204.960458 161.907368 \r\nL 206.055291 162.014152 \r\nL 207.150125 161.273734 \r\nL 209.339791 164.456582 \r\nL 211.529458 163.897883 \r\nL 213.719124 163.820668 \r\nL 214.813957 164.612857 \r\nL 217.003624 163.508028 \r\nL 218.098457 166.450807 \r\nL 219.19329 166.6658 \r\nL 220.288123 163.428123 \r\nL 221.382957 164.989708 \r\nL 222.47779 165.966601 \r\nL 223.572623 163.468017 \r\nL 224.667456 162.396808 \r\nL 226.857123 163.885845 \r\nL 227.951956 164.746171 \r\nL 229.046789 165.465139 \r\nL 230.141622 164.930781 \r\nL 231.236456 164.529934 \r\nL 232.331289 165.343988 \r\nL 233.426122 166.669272 \r\nL 234.520955 167.378242 \r\nL 235.615789 168.5334 \r\nL 236.710622 168.158993 \r\nL 237.805455 168.718559 \r\nL 238.900288 168.473565 \r\nL 239.995121 168.970428 \r\nL 241.089955 167.613066 \r\nL 242.184788 167.664069 \r\nL 243.279621 166.617214 \r\nL 244.374454 166.172837 \r\nL 245.469288 166.021224 \r\nL 246.564121 165.218956 \r\nL 247.658954 165.731077 \r\nL 248.753787 163.924467 \r\nL 249.84862 163.43618 \r\nL 250.943454 164.126242 \r\nL 252.038287 166.250268 \r\nL 254.227953 167.934649 \r\nL 255.322787 167.173807 \r\nL 256.41762 166.948649 \r\nL 257.512453 165.490592 \r\nL 258.607286 163.577323 \r\nL 259.702119 162.918396 \r\nL 260.796953 163.41807 \r\nL 261.891786 165.646421 \r\nL 262.986619 167.511884 \r\nL 264.081452 167.63757 \r\nL 265.176286 168.062208 \r\nL 266.271119 167.237295 \r\nL 267.365952 167.443681 \r\nL 268.460785 169.621854 \r\nL 269.555618 168.425143 \r\nL 270.650452 167.607218 \r\nL 271.745285 166.630912 \r\nL 273.934951 165.03629 \r\nL 275.029785 164.660273 \r\nL 276.124618 165.340719 \r\nL 277.219451 165.679069 \r\nL 278.314284 166.460435 \r\nL 279.409117 167.416469 \r\nL 280.503951 167.627884 \r\nL 282.693617 163.794424 \r\nL 283.78845 163.742888 \r\nL 284.883284 165.201637 \r\nL 285.978117 164.759303 \r\nL 287.07295 166.262244 \r\nL 288.167783 168.015287 \r\nL 289.262616 169.458742 \r\nL 290.35745 169.760634 \r\nL 291.452283 169.52621 \r\nL 292.547116 168.744406 \r\nL 293.641949 169.086348 \r\nL 295.831616 168.53571 \r\nL 296.926449 167.988785 \r\nL 298.021282 167.628475 \r\nL 299.116116 166.574185 \r\nL 300.210949 168.313252 \r\nL 302.400615 167.546797 \r\nL 303.495448 166.665652 \r\nL 304.590282 165.294072 \r\nL 305.685115 164.511983 \r\nL 306.779948 163.874685 \r\nL 307.874781 164.834647 \r\nL 308.969615 165.21224 \r\nL 310.064448 165.343675 \r\nL 311.159281 164.982649 \r\nL 312.254114 165.188045 \r\nL 313.348947 166.381503 \r\nL 314.443781 168.922479 \r\nL 315.538614 168.658375 \r\nL 316.633447 168.902489 \r\nL 317.72828 167.858454 \r\nL 318.823114 168.564254 \r\nL 319.917947 167.201017 \r\nL 322.107613 165.109602 \r\nL 323.202446 165.191332 \r\nL 324.29728 166.414721 \r\nL 326.486946 167.487584 \r\nL 327.581779 165.709947 \r\nL 328.676613 163.383735 \r\nL 329.771446 163.227463 \r\nL 330.866279 164.593017 \r\nL 331.961112 166.802867 \r\nL 333.055945 165.666541 \r\nL 334.150779 168.702771 \r\nL 335.245612 169.198268 \r\nL 336.340445 168.574471 \r\nL 337.435278 166.58576 \r\nL 338.530112 165.854575 \r\nL 339.624945 164.802145 \r\nL 340.719778 166.195382 \r\nL 341.814611 168.375733 \r\nL 342.909444 168.478078 \r\nL 344.004278 167.396047 \r\nL 345.099111 168.793926 \r\nL 346.193944 166.989691 \r\nL 347.288777 167.34945 \r\nL 348.383611 167.90246 \r\nL 349.478444 166.199969 \r\nL 350.573277 166.176486 \r\nL 351.66811 168.321653 \r\nL 352.762944 168.003853 \r\nL 353.857777 168.373295 \r\nL 354.95261 168.370924 \r\nL 356.047443 168.627386 \r\nL 356.047443 168.627386 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 36.465625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 371.265625 224.64 \r\nL 371.265625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 371.265625 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 36.465625 7.2 \r\nL 371.265625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"paca74df3fa\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"36.465625\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "#clf.history['loss']\n",
    "plt.plot(clf.history['loss'][5:])\n",
    "plt.plot(clf.history['val_0_logloss'][5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = clf.predict_proba(X_val.values)\n",
    "predict = np.array(predict)\n",
    "predict = pd.DataFrame(predict.reshape(predict.shape[1],3), columns=[0.0, 1.0, 2.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8292999066046411"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "log_loss(y_val_onehot, predict)"
   ]
  },
  {
   "source": [
    "* n_d=8, n_a=8, n_steps=5, lambda_sparse=1e-1, gamma = 1.3, scheduler_params = {\"gamma\": 0.95, \"step_size\": 20} = **0.8380463815163395**\n",
    "* n_d=8, n_a=8, n_steps=5, lambda_sparse=1e-2, gamma = 1.3, scheduler_params = {\"gamma\": 0.95, \"step_size\": 20} = **0.8327229226728547**\n",
    "* n_d=8, n_a=8, n_steps=5, lambda_sparse=1e-3, gamma = 1.3, scheduler_params = {\"gamma\": 0.95, \"step_size\": 20} = **0.8373788992342593**\n",
    "* n_d=16, n_a=16, n_steps=5, lambda_sparse=1e-2, gamma = 1.3, scheduler_params = {\"gamma\": 0.95, \"step_size\": 20} = **0.8265579702642575**\n",
    "* 나머지 hyperparameter에 대해서는 logloss가 잘 나오지 않았음. 추후 이것저것 더 해볼 예정"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.00882648, 0.03877237, 0.04557721, 0.11152691, 0.01798807,\n",
       "       0.0165193 , 0.03604957, 0.07089741, 0.02188411, 0.04779797,\n",
       "       0.08276292, 0.02145794, 0.07950223, 0.05027626, 0.03550007,\n",
       "       0.07715966, 0.23750152])"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "source": [
    "### 6_1) unsupervised pretraining test\n",
    "* 논문에서 unsupervised training시 효과가 더 좋다고 나와있고, TabNet을 구현한 github repo에서도 해당 기능을 만들어놨는데, 아직까지는 별로 효과가 없어보임."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device used : cuda\n",
      "epoch 0  | loss: 33618089.82115| val_0_unsup_loss: 14424186.0|  0:00:02s\n",
      "epoch 1  | loss: 2333663.33207| val_0_unsup_loss: 3216637.25|  0:00:04s\n",
      "epoch 2  | loss: 889025.13432| val_0_unsup_loss: 1832241.375|  0:00:07s\n",
      "epoch 3  | loss: 234650.33455| val_0_unsup_loss: 3065469.5|  0:00:09s\n",
      "epoch 4  | loss: 135668.29539| val_0_unsup_loss: 2716994.0|  0:00:11s\n",
      "epoch 5  | loss: 76708.91889| val_0_unsup_loss: 5317295.0|  0:00:14s\n",
      "epoch 6  | loss: 61302.79769| val_0_unsup_loss: 6522936.0|  0:00:16s\n",
      "epoch 7  | loss: 46479.54278| val_0_unsup_loss: 8887041.0|  0:00:19s\n",
      "epoch 8  | loss: 59282.07907| val_0_unsup_loss: 13044147.0|  0:00:21s\n",
      "epoch 9  | loss: 80179.28277| val_0_unsup_loss: 11875471.0|  0:00:23s\n",
      "epoch 10 | loss: 85045.88515| val_0_unsup_loss: 25205670.0|  0:00:26s\n",
      "epoch 11 | loss: 60058.42732| val_0_unsup_loss: 23481978.0|  0:00:28s\n",
      "epoch 12 | loss: 57738.88439| val_0_unsup_loss: 15810113.0|  0:00:30s\n",
      "\n",
      "Early stopping occurred at epoch 12 with best_epoch = 2 and best_val_0_unsup_loss = 1832241.375\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cuda\n",
      "Loading weights from unsupervised pretraining\n",
      "epoch 0  | loss: 1.45015 | val_0_logloss: 1.5953  |  0:00:01s\n",
      "epoch 1  | loss: 0.89808 | val_0_logloss: 1.04891 |  0:00:03s\n",
      "epoch 2  | loss: 0.88465 | val_0_logloss: 0.99275 |  0:00:05s\n",
      "epoch 3  | loss: 0.8768  | val_0_logloss: 0.95187 |  0:00:07s\n",
      "epoch 4  | loss: 0.8693  | val_0_logloss: 0.92826 |  0:00:09s\n",
      "epoch 5  | loss: 0.86274 | val_0_logloss: 0.88435 |  0:00:11s\n",
      "epoch 6  | loss: 0.85916 | val_0_logloss: 0.87779 |  0:00:13s\n",
      "epoch 7  | loss: 0.85708 | val_0_logloss: 0.8764  |  0:00:14s\n",
      "epoch 8  | loss: 0.85575 | val_0_logloss: 0.8605  |  0:00:16s\n",
      "epoch 9  | loss: 0.85496 | val_0_logloss: 0.85847 |  0:00:18s\n",
      "epoch 10 | loss: 0.85256 | val_0_logloss: 0.85326 |  0:00:20s\n",
      "epoch 11 | loss: 0.85147 | val_0_logloss: 0.8525  |  0:00:22s\n",
      "epoch 12 | loss: 0.8508  | val_0_logloss: 0.84719 |  0:00:24s\n",
      "epoch 13 | loss: 0.8511  | val_0_logloss: 0.84723 |  0:00:26s\n",
      "epoch 14 | loss: 0.85002 | val_0_logloss: 0.84846 |  0:00:28s\n",
      "epoch 15 | loss: 0.85018 | val_0_logloss: 0.84988 |  0:00:30s\n",
      "epoch 16 | loss: 0.85212 | val_0_logloss: 0.85336 |  0:00:31s\n",
      "epoch 17 | loss: 0.85088 | val_0_logloss: 0.8474  |  0:00:33s\n",
      "epoch 18 | loss: 0.8477  | val_0_logloss: 0.8456  |  0:00:35s\n",
      "epoch 19 | loss: 0.84859 | val_0_logloss: 0.84781 |  0:00:37s\n",
      "epoch 20 | loss: 0.84965 | val_0_logloss: 0.84786 |  0:00:39s\n",
      "epoch 21 | loss: 0.84937 | val_0_logloss: 0.84779 |  0:00:41s\n",
      "epoch 22 | loss: 0.8482  | val_0_logloss: 0.84525 |  0:00:43s\n",
      "epoch 23 | loss: 0.84757 | val_0_logloss: 0.84395 |  0:00:45s\n",
      "epoch 24 | loss: 0.84878 | val_0_logloss: 0.84472 |  0:00:46s\n",
      "epoch 25 | loss: 0.84625 | val_0_logloss: 0.84538 |  0:00:48s\n",
      "epoch 26 | loss: 0.84527 | val_0_logloss: 0.84491 |  0:00:50s\n",
      "epoch 27 | loss: 0.84469 | val_0_logloss: 0.84391 |  0:00:52s\n",
      "epoch 28 | loss: 0.84404 | val_0_logloss: 0.84181 |  0:00:54s\n",
      "epoch 29 | loss: 0.84535 | val_0_logloss: 0.84376 |  0:00:56s\n",
      "epoch 30 | loss: 0.84585 | val_0_logloss: 0.84295 |  0:00:57s\n",
      "epoch 31 | loss: 0.84433 | val_0_logloss: 0.84213 |  0:00:59s\n",
      "epoch 32 | loss: 0.84349 | val_0_logloss: 0.84034 |  0:01:01s\n",
      "epoch 33 | loss: 0.84289 | val_0_logloss: 0.84145 |  0:01:03s\n",
      "epoch 34 | loss: 0.845   | val_0_logloss: 0.83948 |  0:01:05s\n",
      "epoch 35 | loss: 0.84549 | val_0_logloss: 0.84003 |  0:01:07s\n",
      "epoch 36 | loss: 0.84591 | val_0_logloss: 0.84163 |  0:01:09s\n",
      "epoch 37 | loss: 0.84525 | val_0_logloss: 0.83968 |  0:01:11s\n",
      "epoch 38 | loss: 0.84445 | val_0_logloss: 0.84028 |  0:01:12s\n",
      "epoch 39 | loss: 0.84356 | val_0_logloss: 0.84138 |  0:01:14s\n",
      "epoch 40 | loss: 0.8449  | val_0_logloss: 0.8386  |  0:01:16s\n",
      "epoch 41 | loss: 0.84701 | val_0_logloss: 0.84035 |  0:01:18s\n",
      "epoch 42 | loss: 0.84522 | val_0_logloss: 0.84197 |  0:01:20s\n",
      "epoch 43 | loss: 0.84627 | val_0_logloss: 0.8408  |  0:01:22s\n",
      "epoch 44 | loss: 0.84515 | val_0_logloss: 0.84038 |  0:01:24s\n",
      "epoch 45 | loss: 0.84509 | val_0_logloss: 0.84163 |  0:01:25s\n",
      "epoch 46 | loss: 0.84405 | val_0_logloss: 0.84038 |  0:01:27s\n",
      "epoch 47 | loss: 0.84303 | val_0_logloss: 0.8405  |  0:01:29s\n",
      "epoch 48 | loss: 0.84351 | val_0_logloss: 0.84213 |  0:01:31s\n",
      "epoch 49 | loss: 0.84418 | val_0_logloss: 0.84002 |  0:01:33s\n",
      "epoch 50 | loss: 0.84364 | val_0_logloss: 0.84028 |  0:01:35s\n",
      "epoch 51 | loss: 0.84292 | val_0_logloss: 0.83863 |  0:01:37s\n",
      "epoch 52 | loss: 0.8432  | val_0_logloss: 0.83993 |  0:01:39s\n",
      "epoch 53 | loss: 0.84338 | val_0_logloss: 0.84002 |  0:01:40s\n",
      "epoch 54 | loss: 0.84384 | val_0_logloss: 0.84234 |  0:01:42s\n",
      "epoch 55 | loss: 0.84473 | val_0_logloss: 0.84216 |  0:01:44s\n",
      "epoch 56 | loss: 0.84474 | val_0_logloss: 0.84158 |  0:01:46s\n",
      "epoch 57 | loss: 0.84369 | val_0_logloss: 0.84179 |  0:01:48s\n",
      "epoch 58 | loss: 0.84375 | val_0_logloss: 0.84041 |  0:01:50s\n",
      "epoch 59 | loss: 0.84308 | val_0_logloss: 0.83939 |  0:01:51s\n",
      "epoch 60 | loss: 0.84266 | val_0_logloss: 0.8401  |  0:01:53s\n",
      "epoch 61 | loss: 0.84233 | val_0_logloss: 0.83906 |  0:01:55s\n",
      "epoch 62 | loss: 0.84173 | val_0_logloss: 0.84052 |  0:01:57s\n",
      "epoch 63 | loss: 0.842   | val_0_logloss: 0.8394  |  0:01:59s\n",
      "epoch 64 | loss: 0.84204 | val_0_logloss: 0.83916 |  0:02:01s\n",
      "epoch 65 | loss: 0.84152 | val_0_logloss: 0.83822 |  0:02:02s\n",
      "epoch 66 | loss: 0.84152 | val_0_logloss: 0.83861 |  0:02:04s\n",
      "epoch 67 | loss: 0.84096 | val_0_logloss: 0.83889 |  0:02:06s\n",
      "epoch 68 | loss: 0.84106 | val_0_logloss: 0.83927 |  0:02:08s\n",
      "epoch 69 | loss: 0.84146 | val_0_logloss: 0.83784 |  0:02:10s\n",
      "epoch 70 | loss: 0.84122 | val_0_logloss: 0.83866 |  0:02:12s\n",
      "epoch 71 | loss: 0.8415  | val_0_logloss: 0.83779 |  0:02:14s\n",
      "epoch 72 | loss: 0.84082 | val_0_logloss: 0.83766 |  0:02:15s\n",
      "epoch 73 | loss: 0.84055 | val_0_logloss: 0.83798 |  0:02:17s\n",
      "epoch 74 | loss: 0.84041 | val_0_logloss: 0.83858 |  0:02:19s\n",
      "epoch 75 | loss: 0.84114 | val_0_logloss: 0.83866 |  0:02:21s\n",
      "epoch 76 | loss: 0.84157 | val_0_logloss: 0.83764 |  0:02:23s\n",
      "epoch 77 | loss: 0.84174 | val_0_logloss: 0.83776 |  0:02:25s\n",
      "epoch 78 | loss: 0.84156 | val_0_logloss: 0.83915 |  0:02:26s\n",
      "epoch 79 | loss: 0.84168 | val_0_logloss: 0.83835 |  0:02:28s\n",
      "epoch 80 | loss: 0.84109 | val_0_logloss: 0.83791 |  0:02:30s\n",
      "epoch 81 | loss: 0.84161 | val_0_logloss: 0.8386  |  0:02:32s\n",
      "epoch 82 | loss: 0.83992 | val_0_logloss: 0.83792 |  0:02:34s\n",
      "epoch 83 | loss: 0.84035 | val_0_logloss: 0.8373  |  0:02:36s\n",
      "epoch 84 | loss: 0.84003 | val_0_logloss: 0.8375  |  0:02:38s\n",
      "epoch 85 | loss: 0.84173 | val_0_logloss: 0.83723 |  0:02:39s\n",
      "epoch 86 | loss: 0.84085 | val_0_logloss: 0.83692 |  0:02:41s\n",
      "epoch 87 | loss: 0.84071 | val_0_logloss: 0.83655 |  0:02:43s\n",
      "epoch 88 | loss: 0.84041 | val_0_logloss: 0.83689 |  0:02:45s\n",
      "epoch 89 | loss: 0.83921 | val_0_logloss: 0.83686 |  0:02:47s\n",
      "epoch 90 | loss: 0.83882 | val_0_logloss: 0.83635 |  0:02:49s\n",
      "epoch 91 | loss: 0.83876 | val_0_logloss: 0.83735 |  0:02:51s\n",
      "epoch 92 | loss: 0.83927 | val_0_logloss: 0.83802 |  0:02:52s\n",
      "epoch 93 | loss: 0.83848 | val_0_logloss: 0.83723 |  0:02:54s\n",
      "epoch 94 | loss: 0.8393  | val_0_logloss: 0.83819 |  0:02:56s\n",
      "epoch 95 | loss: 0.83831 | val_0_logloss: 0.83678 |  0:02:58s\n",
      "epoch 96 | loss: 0.83902 | val_0_logloss: 0.83676 |  0:03:00s\n",
      "epoch 97 | loss: 0.83827 | val_0_logloss: 0.83776 |  0:03:02s\n",
      "epoch 98 | loss: 0.8381  | val_0_logloss: 0.83738 |  0:03:04s\n",
      "epoch 99 | loss: 0.83751 | val_0_logloss: 0.83835 |  0:03:06s\n",
      "epoch 100| loss: 0.83695 | val_0_logloss: 0.83866 |  0:03:08s\n",
      "epoch 101| loss: 0.8378  | val_0_logloss: 0.83677 |  0:03:09s\n",
      "epoch 102| loss: 0.83741 | val_0_logloss: 0.83794 |  0:03:11s\n",
      "epoch 103| loss: 0.83766 | val_0_logloss: 0.83756 |  0:03:13s\n",
      "epoch 104| loss: 0.83662 | val_0_logloss: 0.83584 |  0:03:15s\n",
      "epoch 105| loss: 0.83701 | val_0_logloss: 0.83674 |  0:03:17s\n",
      "epoch 106| loss: 0.83624 | val_0_logloss: 0.8374  |  0:03:19s\n",
      "epoch 107| loss: 0.83826 | val_0_logloss: 0.83733 |  0:03:21s\n",
      "epoch 108| loss: 0.83763 | val_0_logloss: 0.83647 |  0:03:22s\n",
      "epoch 109| loss: 0.83686 | val_0_logloss: 0.83564 |  0:03:24s\n",
      "epoch 110| loss: 0.83872 | val_0_logloss: 0.8377  |  0:03:26s\n",
      "epoch 111| loss: 0.83809 | val_0_logloss: 0.83644 |  0:03:28s\n",
      "epoch 112| loss: 0.83859 | val_0_logloss: 0.83594 |  0:03:30s\n",
      "epoch 113| loss: 0.83682 | val_0_logloss: 0.83521 |  0:03:32s\n",
      "epoch 114| loss: 0.83656 | val_0_logloss: 0.835   |  0:03:34s\n",
      "epoch 115| loss: 0.83636 | val_0_logloss: 0.83475 |  0:03:35s\n",
      "epoch 116| loss: 0.83513 | val_0_logloss: 0.83395 |  0:03:37s\n",
      "epoch 117| loss: 0.83554 | val_0_logloss: 0.8346  |  0:03:39s\n",
      "epoch 118| loss: 0.83495 | val_0_logloss: 0.83427 |  0:03:41s\n",
      "epoch 119| loss: 0.83464 | val_0_logloss: 0.83463 |  0:03:43s\n",
      "epoch 120| loss: 0.83413 | val_0_logloss: 0.83443 |  0:03:45s\n",
      "epoch 121| loss: 0.83339 | val_0_logloss: 0.83515 |  0:03:47s\n",
      "epoch 122| loss: 0.83436 | val_0_logloss: 0.8355  |  0:03:48s\n",
      "epoch 123| loss: 0.83442 | val_0_logloss: 0.83483 |  0:03:50s\n",
      "epoch 124| loss: 0.83228 | val_0_logloss: 0.83552 |  0:03:52s\n",
      "epoch 125| loss: 0.83295 | val_0_logloss: 0.83475 |  0:03:54s\n",
      "epoch 126| loss: 0.83398 | val_0_logloss: 0.83357 |  0:03:56s\n",
      "epoch 127| loss: 0.8346  | val_0_logloss: 0.83536 |  0:03:58s\n",
      "epoch 128| loss: 0.83429 | val_0_logloss: 0.83474 |  0:04:00s\n",
      "epoch 129| loss: 0.83283 | val_0_logloss: 0.83416 |  0:04:01s\n",
      "epoch 130| loss: 0.83287 | val_0_logloss: 0.83284 |  0:04:03s\n",
      "epoch 131| loss: 0.83184 | val_0_logloss: 0.83376 |  0:04:05s\n",
      "epoch 132| loss: 0.83456 | val_0_logloss: 0.83486 |  0:04:07s\n",
      "epoch 133| loss: 0.83434 | val_0_logloss: 0.83753 |  0:04:09s\n",
      "epoch 134| loss: 0.83378 | val_0_logloss: 0.83628 |  0:04:11s\n",
      "epoch 135| loss: 0.83349 | val_0_logloss: 0.83467 |  0:04:13s\n",
      "epoch 136| loss: 0.83301 | val_0_logloss: 0.83403 |  0:04:14s\n",
      "epoch 137| loss: 0.83328 | val_0_logloss: 0.83326 |  0:04:16s\n",
      "epoch 138| loss: 0.83181 | val_0_logloss: 0.83426 |  0:04:18s\n",
      "epoch 139| loss: 0.83192 | val_0_logloss: 0.83424 |  0:04:20s\n",
      "epoch 140| loss: 0.83119 | val_0_logloss: 0.83398 |  0:04:22s\n",
      "epoch 141| loss: 0.83156 | val_0_logloss: 0.83427 |  0:04:24s\n",
      "epoch 142| loss: 0.83131 | val_0_logloss: 0.83349 |  0:04:26s\n",
      "epoch 143| loss: 0.83076 | val_0_logloss: 0.83632 |  0:04:27s\n",
      "epoch 144| loss: 0.83083 | val_0_logloss: 0.83641 |  0:04:29s\n",
      "epoch 145| loss: 0.83185 | val_0_logloss: 0.8362  |  0:04:31s\n",
      "epoch 146| loss: 0.83087 | val_0_logloss: 0.83649 |  0:04:33s\n",
      "epoch 147| loss: 0.83101 | val_0_logloss: 0.8376  |  0:04:35s\n",
      "epoch 148| loss: 0.83024 | val_0_logloss: 0.83784 |  0:04:37s\n",
      "epoch 149| loss: 0.82971 | val_0_logloss: 0.83562 |  0:04:39s\n",
      "epoch 150| loss: 0.82852 | val_0_logloss: 0.83617 |  0:04:40s\n",
      "epoch 151| loss: 0.8302  | val_0_logloss: 0.83676 |  0:04:42s\n",
      "epoch 152| loss: 0.83002 | val_0_logloss: 0.83525 |  0:04:44s\n",
      "epoch 153| loss: 0.82969 | val_0_logloss: 0.83286 |  0:04:46s\n",
      "epoch 154| loss: 0.82871 | val_0_logloss: 0.83336 |  0:04:48s\n",
      "epoch 155| loss: 0.82887 | val_0_logloss: 0.83464 |  0:04:50s\n",
      "epoch 156| loss: 0.82766 | val_0_logloss: 0.83356 |  0:04:52s\n",
      "epoch 157| loss: 0.82757 | val_0_logloss: 0.83437 |  0:04:54s\n",
      "epoch 158| loss: 0.82696 | val_0_logloss: 0.83347 |  0:04:55s\n",
      "epoch 159| loss: 0.82739 | val_0_logloss: 0.83673 |  0:04:57s\n",
      "epoch 160| loss: 0.82736 | val_0_logloss: 0.83533 |  0:04:59s\n",
      "epoch 161| loss: 0.82869 | val_0_logloss: 0.83511 |  0:05:01s\n",
      "epoch 162| loss: 0.82762 | val_0_logloss: 0.83576 |  0:05:03s\n",
      "epoch 163| loss: 0.82899 | val_0_logloss: 0.83227 |  0:05:05s\n",
      "epoch 164| loss: 0.82607 | val_0_logloss: 0.83222 |  0:05:07s\n",
      "epoch 165| loss: 0.82618 | val_0_logloss: 0.83234 |  0:05:08s\n",
      "epoch 166| loss: 0.82509 | val_0_logloss: 0.8318  |  0:05:10s\n",
      "epoch 167| loss: 0.82701 | val_0_logloss: 0.83278 |  0:05:12s\n",
      "epoch 168| loss: 0.82573 | val_0_logloss: 0.83379 |  0:05:14s\n",
      "epoch 169| loss: 0.82708 | val_0_logloss: 0.83543 |  0:05:16s\n",
      "epoch 170| loss: 0.82791 | val_0_logloss: 0.83341 |  0:05:18s\n",
      "epoch 171| loss: 0.82653 | val_0_logloss: 0.83371 |  0:05:19s\n",
      "epoch 172| loss: 0.82449 | val_0_logloss: 0.83255 |  0:05:21s\n",
      "epoch 173| loss: 0.82406 | val_0_logloss: 0.83227 |  0:05:23s\n",
      "epoch 174| loss: 0.82334 | val_0_logloss: 0.83414 |  0:05:25s\n",
      "epoch 175| loss: 0.82371 | val_0_logloss: 0.83546 |  0:05:27s\n",
      "epoch 176| loss: 0.8262  | val_0_logloss: 0.83364 |  0:05:29s\n",
      "epoch 177| loss: 0.82522 | val_0_logloss: 0.8339  |  0:05:31s\n",
      "epoch 178| loss: 0.82362 | val_0_logloss: 0.83463 |  0:05:32s\n",
      "epoch 179| loss: 0.82692 | val_0_logloss: 0.83347 |  0:05:34s\n",
      "epoch 180| loss: 0.82758 | val_0_logloss: 0.83364 |  0:05:36s\n",
      "epoch 181| loss: 0.82821 | val_0_logloss: 0.83188 |  0:05:38s\n",
      "epoch 182| loss: 0.82939 | val_0_logloss: 0.8317  |  0:05:40s\n",
      "epoch 183| loss: 0.82731 | val_0_logloss: 0.83201 |  0:05:42s\n",
      "epoch 184| loss: 0.82539 | val_0_logloss: 0.83282 |  0:05:44s\n",
      "epoch 185| loss: 0.82595 | val_0_logloss: 0.83303 |  0:05:45s\n",
      "epoch 186| loss: 0.82477 | val_0_logloss: 0.83236 |  0:05:47s\n",
      "epoch 187| loss: 0.82436 | val_0_logloss: 0.83223 |  0:05:49s\n",
      "epoch 188| loss: 0.82519 | val_0_logloss: 0.83499 |  0:05:51s\n",
      "epoch 189| loss: 0.82613 | val_0_logloss: 0.83331 |  0:05:53s\n",
      "epoch 190| loss: 0.82461 | val_0_logloss: 0.83266 |  0:05:55s\n",
      "epoch 191| loss: 0.82456 | val_0_logloss: 0.83355 |  0:05:56s\n",
      "epoch 192| loss: 0.82461 | val_0_logloss: 0.83305 |  0:05:58s\n",
      "epoch 193| loss: 0.82421 | val_0_logloss: 0.83264 |  0:06:00s\n",
      "epoch 194| loss: 0.82341 | val_0_logloss: 0.83125 |  0:06:02s\n",
      "epoch 195| loss: 0.8222  | val_0_logloss: 0.8342  |  0:06:04s\n",
      "epoch 196| loss: 0.82261 | val_0_logloss: 0.83259 |  0:06:06s\n",
      "epoch 197| loss: 0.82391 | val_0_logloss: 0.83234 |  0:06:08s\n",
      "epoch 198| loss: 0.82266 | val_0_logloss: 0.83102 |  0:06:09s\n",
      "epoch 199| loss: 0.82162 | val_0_logloss: 0.8327  |  0:06:11s\n",
      "epoch 200| loss: 0.82251 | val_0_logloss: 0.83261 |  0:06:13s\n",
      "epoch 201| loss: 0.82408 | val_0_logloss: 0.83164 |  0:06:15s\n",
      "epoch 202| loss: 0.82203 | val_0_logloss: 0.82955 |  0:06:17s\n",
      "epoch 203| loss: 0.82215 | val_0_logloss: 0.82996 |  0:06:19s\n",
      "epoch 204| loss: 0.8224  | val_0_logloss: 0.83018 |  0:06:21s\n",
      "epoch 205| loss: 0.82496 | val_0_logloss: 0.83072 |  0:06:23s\n",
      "epoch 206| loss: 0.82389 | val_0_logloss: 0.8324  |  0:06:24s\n",
      "epoch 207| loss: 0.82231 | val_0_logloss: 0.83212 |  0:06:26s\n",
      "epoch 208| loss: 0.82093 | val_0_logloss: 0.83241 |  0:06:28s\n",
      "epoch 209| loss: 0.82128 | val_0_logloss: 0.83314 |  0:06:30s\n",
      "epoch 210| loss: 0.82219 | val_0_logloss: 0.83478 |  0:06:32s\n",
      "epoch 211| loss: 0.82209 | val_0_logloss: 0.83243 |  0:06:34s\n",
      "epoch 212| loss: 0.82047 | val_0_logloss: 0.83133 |  0:06:35s\n",
      "epoch 213| loss: 0.82148 | val_0_logloss: 0.83164 |  0:06:37s\n",
      "epoch 214| loss: 0.82208 | val_0_logloss: 0.83311 |  0:06:39s\n",
      "epoch 215| loss: 0.82107 | val_0_logloss: 0.83265 |  0:06:41s\n",
      "epoch 216| loss: 0.82157 | val_0_logloss: 0.82903 |  0:06:43s\n",
      "epoch 217| loss: 0.82159 | val_0_logloss: 0.82877 |  0:06:45s\n",
      "epoch 218| loss: 0.81956 | val_0_logloss: 0.82991 |  0:06:47s\n",
      "epoch 219| loss: 0.82045 | val_0_logloss: 0.83043 |  0:06:48s\n",
      "epoch 220| loss: 0.81953 | val_0_logloss: 0.83107 |  0:06:50s\n",
      "epoch 221| loss: 0.81746 | val_0_logloss: 0.82987 |  0:06:52s\n",
      "epoch 222| loss: 0.81794 | val_0_logloss: 0.82987 |  0:06:54s\n",
      "epoch 223| loss: 0.81765 | val_0_logloss: 0.83015 |  0:06:56s\n",
      "epoch 224| loss: 0.81798 | val_0_logloss: 0.83025 |  0:06:58s\n",
      "epoch 225| loss: 0.81742 | val_0_logloss: 0.83187 |  0:07:00s\n",
      "epoch 226| loss: 0.81712 | val_0_logloss: 0.83347 |  0:07:01s\n",
      "epoch 227| loss: 0.81914 | val_0_logloss: 0.83119 |  0:07:03s\n",
      "epoch 228| loss: 0.81962 | val_0_logloss: 0.83143 |  0:07:05s\n",
      "epoch 229| loss: 0.819   | val_0_logloss: 0.83299 |  0:07:07s\n",
      "epoch 230| loss: 0.81782 | val_0_logloss: 0.83402 |  0:07:09s\n",
      "epoch 231| loss: 0.81769 | val_0_logloss: 0.83165 |  0:07:11s\n",
      "epoch 232| loss: 0.81658 | val_0_logloss: 0.83019 |  0:07:12s\n",
      "epoch 233| loss: 0.81464 | val_0_logloss: 0.82916 |  0:07:14s\n",
      "epoch 234| loss: 0.81482 | val_0_logloss: 0.83325 |  0:07:16s\n",
      "epoch 235| loss: 0.81742 | val_0_logloss: 0.83242 |  0:07:18s\n",
      "epoch 236| loss: 0.81599 | val_0_logloss: 0.83004 |  0:07:20s\n",
      "epoch 237| loss: 0.8149  | val_0_logloss: 0.83008 |  0:07:22s\n",
      "epoch 238| loss: 0.81409 | val_0_logloss: 0.82971 |  0:07:24s\n",
      "epoch 239| loss: 0.8125  | val_0_logloss: 0.83067 |  0:07:26s\n",
      "epoch 240| loss: 0.81356 | val_0_logloss: 0.82868 |  0:07:27s\n",
      "epoch 241| loss: 0.81358 | val_0_logloss: 0.82912 |  0:07:29s\n",
      "epoch 242| loss: 0.81231 | val_0_logloss: 0.82905 |  0:07:31s\n",
      "epoch 243| loss: 0.81384 | val_0_logloss: 0.82808 |  0:07:33s\n",
      "epoch 244| loss: 0.81189 | val_0_logloss: 0.82839 |  0:07:35s\n",
      "epoch 245| loss: 0.81445 | val_0_logloss: 0.83099 |  0:07:37s\n",
      "epoch 246| loss: 0.81313 | val_0_logloss: 0.83029 |  0:07:39s\n",
      "epoch 247| loss: 0.81261 | val_0_logloss: 0.82964 |  0:07:40s\n",
      "epoch 248| loss: 0.81314 | val_0_logloss: 0.82958 |  0:07:42s\n",
      "epoch 249| loss: 0.81342 | val_0_logloss: 0.82972 |  0:07:44s\n",
      "epoch 250| loss: 0.81267 | val_0_logloss: 0.8277  |  0:07:46s\n",
      "epoch 251| loss: 0.81387 | val_0_logloss: 0.82844 |  0:07:48s\n",
      "epoch 252| loss: 0.81279 | val_0_logloss: 0.82752 |  0:07:50s\n",
      "epoch 253| loss: 0.81201 | val_0_logloss: 0.83001 |  0:07:52s\n",
      "epoch 254| loss: 0.81221 | val_0_logloss: 0.82948 |  0:07:54s\n",
      "epoch 255| loss: 0.81239 | val_0_logloss: 0.82915 |  0:07:55s\n",
      "epoch 256| loss: 0.81283 | val_0_logloss: 0.82939 |  0:07:57s\n",
      "epoch 257| loss: 0.81355 | val_0_logloss: 0.83202 |  0:07:59s\n",
      "epoch 258| loss: 0.81167 | val_0_logloss: 0.83498 |  0:08:01s\n",
      "epoch 259| loss: 0.81282 | val_0_logloss: 0.83349 |  0:08:03s\n",
      "epoch 260| loss: 0.81271 | val_0_logloss: 0.83216 |  0:08:05s\n",
      "epoch 261| loss: 0.81116 | val_0_logloss: 0.83125 |  0:08:07s\n",
      "epoch 262| loss: 0.80963 | val_0_logloss: 0.83235 |  0:08:10s\n",
      "epoch 263| loss: 0.80928 | val_0_logloss: 0.83178 |  0:08:12s\n",
      "epoch 264| loss: 0.80905 | val_0_logloss: 0.83277 |  0:08:14s\n",
      "epoch 265| loss: 0.80932 | val_0_logloss: 0.83189 |  0:08:16s\n",
      "epoch 266| loss: 0.8078  | val_0_logloss: 0.83384 |  0:08:18s\n",
      "epoch 267| loss: 0.80924 | val_0_logloss: 0.83243 |  0:08:20s\n",
      "epoch 268| loss: 0.80604 | val_0_logloss: 0.83255 |  0:08:22s\n",
      "epoch 269| loss: 0.8082  | val_0_logloss: 0.83293 |  0:08:24s\n",
      "epoch 270| loss: 0.80987 | val_0_logloss: 0.83352 |  0:08:26s\n",
      "epoch 271| loss: 0.80855 | val_0_logloss: 0.83436 |  0:08:28s\n",
      "epoch 272| loss: 0.80992 | val_0_logloss: 0.83344 |  0:08:29s\n",
      "epoch 273| loss: 0.80845 | val_0_logloss: 0.8336  |  0:08:31s\n",
      "epoch 274| loss: 0.8092  | val_0_logloss: 0.83352 |  0:08:33s\n",
      "epoch 275| loss: 0.8079  | val_0_logloss: 0.83282 |  0:08:35s\n",
      "epoch 276| loss: 0.80605 | val_0_logloss: 0.83288 |  0:08:37s\n",
      "epoch 277| loss: 0.80864 | val_0_logloss: 0.83264 |  0:08:39s\n",
      "epoch 278| loss: 0.80701 | val_0_logloss: 0.83053 |  0:08:41s\n",
      "epoch 279| loss: 0.80546 | val_0_logloss: 0.82987 |  0:08:42s\n",
      "epoch 280| loss: 0.80571 | val_0_logloss: 0.82889 |  0:08:44s\n",
      "epoch 281| loss: 0.80508 | val_0_logloss: 0.83151 |  0:08:46s\n",
      "epoch 282| loss: 0.80401 | val_0_logloss: 0.83268 |  0:08:48s\n",
      "epoch 283| loss: 0.80733 | val_0_logloss: 0.83207 |  0:08:50s\n",
      "epoch 284| loss: 0.80585 | val_0_logloss: 0.83139 |  0:08:52s\n",
      "epoch 285| loss: 0.80448 | val_0_logloss: 0.83154 |  0:08:54s\n",
      "epoch 286| loss: 0.80644 | val_0_logloss: 0.83165 |  0:08:55s\n",
      "epoch 287| loss: 0.80772 | val_0_logloss: 0.83301 |  0:08:57s\n",
      "epoch 288| loss: 0.80633 | val_0_logloss: 0.83459 |  0:08:59s\n",
      "epoch 289| loss: 0.80583 | val_0_logloss: 0.83418 |  0:09:01s\n",
      "epoch 290| loss: 0.80682 | val_0_logloss: 0.83376 |  0:09:03s\n",
      "epoch 291| loss: 0.80498 | val_0_logloss: 0.83267 |  0:09:05s\n",
      "epoch 292| loss: 0.80454 | val_0_logloss: 0.8323  |  0:09:07s\n",
      "epoch 293| loss: 0.80483 | val_0_logloss: 0.83306 |  0:09:08s\n",
      "epoch 294| loss: 0.80443 | val_0_logloss: 0.83134 |  0:09:10s\n",
      "epoch 295| loss: 0.80366 | val_0_logloss: 0.83362 |  0:09:12s\n",
      "epoch 296| loss: 0.80394 | val_0_logloss: 0.83358 |  0:09:14s\n",
      "epoch 297| loss: 0.80351 | val_0_logloss: 0.8337  |  0:09:16s\n",
      "epoch 298| loss: 0.80175 | val_0_logloss: 0.83304 |  0:09:18s\n",
      "epoch 299| loss: 0.80477 | val_0_logloss: 0.83251 |  0:09:20s\n",
      "Stop training because you reached max_epochs = 300 with best_epoch = 252 and best_val_0_logloss = 0.82752\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "from pytorch_tabnet.multitask import TabNetMultiTaskClassifier\n",
    "\n",
    "unsupervised_model = TabNetPretrainer(\n",
    "    n_d=16, n_a=16, n_steps=5,\n",
    "    lambda_sparse=1e-2,\n",
    "    gamma = 1.3,\n",
    "    optimizer_fn = torch.optim.Adam,\n",
    "    optimizer_params = dict(lr=2e-2),\n",
    "    scheduler_params = {\"gamma\": 0.9, \"step_size\": 20},\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    mask_type='entmax',\n",
    "    device_name='cuda'\n",
    ")\n",
    "\n",
    "unsupervised_model.fit(\n",
    "    X_train = X_train.values,\n",
    "    eval_set = [X_val.values],\n",
    "    pretraining_ratio=0.8\n",
    ")\n",
    "\n",
    "clf = TabNetMultiTaskClassifier(\n",
    "    n_d=16, n_a=16, n_steps=5,\n",
    "    lambda_sparse=1e-2,\n",
    "    gamma = 1.3,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=1e-2),\n",
    "    scheduler_params = {\"gamma\": 0.9, \"step_size\": 20},\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    mask_type='entmax',\n",
    "    device_name='cuda'\n",
    ")\n",
    "\n",
    "clf.fit(\n",
    "    X_train = X_train.values, y_train = np.array(Y_train).reshape(Y_train.shape[0],1),\n",
    "    eval_set = [(X_val.values, np.array(Y_val).reshape(Y_val.shape[0],1))],\n",
    "    max_epochs=300,\n",
    "    patience=50,\n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    from_unsupervised=unsupervised_model,\n",
    "    drop_last=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8275176201233926"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "predict = clf.predict_proba(X_val.values)\n",
    "predict = pd.DataFrame(np.reshape(np.array(predict), (np.array(predict).shape[1],3)), columns=[0.0, 1.0, 2.0])\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "log_loss(y_val_onehot, predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c9a651fd00>]"
      ]
     },
     "metadata": {},
     "execution_count": 19
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 379.002696 248.518125\" width=\"379.002696pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-05-11T19:26:08.155068</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.4.1, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 379.002696 248.518125 \r\nL 379.002696 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 371.265625 224.64 \r\nL 371.265625 7.2 \r\nL 36.465625 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m2d2ee22692\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.683807\" xlink:href=\"#m2d2ee22692\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(48.502557 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 4250 \r\nQ 1547 4250 1301 3770 \r\nQ 1056 3291 1056 2328 \r\nQ 1056 1369 1301 889 \r\nQ 1547 409 2034 409 \r\nQ 2525 409 2770 889 \r\nQ 3016 1369 3016 2328 \r\nQ 3016 3291 2770 3770 \r\nQ 2525 4250 2034 4250 \r\nz\r\nM 2034 4750 \r\nQ 2819 4750 3233 4129 \r\nQ 3647 3509 3647 2328 \r\nQ 3647 1150 3233 529 \r\nQ 2819 -91 2034 -91 \r\nQ 1250 -91 836 529 \r\nQ 422 1150 422 2328 \r\nQ 422 3509 836 4129 \r\nQ 1250 4750 2034 4750 \r\nz\r\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"103.44633\" xlink:href=\"#m2d2ee22692\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 50 -->\r\n      <g transform=\"translate(97.08383 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 691 4666 \r\nL 3169 4666 \r\nL 3169 4134 \r\nL 1269 4134 \r\nL 1269 2991 \r\nQ 1406 3038 1543 3061 \r\nQ 1681 3084 1819 3084 \r\nQ 2600 3084 3056 2656 \r\nQ 3513 2228 3513 1497 \r\nQ 3513 744 3044 326 \r\nQ 2575 -91 1722 -91 \r\nQ 1428 -91 1123 -41 \r\nQ 819 9 494 109 \r\nL 494 744 \r\nQ 775 591 1075 516 \r\nQ 1375 441 1709 441 \r\nQ 2250 441 2565 725 \r\nQ 2881 1009 2881 1497 \r\nQ 2881 1984 2565 2268 \r\nQ 2250 2553 1709 2553 \r\nQ 1456 2553 1204 2497 \r\nQ 953 2441 691 2322 \r\nL 691 4666 \r\nz\r\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"155.208853\" xlink:href=\"#m2d2ee22692\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 100 -->\r\n      <g transform=\"translate(145.665103 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 794 531 \r\nL 1825 531 \r\nL 1825 4091 \r\nL 703 3866 \r\nL 703 4441 \r\nL 1819 4666 \r\nL 2450 4666 \r\nL 2450 531 \r\nL 3481 531 \r\nL 3481 0 \r\nL 794 0 \r\nL 794 531 \r\nz\r\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"206.971376\" xlink:href=\"#m2d2ee22692\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 150 -->\r\n      <g transform=\"translate(197.427626 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"258.7339\" xlink:href=\"#m2d2ee22692\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 200 -->\r\n      <g transform=\"translate(249.19015 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 1228 531 \r\nL 3431 531 \r\nL 3431 0 \r\nL 469 0 \r\nL 469 531 \r\nQ 828 903 1448 1529 \r\nQ 2069 2156 2228 2338 \r\nQ 2531 2678 2651 2914 \r\nQ 2772 3150 2772 3378 \r\nQ 2772 3750 2511 3984 \r\nQ 2250 4219 1831 4219 \r\nQ 1534 4219 1204 4116 \r\nQ 875 4013 500 3803 \r\nL 500 4441 \r\nQ 881 4594 1212 4672 \r\nQ 1544 4750 1819 4750 \r\nQ 2544 4750 2975 4387 \r\nQ 3406 4025 3406 3419 \r\nQ 3406 3131 3298 2873 \r\nQ 3191 2616 2906 2266 \r\nQ 2828 2175 2409 1742 \r\nQ 1991 1309 1228 531 \r\nz\r\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"310.496423\" xlink:href=\"#m2d2ee22692\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 250 -->\r\n      <g transform=\"translate(300.952673 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"362.258946\" xlink:href=\"#m2d2ee22692\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 300 -->\r\n      <g transform=\"translate(352.715196 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2597 2516 \r\nQ 3050 2419 3304 2112 \r\nQ 3559 1806 3559 1356 \r\nQ 3559 666 3084 287 \r\nQ 2609 -91 1734 -91 \r\nQ 1441 -91 1130 -33 \r\nQ 819 25 488 141 \r\nL 488 750 \r\nQ 750 597 1062 519 \r\nQ 1375 441 1716 441 \r\nQ 2309 441 2620 675 \r\nQ 2931 909 2931 1356 \r\nQ 2931 1769 2642 2001 \r\nQ 2353 2234 1838 2234 \r\nL 1294 2234 \r\nL 1294 2753 \r\nL 1863 2753 \r\nQ 2328 2753 2575 2939 \r\nQ 2822 3125 2822 3475 \r\nQ 2822 3834 2567 4026 \r\nQ 2313 4219 1838 4219 \r\nQ 1578 4219 1281 4162 \r\nQ 984 4106 628 3988 \r\nL 628 4550 \r\nQ 988 4650 1302 4700 \r\nQ 1616 4750 1894 4750 \r\nQ 2613 4750 3031 4423 \r\nQ 3450 4097 3450 3541 \r\nQ 3450 3153 3228 2886 \r\nQ 3006 2619 2597 2516 \r\nz\r\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-33\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_8\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"mb4b11a9a18\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mb4b11a9a18\" y=\"218.949132\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.80 -->\r\n      <g transform=\"translate(7.2 222.74835)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 684 794 \r\nL 1344 794 \r\nL 1344 0 \r\nL 684 0 \r\nL 684 794 \r\nz\r\n\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\r\n        <path d=\"M 2034 2216 \r\nQ 1584 2216 1326 1975 \r\nQ 1069 1734 1069 1313 \r\nQ 1069 891 1326 650 \r\nQ 1584 409 2034 409 \r\nQ 2484 409 2743 651 \r\nQ 3003 894 3003 1313 \r\nQ 3003 1734 2745 1975 \r\nQ 2488 2216 2034 2216 \r\nz\r\nM 1403 2484 \r\nQ 997 2584 770 2862 \r\nQ 544 3141 544 3541 \r\nQ 544 4100 942 4425 \r\nQ 1341 4750 2034 4750 \r\nQ 2731 4750 3128 4425 \r\nQ 3525 4100 3525 3541 \r\nQ 3525 3141 3298 2862 \r\nQ 3072 2584 2669 2484 \r\nQ 3125 2378 3379 2068 \r\nQ 3634 1759 3634 1313 \r\nQ 3634 634 3220 271 \r\nQ 2806 -91 2034 -91 \r\nQ 1263 -91 848 271 \r\nQ 434 634 434 1313 \r\nQ 434 1759 690 2068 \r\nQ 947 2378 1403 2484 \r\nz\r\nM 1172 3481 \r\nQ 1172 3119 1398 2916 \r\nQ 1625 2713 2034 2713 \r\nQ 2441 2713 2670 2916 \r\nQ 2900 3119 2900 3481 \r\nQ 2900 3844 2670 4047 \r\nQ 2441 4250 2034 4250 \r\nQ 1625 4250 1398 4047 \r\nQ 1172 3844 1172 3481 \r\nz\r\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mb4b11a9a18\" y=\"171.086034\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.82 -->\r\n      <g transform=\"translate(7.2 174.885252)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-32\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mb4b11a9a18\" y=\"123.222936\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.84 -->\r\n      <g transform=\"translate(7.2 127.022154)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2419 4116 \r\nL 825 1625 \r\nL 2419 1625 \r\nL 2419 4116 \r\nz\r\nM 2253 4666 \r\nL 3047 4666 \r\nL 3047 1625 \r\nL 3713 1625 \r\nL 3713 1100 \r\nL 3047 1100 \r\nL 3047 0 \r\nL 2419 0 \r\nL 2419 1100 \r\nL 313 1100 \r\nL 313 1709 \r\nL 2253 4666 \r\nz\r\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-34\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mb4b11a9a18\" y=\"75.359838\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.86 -->\r\n      <g transform=\"translate(7.2 79.159056)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2113 2584 \r\nQ 1688 2584 1439 2293 \r\nQ 1191 2003 1191 1497 \r\nQ 1191 994 1439 701 \r\nQ 1688 409 2113 409 \r\nQ 2538 409 2786 701 \r\nQ 3034 994 3034 1497 \r\nQ 3034 2003 2786 2293 \r\nQ 2538 2584 2113 2584 \r\nz\r\nM 3366 4563 \r\nL 3366 3988 \r\nQ 3128 4100 2886 4159 \r\nQ 2644 4219 2406 4219 \r\nQ 1781 4219 1451 3797 \r\nQ 1122 3375 1075 2522 \r\nQ 1259 2794 1537 2939 \r\nQ 1816 3084 2150 3084 \r\nQ 2853 3084 3261 2657 \r\nQ 3669 2231 3669 1497 \r\nQ 3669 778 3244 343 \r\nQ 2819 -91 2113 -91 \r\nQ 1303 -91 875 529 \r\nQ 447 1150 447 2328 \r\nQ 447 3434 972 4092 \r\nQ 1497 4750 2381 4750 \r\nQ 2619 4750 2861 4703 \r\nQ 3103 4656 3366 4563 \r\nz\r\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-36\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mb4b11a9a18\" y=\"27.49674\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.88 -->\r\n      <g transform=\"translate(7.2 31.295958)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-38\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_13\">\r\n    <path clip-path=\"url(#p5f4b5bf5fa)\" d=\"M 51.683807 68.800567 \r\nL 52.719057 77.361957 \r\nL 53.754308 82.340974 \r\nL 54.789558 85.534093 \r\nL 55.824809 87.421099 \r\nL 56.860059 93.162944 \r\nL 57.89531 95.770931 \r\nL 58.93056 97.388339 \r\nL 59.965811 96.650868 \r\nL 61.001061 99.237352 \r\nL 62.036311 98.868762 \r\nL 63.071562 94.210766 \r\nL 64.106812 97.190961 \r\nL 65.142063 104.803176 \r\nL 66.177313 102.672439 \r\nL 67.212564 100.13142 \r\nL 68.247814 100.796787 \r\nL 69.283065 103.607974 \r\nL 70.318315 105.095585 \r\nL 71.353566 102.21523 \r\nL 72.388816 108.270679 \r\nL 73.424067 110.619039 \r\nL 75.494567 113.563113 \r\nL 76.529818 110.430159 \r\nL 77.565068 109.212673 \r\nL 78.600319 112.851258 \r\nL 79.635569 114.871941 \r\nL 80.67082 116.297197 \r\nL 81.70607 111.250815 \r\nL 83.776571 109.079809 \r\nL 84.811822 110.664325 \r\nL 85.847072 112.561773 \r\nL 86.882323 114.695535 \r\nL 87.917573 111.504208 \r\nL 88.952824 106.447934 \r\nL 89.988074 110.724471 \r\nL 91.023324 108.21272 \r\nL 92.058575 110.886424 \r\nL 93.093825 111.03822 \r\nL 95.164326 115.960583 \r\nL 96.199577 114.813622 \r\nL 97.234827 113.230674 \r\nL 98.270078 114.502241 \r\nL 99.305328 116.244536 \r\nL 100.340579 115.560979 \r\nL 101.375829 115.124312 \r\nL 102.41108 114.035906 \r\nL 103.44633 111.895368 \r\nL 104.48158 111.879239 \r\nL 105.516831 114.387425 \r\nL 106.552081 114.250598 \r\nL 107.587332 115.860654 \r\nL 108.622582 116.864362 \r\nL 109.657833 117.653592 \r\nL 110.693083 119.093513 \r\nL 111.728334 118.435401 \r\nL 112.763584 118.349611 \r\nL 113.798835 119.583303 \r\nL 114.834085 119.595586 \r\nL 115.869336 120.918894 \r\nL 116.904586 120.692858 \r\nL 117.939837 119.738242 \r\nL 118.975087 120.29909 \r\nL 120.010337 119.629355 \r\nL 121.045588 121.264292 \r\nL 122.080838 121.898338 \r\nL 123.116089 122.230546 \r\nL 124.151339 120.4962 \r\nL 125.18659 119.473673 \r\nL 126.22184 119.057234 \r\nL 127.257091 119.490546 \r\nL 128.292341 119.191372 \r\nL 129.327592 120.618309 \r\nL 130.362842 119.36993 \r\nL 131.398093 123.410883 \r\nL 132.433343 122.393236 \r\nL 133.468593 123.145976 \r\nL 134.503844 119.071226 \r\nL 135.539094 121.19351 \r\nL 136.574345 121.522911 \r\nL 137.609595 122.237191 \r\nL 138.644846 125.117724 \r\nL 139.680096 126.05194 \r\nL 140.715347 126.191581 \r\nL 141.750597 124.966988 \r\nL 142.785848 126.865029 \r\nL 143.821098 124.89659 \r\nL 144.856349 127.258149 \r\nL 145.891599 125.571046 \r\nL 146.926849 127.357194 \r\nL 147.9621 127.77979 \r\nL 150.032601 130.531504 \r\nL 151.067851 128.477987 \r\nL 152.103102 129.412222 \r\nL 153.138352 128.821921 \r\nL 154.173603 131.30004 \r\nL 155.208853 130.375405 \r\nL 156.244104 132.229446 \r\nL 157.279354 127.390033 \r\nL 158.314605 128.890711 \r\nL 159.349855 130.743477 \r\nL 160.385106 126.279282 \r\nL 161.420356 127.785413 \r\nL 162.455606 126.597271 \r\nL 163.490857 130.843452 \r\nL 165.561358 131.930318 \r\nL 166.596608 134.875421 \r\nL 167.631859 133.890135 \r\nL 168.667109 135.299727 \r\nL 169.70236 136.038617 \r\nL 170.73761 137.273518 \r\nL 171.772861 139.036821 \r\nL 172.808111 136.711421 \r\nL 173.843362 136.575585 \r\nL 174.878612 141.703839 \r\nL 175.913862 140.104329 \r\nL 176.949113 137.634339 \r\nL 177.984363 136.140788 \r\nL 179.019614 136.88494 \r\nL 180.054864 140.389014 \r\nL 181.090115 140.284983 \r\nL 182.125365 142.754608 \r\nL 183.160616 136.246336 \r\nL 184.195866 136.775596 \r\nL 185.231117 138.101476 \r\nL 186.266367 138.805043 \r\nL 187.301618 139.962212 \r\nL 188.336868 139.302439 \r\nL 189.372119 142.833756 \r\nL 190.407369 142.564909 \r\nL 191.442619 144.317391 \r\nL 192.47787 143.428375 \r\nL 193.51312 144.011639 \r\nL 194.548371 145.329952 \r\nL 195.583621 145.157222 \r\nL 196.618872 142.730395 \r\nL 197.654122 145.078928 \r\nL 198.689373 144.74039 \r\nL 199.724623 146.588741 \r\nL 200.759874 147.859379 \r\nL 201.795124 150.68579 \r\nL 202.830375 146.668153 \r\nL 203.865625 147.097033 \r\nL 204.900875 147.905631 \r\nL 205.936126 150.24032 \r\nL 206.971376 149.854309 \r\nL 208.006627 152.762714 \r\nL 209.041877 152.97212 \r\nL 210.077128 154.440889 \r\nL 211.112378 153.396449 \r\nL 212.147629 153.471893 \r\nL 213.182879 150.283291 \r\nL 214.21813 152.848964 \r\nL 215.25338 149.57587 \r\nL 216.288631 156.555819 \r\nL 217.323881 156.301491 \r\nL 218.359131 158.916765 \r\nL 219.394382 154.310638 \r\nL 220.429632 157.383701 \r\nL 221.464883 154.150807 \r\nL 222.500133 152.144626 \r\nL 223.535384 155.458293 \r\nL 224.570634 160.34293 \r\nL 225.605885 161.359494 \r\nL 226.641135 163.102236 \r\nL 227.676386 162.201065 \r\nL 228.711636 156.240842 \r\nL 229.746887 158.591348 \r\nL 230.782137 162.412467 \r\nL 231.817388 154.519769 \r\nL 233.887888 151.429853 \r\nL 234.923139 148.61176 \r\nL 236.99364 158.177617 \r\nL 238.02889 156.854496 \r\nL 239.064141 159.668506 \r\nL 240.099391 160.640215 \r\nL 241.134642 158.668195 \r\nL 242.169892 156.420336 \r\nL 243.205143 160.04781 \r\nL 244.240393 160.167727 \r\nL 245.275644 160.0433 \r\nL 246.310894 161.022428 \r\nL 247.346144 162.925868 \r\nL 248.381395 165.82179 \r\nL 249.416645 164.851847 \r\nL 250.451896 161.723444 \r\nL 251.487146 164.71955 \r\nL 252.522397 167.211932 \r\nL 253.557647 165.090293 \r\nL 254.592898 161.311983 \r\nL 255.628148 166.228479 \r\nL 256.663399 165.937012 \r\nL 257.698649 165.341819 \r\nL 258.7339 159.21166 \r\nL 259.76915 161.785023 \r\nL 260.804401 165.546283 \r\nL 261.839651 168.870391 \r\nL 262.874901 168.031303 \r\nL 263.910152 165.849307 \r\nL 264.945402 166.072584 \r\nL 265.980653 169.95068 \r\nL 267.015903 167.545011 \r\nL 268.051154 166.119365 \r\nL 269.086404 168.530218 \r\nL 270.121655 167.333703 \r\nL 271.156905 167.291939 \r\nL 272.192156 172.141717 \r\nL 273.227406 170.011056 \r\nL 274.262657 172.213909 \r\nL 275.297907 177.175046 \r\nL 276.333157 176.012843 \r\nL 277.368408 176.700312 \r\nL 278.403658 175.929344 \r\nL 279.438909 177.24933 \r\nL 280.474159 177.966392 \r\nL 281.50941 173.135589 \r\nL 282.54466 172.001821 \r\nL 283.579911 173.479471 \r\nL 284.615161 176.313497 \r\nL 285.650412 176.624092 \r\nL 286.685662 179.26696 \r\nL 287.720913 183.911417 \r\nL 288.756163 183.480629 \r\nL 289.791413 177.251945 \r\nL 290.826664 180.676725 \r\nL 291.861914 183.282925 \r\nL 292.897165 185.233911 \r\nL 293.932415 189.025979 \r\nL 294.967666 186.508479 \r\nL 296.002916 186.45451 \r\nL 297.038167 189.485898 \r\nL 298.073417 185.835516 \r\nL 299.108668 190.498115 \r\nL 300.143918 184.365744 \r\nL 301.179169 187.533689 \r\nL 302.214419 188.763737 \r\nL 303.24967 187.512857 \r\nL 304.28492 186.82555 \r\nL 305.32017 188.617674 \r\nL 306.355421 185.767375 \r\nL 307.390671 188.347925 \r\nL 308.425922 190.2078 \r\nL 310.496423 189.308641 \r\nL 311.531673 188.234749 \r\nL 312.566924 186.521003 \r\nL 313.602174 191.027626 \r\nL 314.637425 188.260167 \r\nL 315.672675 188.527694 \r\nL 317.743176 195.893399 \r\nL 318.778426 196.736315 \r\nL 319.813677 197.297465 \r\nL 320.848927 196.646072 \r\nL 321.884178 200.283134 \r\nL 322.919428 196.837918 \r\nL 323.954679 204.48543 \r\nL 324.989929 199.31562 \r\nL 326.02518 195.321497 \r\nL 327.06043 198.492232 \r\nL 328.095681 195.202633 \r\nL 329.130931 198.735722 \r\nL 330.166182 196.930635 \r\nL 331.201432 200.039136 \r\nL 332.236683 204.481288 \r\nL 333.271933 198.261473 \r\nL 335.342434 205.889421 \r\nL 336.377684 205.285659 \r\nL 337.412935 206.78237 \r\nL 338.448185 209.353157 \r\nL 339.483436 201.411204 \r\nL 341.553937 208.220408 \r\nL 342.589187 203.529099 \r\nL 343.624438 200.484916 \r\nL 344.659688 203.792956 \r\nL 345.694939 204.995413 \r\nL 346.730189 202.628018 \r\nL 347.765439 207.0206 \r\nL 348.80069 208.083189 \r\nL 349.83594 207.378889 \r\nL 350.871191 208.354828 \r\nL 351.906441 210.182605 \r\nL 352.941692 209.51997 \r\nL 353.976942 210.552322 \r\nL 355.012193 214.756364 \r\nL 356.047443 207.526387 \r\nL 356.047443 207.526387 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_14\">\r\n    <path clip-path=\"url(#p5f4b5bf5fa)\" d=\"M 51.683807 17.083636 \r\nL 52.719057 32.78044 \r\nL 53.754308 36.103729 \r\nL 54.789558 74.168641 \r\nL 55.824809 79.020104 \r\nL 56.860059 91.489674 \r\nL 57.89531 93.31107 \r\nL 58.93056 106.011397 \r\nL 59.965811 105.922818 \r\nL 61.001061 102.984294 \r\nL 62.036311 99.577367 \r\nL 63.071562 91.244951 \r\nL 64.106812 105.522365 \r\nL 65.142063 109.826748 \r\nL 66.177313 104.540464 \r\nL 67.212564 104.405376 \r\nL 68.247814 104.578458 \r\nL 69.283065 110.665295 \r\nL 70.318315 113.761873 \r\nL 71.353566 111.92598 \r\nL 72.388816 110.341827 \r\nL 73.424067 111.460968 \r\nL 74.459317 113.860383 \r\nL 75.494567 118.887548 \r\nL 76.529818 114.231023 \r\nL 78.600319 118.12519 \r\nL 79.635569 122.406164 \r\nL 80.67082 119.741758 \r\nL 81.70607 124.457933 \r\nL 82.741321 123.152357 \r\nL 83.776571 119.312613 \r\nL 84.811822 123.995723 \r\nL 85.847072 122.547514 \r\nL 86.882323 119.922764 \r\nL 87.917573 126.565028 \r\nL 89.988074 118.502097 \r\nL 91.023324 121.296473 \r\nL 92.058575 122.314882 \r\nL 93.093825 119.330295 \r\nL 94.129076 122.320435 \r\nL 95.164326 122.027015 \r\nL 96.199577 118.113811 \r\nL 97.234827 123.165557 \r\nL 98.270078 122.555391 \r\nL 99.305328 126.490949 \r\nL 100.340579 123.386122 \r\nL 101.375829 123.185079 \r\nL 102.41108 117.624753 \r\nL 103.44633 118.05903 \r\nL 104.48158 119.437756 \r\nL 105.516831 118.9505 \r\nL 106.552081 122.248334 \r\nL 107.587332 124.693055 \r\nL 108.622582 122.982992 \r\nL 109.657833 125.483727 \r\nL 110.693083 121.97583 \r\nL 111.728334 124.648367 \r\nL 112.763584 125.240234 \r\nL 113.798835 127.485392 \r\nL 114.834085 126.544739 \r\nL 115.869336 125.885865 \r\nL 116.904586 124.973512 \r\nL 117.939837 128.397089 \r\nL 118.975087 126.441515 \r\nL 120.010337 128.513229 \r\nL 121.045588 128.82728 \r\nL 122.080838 128.059084 \r\nL 123.116089 126.622739 \r\nL 124.151339 126.421204 \r\nL 125.18659 128.881937 \r\nL 126.22184 128.594455 \r\nL 127.257091 125.260488 \r\nL 128.292341 127.160612 \r\nL 129.327592 128.216709 \r\nL 130.362842 126.568684 \r\nL 132.433343 129.693367 \r\nL 133.468593 129.201008 \r\nL 135.539094 130.590945 \r\nL 136.574345 131.469683 \r\nL 137.609595 130.675092 \r\nL 138.644846 130.74518 \r\nL 139.680096 131.946142 \r\nL 140.715347 129.568014 \r\nL 141.750597 127.963762 \r\nL 142.785848 129.857443 \r\nL 143.821098 127.563933 \r\nL 144.856349 130.91706 \r\nL 145.891599 130.976611 \r\nL 146.926849 128.577056 \r\nL 147.9621 129.485648 \r\nL 148.99735 127.171555 \r\nL 150.032601 126.433538 \r\nL 151.067851 130.961779 \r\nL 152.103102 128.156469 \r\nL 153.138352 129.066024 \r\nL 154.173603 133.182122 \r\nL 155.208853 131.034437 \r\nL 156.244104 129.436871 \r\nL 157.279354 129.601424 \r\nL 159.349855 133.647362 \r\nL 160.385106 128.72911 \r\nL 161.420356 131.732666 \r\nL 162.455606 132.935474 \r\nL 163.490857 134.693309 \r\nL 165.561358 135.780724 \r\nL 166.596608 137.697951 \r\nL 167.631859 136.145511 \r\nL 168.667109 136.93356 \r\nL 169.70236 136.072246 \r\nL 170.73761 136.561561 \r\nL 171.772861 134.835777 \r\nL 172.808111 133.989868 \r\nL 173.843362 135.589145 \r\nL 174.878612 133.947451 \r\nL 175.913862 135.792467 \r\nL 176.949113 138.606584 \r\nL 177.984363 134.321206 \r\nL 180.054864 137.210524 \r\nL 181.090115 140.364065 \r\nL 182.125365 138.164944 \r\nL 183.160616 135.529599 \r\nL 184.195866 129.129507 \r\nL 185.231117 132.132826 \r\nL 186.266367 135.986115 \r\nL 187.301618 137.518668 \r\nL 188.336868 139.344796 \r\nL 189.372119 136.951141 \r\nL 190.407369 137.017836 \r\nL 191.442619 137.634872 \r\nL 192.47787 136.935646 \r\nL 193.51312 138.809746 \r\nL 194.548371 132.025247 \r\nL 195.583621 131.814678 \r\nL 196.618872 132.309306 \r\nL 197.654122 131.617942 \r\nL 198.689373 128.962691 \r\nL 199.724623 128.401666 \r\nL 200.759874 133.716563 \r\nL 202.830375 130.980041 \r\nL 203.865625 134.58229 \r\nL 204.900875 140.3185 \r\nL 205.936126 139.10426 \r\nL 206.971376 136.04194 \r\nL 208.006627 138.629039 \r\nL 209.041877 136.694949 \r\nL 210.077128 138.857064 \r\nL 211.112378 131.040447 \r\nL 212.147629 134.39928 \r\nL 213.182879 134.92421 \r\nL 214.21813 133.360953 \r\nL 215.25338 141.725709 \r\nL 216.288631 141.831381 \r\nL 217.323881 141.558513 \r\nL 218.359131 142.856795 \r\nL 220.429632 138.076422 \r\nL 221.464883 134.152047 \r\nL 222.500133 139.005431 \r\nL 223.535384 138.277391 \r\nL 224.570634 141.059365 \r\nL 225.605885 141.730684 \r\nL 226.641135 137.248733 \r\nL 227.676386 134.090498 \r\nL 228.711636 138.431619 \r\nL 229.746887 137.82257 \r\nL 230.782137 136.07167 \r\nL 231.817388 138.858119 \r\nL 232.852638 138.438538 \r\nL 233.887888 142.64906 \r\nL 234.923139 143.093487 \r\nL 235.958389 142.338857 \r\nL 236.99364 140.410181 \r\nL 238.02889 139.897799 \r\nL 239.064141 141.517761 \r\nL 240.099391 141.827475 \r\nL 241.134642 135.204346 \r\nL 242.169892 139.242868 \r\nL 243.205143 140.7813 \r\nL 244.240393 138.670194 \r\nL 245.275644 139.866438 \r\nL 246.310894 140.846388 \r\nL 247.346144 144.166715 \r\nL 248.381395 137.099288 \r\nL 249.416645 140.952974 \r\nL 250.451896 141.561507 \r\nL 251.487146 144.720717 \r\nL 252.522397 140.703335 \r\nL 253.557647 140.907733 \r\nL 254.592898 143.236096 \r\nL 255.628148 148.220255 \r\nL 256.663399 147.239916 \r\nL 257.698649 146.725786 \r\nL 258.7339 145.434494 \r\nL 259.76915 141.410087 \r\nL 260.804401 142.0771 \r\nL 261.839651 141.385896 \r\nL 262.874901 139.634494 \r\nL 263.910152 135.715649 \r\nL 264.945402 141.346908 \r\nL 265.980653 143.97488 \r\nL 267.015903 143.236519 \r\nL 268.051154 139.706964 \r\nL 269.086404 140.808232 \r\nL 270.121655 149.482478 \r\nL 271.156905 150.102158 \r\nL 272.192156 147.374774 \r\nL 273.227406 146.131169 \r\nL 274.262657 144.60013 \r\nL 275.297907 147.46207 \r\nL 276.333157 147.470873 \r\nL 277.368408 146.788102 \r\nL 278.403658 146.55525 \r\nL 280.474159 138.84865 \r\nL 281.50941 144.315165 \r\nL 282.54466 143.731892 \r\nL 283.579911 139.994819 \r\nL 284.615161 137.528505 \r\nL 285.650412 143.196946 \r\nL 286.685662 146.692395 \r\nL 287.720913 149.153478 \r\nL 288.756163 139.378096 \r\nL 289.791413 141.367615 \r\nL 290.826664 147.057627 \r\nL 291.861914 146.971325 \r\nL 292.897165 147.853959 \r\nL 293.932415 145.544391 \r\nL 294.967666 150.310431 \r\nL 296.002916 149.267505 \r\nL 297.038167 149.434383 \r\nL 298.073417 151.753933 \r\nL 299.108668 151.016059 \r\nL 300.143918 144.782301 \r\nL 302.214419 148.026668 \r\nL 303.24967 148.163621 \r\nL 304.28492 147.829325 \r\nL 305.32017 152.649048 \r\nL 306.355421 150.897141 \r\nL 307.390671 153.095207 \r\nL 308.425922 147.140966 \r\nL 309.461172 148.396316 \r\nL 310.496423 149.183919 \r\nL 311.531673 148.602864 \r\nL 312.566924 142.320451 \r\nL 313.602174 135.22801 \r\nL 314.637425 138.810484 \r\nL 315.672675 141.977941 \r\nL 316.707926 144.162542 \r\nL 317.743176 141.534064 \r\nL 318.778426 142.905968 \r\nL 319.813677 140.517829 \r\nL 320.848927 142.626302 \r\nL 321.884178 137.959829 \r\nL 322.919428 141.335655 \r\nL 323.954679 141.043228 \r\nL 324.989929 140.149428 \r\nL 326.02518 138.734161 \r\nL 327.06043 136.714088 \r\nL 328.095681 138.91431 \r\nL 329.130931 138.531376 \r\nL 330.166182 138.719212 \r\nL 331.201432 140.395061 \r\nL 332.236683 140.261652 \r\nL 333.271933 140.832832 \r\nL 334.307183 145.886085 \r\nL 335.342434 147.472145 \r\nL 336.377684 149.812874 \r\nL 337.412935 143.53665 \r\nL 338.448185 140.739397 \r\nL 339.483436 142.190113 \r\nL 340.518686 143.83391 \r\nL 342.589187 143.198591 \r\nL 343.624438 139.93957 \r\nL 344.659688 136.163751 \r\nL 346.730189 138.153593 \r\nL 347.765439 140.764862 \r\nL 348.80069 141.640966 \r\nL 349.83594 139.836427 \r\nL 350.871191 143.9585 \r\nL 351.906441 138.482863 \r\nL 352.941692 138.594396 \r\nL 353.976942 138.311534 \r\nL 355.012193 139.874649 \r\nL 356.047443 141.145046 \r\nL 356.047443 141.145046 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 36.465625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 371.265625 224.64 \r\nL 371.265625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 371.265625 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 36.465625 7.2 \r\nL 371.265625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p5f4b5bf5fa\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"36.465625\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.plot(clf.history['loss'][5:])\n",
    "plt.plot(clf.history['val_0_logloss'][5:])"
   ]
  },
  {
   "source": [
    "## 7) Neural Network using PyTorch(미완성)\n",
    "* PyTorch 튜토리얼 참고함\n",
    "* 보기에는 loss가 낮게 나오긴 한데 validation set에 대한 accuracy도 낮고, 실제로 predict값을 확인해보면 학습할수록 예측값이 2로 치우쳐져 있음... 이것저것 해보면서 수정할 예정\n",
    "* logloss는 아직 안뽑아봄"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "device : cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from  torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('device : {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ts = torch.from_numpy(X_train.values)\n",
    "Y_train_ts = np.reshape(Y_train.values, ((Y_train.values.shape[0], 1)))\n",
    "Y_train_ts = torch.from_numpy(Y_train_ts)\n",
    "\n",
    "X_val_ts = torch.from_numpy(X_val.values)\n",
    "Y_val_ts = np.reshape(Y_val.values, ((Y_val.values.shape[0], 1)))\n",
    "Y_val_ts = torch.from_numpy(Y_val_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train_ts, Y_train_ts)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val_ts, Y_val_ts)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.043623\n",
      "Test Error: \n",
      " Accuracy: 54.4%, Avg loss: 0.105044 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.990763\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Avg loss: 0.099727 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.967593\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.096124 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.924047\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.094059 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.919947\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.092716 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.917004\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.091642 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.901252\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.091211 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.911933\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.090974 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.904229\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.090656 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.907615\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.090468 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.889362\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.090115 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.897094\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.089890 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.900430\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.089993 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.877917\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.090012 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.889668\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.089879 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.898133\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.090001 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.909671\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.089980 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.871264\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.089995 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.886300\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.089697 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.884738\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.089882 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.911368\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.089790 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.859194\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.089896 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.866716\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.089482 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.898161\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.089586 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.880817\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.089919 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.879343\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.089797 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.875105\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.089692 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.862707\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.089723 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.873609\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.090011 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.866503\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.089687 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.883527\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.089848 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.918122\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.089732 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.900352\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.089873 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.879232\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.089675 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.897815\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.089624 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.890801\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.089800 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.896767\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.089614 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.857145\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 0.089405 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.901287\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.090101 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.885089\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.089949 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.879122\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.089662 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.883504\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.090109 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.877569\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.089980 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.887266\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.089818 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.875162\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 0.090355 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.871140\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.089585 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.883717\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.090328 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.886134\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.090142 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.868526\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.090019 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.890184\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.089559 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.892244\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.090019 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.838882\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.090144 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.841668\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.089943 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.881349\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.089868 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.861265\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.089724 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.858283\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.089846 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.872125\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.089912 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.876353\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.090085 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.868764\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.089791 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.863776\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.089815 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.884288\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.090148 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.879640\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.089848 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.877372\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.089806 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.871152\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.090219 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.868123\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.089740 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.876085\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.089855 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.875566\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.090351 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.882686\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.089833 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.870370\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.089977 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.861005\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.090185 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.859879\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.089601 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.855822\n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.089502 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.894916\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.090028 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.863057\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.089669 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.872066\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.090047 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.906616\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.089969 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.891612\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.090166 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.881250\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.089732 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.836982\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.090144 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.858561\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.090088 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.890011\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.090039 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.898369\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.090254 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.835010\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.089741 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.868988\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.090171 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.835389\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.089831 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.861649\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.089952 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.879637\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.090183 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.864085\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 0.089729 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.866816\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.089728 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.889097\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.089898 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.852480\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.090253 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.841112\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.090104 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.874286\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.090410 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.849242\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.090349 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.856006\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.089670 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.838270\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.089902 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.848676\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.090012 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.879288\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.090069 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.821361\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.089899 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.845332\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 0.090336 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(17, 50),\n",
    "            nn.BatchNorm1d(50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.BatchNorm1d(50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()\n",
    "learning_rate = 1e-3\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # 예측(prediction)과 손실(loss) 계산\n",
    "        X=X.float()\n",
    "        pred = model(X)\n",
    "        y = y.reshape(y.shape[0],).long()\n",
    "        #print(f\"{pred.shape}, {y.shape}\")\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # 역전파\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    loss, current = loss.item(), batch * len(X)\n",
    "    print(f\"loss: {loss:>7f}\")\n",
    "            \n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.float()\n",
    "            pred = model(X)\n",
    "            y = y.reshape(y.shape[0],).long()\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 100\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(val_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}