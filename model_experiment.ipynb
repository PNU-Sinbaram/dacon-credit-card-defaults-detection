{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd0bed0ea09e0a217f0cd8c3af8b97b5ce48feb5846fec0e29c23d707f4dd7f9787",
   "display_name": "Python 3.8.3 64-bit ('mlenv': venv)",
   "language": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 0) 데이터 전처리\n",
    "* Baseline코드에서 사용했던 전처리 방식을 그대로 사용\n",
    "* index column은 학습에 관련이 없으니 제거"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#train = pd.read_csv(\"../data/train.csv\")                            #r*c = 20000*20, test와 달리 credit이라는 column을 갖고 있고, 이 값을 예측\n",
    "#test = pd.read_csv(\"../data/test.csv\")                              #10000*19. train으로 학습시키고 test데이터를 입력으로 넣어서 credit을 예측\n",
    "#sample_submission = pd.read_csv(\"../data/sample_submission.csv\")    #예측값은 sample_submission과 형태가 같아야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../../PracticeCodes/DACON/data/train.csv\")                            #r*c = 20000*20, test와 달리 credit이라는 column을 갖고 있고, 이 값을 예측\n",
    "test = pd.read_csv(\"../../PracticeCodes/DACON/data/test.csv\")                              #10000*19. train으로 학습시키고 test데이터를 입력으로 넣어서 credit을 예측\n",
    "sample_submission = pd.read_csv(\"../../PracticeCodes/DACON/data/sample_submission.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train, test], axis=0)    # train데이터 밑에 test데이터가 붙음. test에는 credit이 없으므로, 결측치(NaN)형태로 저장됨\n",
    "# 실제로는 결측치를 완전히 날리는건 좋지 않지만, 1회 출제용으로 사용하기때문에 완전히 날림\n",
    "data = data.drop(\"occyp_type\", axis=1) # occyp_type column을 지움. axis : occyp_type이 row에 있는지 column에 있는지 알려줌. 1이면 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_len = data.apply(lambda x : len(x.unique()))  # 각 column별로 unique()를 수행하여, 그 길이를 반환. 즉, 모든 column의 요소의 개수를 출력\n",
    "group_1 = unique_len[unique_len <= 2].index   # 요소의 값이 2개 이하인 column들의 이름을 추출\n",
    "group_2 = unique_len[(unique_len > 2) & (unique_len <= 10)].index\n",
    "group_3 = unique_len[(unique_len > 10)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['gender'] = data['gender'].replace(['F','M'], [0, 1])   # F를 0으로, M을 1로 교체\n",
    "data['car'] = data['car'].replace(['N', 'Y'], [0, 1])\n",
    "data['reality'] = data['reality'].replace(['N', 'Y'], [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['child_num']>2, 'child_num'] = 2  # child_num>2인 child_num column을 가져와서 2로 바꿈\n",
    "data[group_2].apply(lambda x : len(x.unique()))\n",
    "label_encoder = preprocessing.LabelEncoder() # categorical 변수(문자로 되어있는 변수)들을 숫자로 인코딩해주는 함수\n",
    "set(label_encoder.fit_transform(data['income_type'])) # income_type column에서 각 요소들을 숫자로 바꿔줌. fit_transform이 배열을 반환해서 unique()대신 set을 사용\n",
    "data['income_type'] = label_encoder.fit_transform(data['income_type'])\n",
    "data['edu_type'] = label_encoder.fit_transform(data['edu_type'])\n",
    "data['family_type'] = label_encoder.fit_transform(data['family_type'])\n",
    "data['house_type'] = label_encoder.fit_transform(data['house_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, bin_dividers = np.histogram(data['income_total'], bins=7) # 연속형 변수를 입력으로 받아 몇 개의 구간으로 나눌지 설정해줌. 각 구간들의 분절점(나누는 기준) 및 구간별 요소의 개수를 출력해줌\n",
    "data['income_total'] = pd.factorize(pd.cut(data['income_total'], bins = bin_dividers, include_lowest=True, labels=[0,1,2,3,4,5,6]))[0] # pd.cut의 반환 데이터 타입은 category이기 때문에, series타입(int형 배열)으로 바꿔주는 작업을 거쳐야 함\n",
    "#위의 과정을 함수로 만듬\n",
    "def make_bin(array, N):\n",
    "    array = -array      #DAYS_BIRTH등의 column은 음수이기 때문에 양수로 바꿔줌\n",
    "    _, bin_dividers = np.histogram(array, bins = N)       # 여기선 counts 변수를 사용하지 않을 것이기 때문에 사용하지 않는다는 의미로 _로 설정.\n",
    "    cut_categories = pd.cut(array, bin_dividers, labels = [i for i in range(N)], include_lowest=True)\n",
    "    bined_array = pd.factorize(cut_categories)[0]\n",
    "    return bined_array\n",
    "data['DAYS_BIRTH'] = make_bin(data['DAYS_BIRTH'], 10)\n",
    "data['DAYS_EMPLOYED'] = make_bin(data['DAYS_EMPLOYED'], 10)\n",
    "data['begin_month'] = make_bin(data['begin_month'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop('index', axis=1)\n",
    "train = data[:-10000]   # train에 해당하는 값\n",
    "test = data[-10000:]   #test에 해당하는 값\n",
    "train_x = train.drop(\"credit\", axis = 1) # credit은 출력값이고, credit을 제외한 값들이 모델의 입력값이므로 column들 중(axis =1) credit을 찾아 없앰.\n",
    "train_y = train['credit']               # 모델의 출력이 credit\n",
    "test_x = test.drop(\"credit\", axis=1)        # data라는 dataframe을 만들면서 test set에 없던 credit이라는 column이 생겼으므로, 이를 다시 제거"
   ]
  },
  {
   "source": [
    "## 1) RandomForestClassifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, stratify=train_y, test_size=0.2, random_state=10086)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.466047343279552\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, Y_train)\n",
    "predict = clf.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "logloss = log_loss(y_val_onehot, predict)\n",
    "print(logloss)"
   ]
  },
  {
   "source": [
    "### 1_1) 하이퍼파라미터 조정"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_param_grid = {\n",
    "    'n_estimators' : [100, 200],\n",
    "    'max_depth' : [8, 10, 12],\n",
    "    'min_samples_leaf' : [3, 5, 7],\n",
    "    'min_samples_split' : [3, 4, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "0.8262986236456679\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, stratify=train_y, test_size=0.25, random_state=10086)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf_grid = GridSearchCV(clf, param_grid=rf_param_grid, scoring=\"accuracy\", n_jobs=-1, verbose=2)\n",
    "clf_grid.fit(X_train, Y_train)\n",
    "\n",
    "predict = clf_grid.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "logloss=log_loss(y_val_onehot, predict)\n",
    "print(logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[0.8282641291283019, 0.8225467954824329, 0.8253606275571261, 0.8224319510375313, 0.8201274143462735]\n"
     ]
    }
   ],
   "source": [
    "folds = StratifiedKFold(n_splits = 5, shuffle=True, random_state = 55)  # 5개의 fold로 나눔\n",
    "\n",
    "outcomes = []\n",
    "for n_fold, (train_index, val_index) in enumerate(folds.split(train_x, train_y)):\n",
    "    X_train, X_val = train_x.iloc[train_index], train_x.iloc[val_index]\n",
    "    Y_train, Y_val = train_y.iloc[train_index], train_y.iloc[val_index]\n",
    "    clf = RandomForestClassifier()\n",
    "    clf_grid = GridSearchCV(clf, param_grid=rf_param_grid, scoring=\"accuracy\", n_jobs=-1, verbose=2)\n",
    "    clf_grid.fit(X_train, Y_train)\n",
    "    predictions = clf_grid.predict_proba(X_val)\n",
    "    y_val_onehot = pd.get_dummies(Y_val)\n",
    "\n",
    "    logloss = log_loss(y_val_onehot, predictions)\n",
    "    outcomes.append(logloss)\n",
    "\n",
    "print(outcomes)"
   ]
  },
  {
   "source": [
    "## 2) Gradient Boosting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8335746908166416\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, stratify=train_y, test_size=0.25, random_state=10086)\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(X_train, Y_train)\n",
    "predict = gb.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "logloss = log_loss(y_val_onehot, predict)\n",
    "print(logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.8417415560977669, 0.8341085098795408, 0.8386208102239824, 0.8392900501671904, 0.8335760935299279]\n"
     ]
    }
   ],
   "source": [
    "folds = StratifiedKFold(n_splits = 5, shuffle=True, random_state = 55)  # 5개의 fold로 나눔\n",
    "\n",
    "outcomes = []\n",
    "for n_fold, (train_index, val_index) in enumerate(folds.split(train_x, train_y)):\n",
    "    X_train, X_val = train_x.iloc[train_index], train_x.iloc[val_index]\n",
    "    Y_train, Y_val = train_y.iloc[train_index], train_y.iloc[val_index]\n",
    "    gb = GradientBoostingClassifier()\n",
    "    gb.fit(X_train, Y_train)\n",
    "    predictions = gb.predict_proba(X_val)\n",
    "    y_val_onehot = pd.get_dummies(Y_val)\n",
    "\n",
    "    logloss = log_loss(y_val_onehot, predictions)\n",
    "    outcomes.append(logloss)\n",
    "print(outcomes)"
   ]
  },
  {
   "source": [
    "### 2_1) Gradient Boosting parameter 조정\n",
    "* **주의. 한번 실행에 시간 소요가 너무 김**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_param_grid = {\n",
    "    'n_estimators' : [100, 200],\n",
    "    'max_depth' : [8, 10, 12],\n",
    "    'min_samples_leaf' : [5, 7, 10],\n",
    "    'min_samples_split' : [2, 3, 5],\n",
    "    'learning_rate' : [0.05, 0.1, 0.2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-4a78ca480250>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mgb_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgb_param_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mgb_grid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgb_grid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0my_val_onehot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pythonenv\\mlenv\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pythonenv\\mlenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    839\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    842\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pythonenv\\mlenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1294\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1295\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1296\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pythonenv\\mlenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    793\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[0;32m    796\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pythonenv\\mlenv\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1054\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1055\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pythonenv\\mlenv\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    931\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pythonenv\\mlenv\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    432\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "gb_grid = GridSearchCV(gb, param_grid = gb_param_grid, scoring=\"accuracy\", n_jobs=-1, verbose=3)\n",
    "gb_grid.fit(X_train, Y_train)\n",
    "predict = gb_grid.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "logloss = log_loss(y_val_onehot, predict)\n",
    "print(logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'learning_rate': 0.05,\n",
       " 'max_depth': 8,\n",
       " 'min_samples_leaf': 7,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 100}"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "gb_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.8299092537458699, 0.8195784379737493, 0.822702511343288, 0.8219319425895892, 0.8183218626777928]\n"
     ]
    }
   ],
   "source": [
    "folds = StratifiedKFold(n_splits = 5, shuffle=True, random_state = 55)  # 5개의 fold로 나눔\n",
    "\n",
    "outcomes = []\n",
    "for n_fold, (train_index, val_index) in enumerate(folds.split(train_x, train_y)):\n",
    "    X_train, X_val = train_x.iloc[train_index], train_x.iloc[val_index]\n",
    "    Y_train, Y_val = train_y.iloc[train_index], train_y.iloc[val_index]\n",
    "    gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.05, max_depth=8, min_samples_leaf=7, min_samples_split=2)\n",
    "    gb.fit(X_train, Y_train)\n",
    "    predict = gb.predict_proba(X_val)\n",
    "    y_val_onehot = pd.get_dummies(Y_val)\n",
    "    logloss = log_loss(y_val_onehot, predict)\n",
    "    outcomes.append(logloss)\n",
    "print(outcomes)"
   ]
  },
  {
   "source": [
    "## 3) AdaBoost"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.0877788002340585"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, stratify=train_y, test_size=0.25, random_state=10086)\n",
    "\n",
    "ag = AdaBoostClassifier()\n",
    "ag.fit(X_train, Y_train)\n",
    "\n",
    "predict = ag.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "log_loss(y_val_onehot, predict)"
   ]
  },
  {
   "source": [
    "## 4) XgBoost"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[20:56:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "[20:56:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "[20:56:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "[20:56:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "[20:56:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.8324195141525684, 0.8266500276527476, 0.8265990901853171, 0.8235172349839919, 0.8234311175080091]\n",
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "outcomes = []\n",
    "for n_fold, (train_index, val_index) in enumerate(folds.split(train_x, train_y)):\n",
    "    X_train, X_val = train_x.iloc[train_index], train_x.iloc[val_index]\n",
    "    Y_train, Y_val = train_y.iloc[train_index], train_y.iloc[val_index]\n",
    "    xgb = XGBClassifier(n_estimators=500, learning_rate=0.1, max_depth = 4, use_label_encoder=False)\n",
    "    xgb.fit(X_train, Y_train)\n",
    "    predictions = xgb.predict_proba(X_val)\n",
    "    y_val_onehot = pd.get_dummies(Y_val)\n",
    "\n",
    "    logloss = log_loss(y_val_onehot, predictions)\n",
    "    outcomes.append(logloss)\n",
    "print(outcomes)"
   ]
  },
  {
   "source": [
    "### 4_1) XgBoost hyperparameter tuning\n",
    "* **주의. 한번 실행에 시간 소요가 너무 김**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "xgb_param_grid = {\n",
    "    'n_estimators' : [300, 500, 600],\n",
    "    'learning_rate' : [0.01, 0.05, 0.1, 0.15],\n",
    "    'max_depth' : [3, 4, 6, 8]\n",
    "}\n",
    "\n",
    "xgb_grid = GridSearchCV(xgb, param_grid = xgb_param_grid, scoring=\"accuracy\", n_jobs= -1, verbose = 3)\n",
    "xgb_grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "실행 시간이 너무 오래 걸려 전에 돌려놨던 결과로 대체함\n",
    "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
    "c:\\Users\\lijm1\\Desktop\\pythonenv\\mlenv\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
    "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
    "[01:20:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
    "GridSearchCV(estimator=XGBClassifier(base_score=None, booster=None,\n",
    "                                     colsample_bylevel=None,\n",
    "                                     colsample_bynode=None,\n",
    "                                     colsample_bytree=None, gamma=None,\n",
    "                                     gpu_id=None, importance_type='gain',\n",
    "                                     interaction_constraints=None,\n",
    "                                     learning_rate=None, max_delta_step=None,\n",
    "                                     max_depth=None, min_child_weight=None,\n",
    "                                     missing=nan, monotone_constraints=None,\n",
    "                                     n_estimators=100, n_jobs=None,\n",
    "                                     num_parallel_tree=None, random_state=None,\n",
    "                                     reg_alpha=None, reg_lambda=None,\n",
    "                                     scale_pos_weight=None, subsample=None,\n",
    "                                     tree_method=None, validate_parameters=None,\n",
    "                                     verbosity=None),\n",
    "             n_jobs=-1,\n",
    "             param_grid={'learning_rate': [0.01], 'max_depth': [4],\n",
    "                         'n_estimators': [500]},\n",
    "             scoring='accuracy', verbose=3)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[20:57:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8359875737021389"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, stratify=train_y, test_size=0.25, random_state=10086)\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=500, learning_rate=0.01, max_depth = 4, use_label_encoder=False)\n",
    "xgb.fit(X_train, Y_train)\n",
    "predictions = xgb.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "\n",
    "log_loss(y_val_onehot, predictions)"
   ]
  },
  {
   "source": [
    "## 5) LightGBM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8248750259707588"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier, plot_importance\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, stratify=train_y, test_size=0.25, random_state=10086)\n",
    "\n",
    "lgb = LGBMClassifier(n_estimators=400)\n",
    "lgb.fit(X_train, Y_train)\n",
    "predict = lgb.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "log_loss(y_val_onehot, predict)"
   ]
  },
  {
   "source": [
    "### 5_1) LGBM Early stopping 적용"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1]\tvalid_0's multi_logloss: 0.871384\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tvalid_0's multi_logloss: 0.863215\n",
      "[3]\tvalid_0's multi_logloss: 0.857059\n",
      "[4]\tvalid_0's multi_logloss: 0.852325\n",
      "[5]\tvalid_0's multi_logloss: 0.848258\n",
      "[6]\tvalid_0's multi_logloss: 0.84532\n",
      "[7]\tvalid_0's multi_logloss: 0.842952\n",
      "[8]\tvalid_0's multi_logloss: 0.840694\n",
      "[9]\tvalid_0's multi_logloss: 0.83901\n",
      "[10]\tvalid_0's multi_logloss: 0.837603\n",
      "[11]\tvalid_0's multi_logloss: 0.836505\n",
      "[12]\tvalid_0's multi_logloss: 0.835527\n",
      "[13]\tvalid_0's multi_logloss: 0.834671\n",
      "[14]\tvalid_0's multi_logloss: 0.833793\n",
      "[15]\tvalid_0's multi_logloss: 0.833183\n",
      "[16]\tvalid_0's multi_logloss: 0.83234\n",
      "[17]\tvalid_0's multi_logloss: 0.831761\n",
      "[18]\tvalid_0's multi_logloss: 0.831425\n",
      "[19]\tvalid_0's multi_logloss: 0.830885\n",
      "[20]\tvalid_0's multi_logloss: 0.830542\n",
      "[21]\tvalid_0's multi_logloss: 0.830351\n",
      "[22]\tvalid_0's multi_logloss: 0.830166\n",
      "[23]\tvalid_0's multi_logloss: 0.830036\n",
      "[24]\tvalid_0's multi_logloss: 0.829671\n",
      "[25]\tvalid_0's multi_logloss: 0.829519\n",
      "[26]\tvalid_0's multi_logloss: 0.829229\n",
      "[27]\tvalid_0's multi_logloss: 0.829262\n",
      "[28]\tvalid_0's multi_logloss: 0.829037\n",
      "[29]\tvalid_0's multi_logloss: 0.828849\n",
      "[30]\tvalid_0's multi_logloss: 0.828643\n",
      "[31]\tvalid_0's multi_logloss: 0.828665\n",
      "[32]\tvalid_0's multi_logloss: 0.828359\n",
      "[33]\tvalid_0's multi_logloss: 0.828262\n",
      "[34]\tvalid_0's multi_logloss: 0.827804\n",
      "[35]\tvalid_0's multi_logloss: 0.827619\n",
      "[36]\tvalid_0's multi_logloss: 0.827389\n",
      "[37]\tvalid_0's multi_logloss: 0.827356\n",
      "[38]\tvalid_0's multi_logloss: 0.827293\n",
      "[39]\tvalid_0's multi_logloss: 0.826958\n",
      "[40]\tvalid_0's multi_logloss: 0.826816\n",
      "[41]\tvalid_0's multi_logloss: 0.826786\n",
      "[42]\tvalid_0's multi_logloss: 0.826573\n",
      "[43]\tvalid_0's multi_logloss: 0.82634\n",
      "[44]\tvalid_0's multi_logloss: 0.826291\n",
      "[45]\tvalid_0's multi_logloss: 0.826135\n",
      "[46]\tvalid_0's multi_logloss: 0.826101\n",
      "[47]\tvalid_0's multi_logloss: 0.825966\n",
      "[48]\tvalid_0's multi_logloss: 0.825712\n",
      "[49]\tvalid_0's multi_logloss: 0.825559\n",
      "[50]\tvalid_0's multi_logloss: 0.825187\n",
      "[51]\tvalid_0's multi_logloss: 0.825019\n",
      "[52]\tvalid_0's multi_logloss: 0.825123\n",
      "[53]\tvalid_0's multi_logloss: 0.825115\n",
      "[54]\tvalid_0's multi_logloss: 0.824682\n",
      "[55]\tvalid_0's multi_logloss: 0.824667\n",
      "[56]\tvalid_0's multi_logloss: 0.824501\n",
      "[57]\tvalid_0's multi_logloss: 0.824404\n",
      "[58]\tvalid_0's multi_logloss: 0.824241\n",
      "[59]\tvalid_0's multi_logloss: 0.824274\n",
      "[60]\tvalid_0's multi_logloss: 0.824144\n",
      "[61]\tvalid_0's multi_logloss: 0.823991\n",
      "[62]\tvalid_0's multi_logloss: 0.823945\n",
      "[63]\tvalid_0's multi_logloss: 0.823793\n",
      "[64]\tvalid_0's multi_logloss: 0.82363\n",
      "[65]\tvalid_0's multi_logloss: 0.823577\n",
      "[66]\tvalid_0's multi_logloss: 0.823364\n",
      "[67]\tvalid_0's multi_logloss: 0.823236\n",
      "[68]\tvalid_0's multi_logloss: 0.823105\n",
      "[69]\tvalid_0's multi_logloss: 0.823008\n",
      "[70]\tvalid_0's multi_logloss: 0.823012\n",
      "[71]\tvalid_0's multi_logloss: 0.823018\n",
      "[72]\tvalid_0's multi_logloss: 0.822906\n",
      "[73]\tvalid_0's multi_logloss: 0.822797\n",
      "[74]\tvalid_0's multi_logloss: 0.822914\n",
      "[75]\tvalid_0's multi_logloss: 0.822859\n",
      "[76]\tvalid_0's multi_logloss: 0.822457\n",
      "[77]\tvalid_0's multi_logloss: 0.822447\n",
      "[78]\tvalid_0's multi_logloss: 0.822259\n",
      "[79]\tvalid_0's multi_logloss: 0.822138\n",
      "[80]\tvalid_0's multi_logloss: 0.822083\n",
      "[81]\tvalid_0's multi_logloss: 0.822013\n",
      "[82]\tvalid_0's multi_logloss: 0.822043\n",
      "[83]\tvalid_0's multi_logloss: 0.822145\n",
      "[84]\tvalid_0's multi_logloss: 0.822083\n",
      "[85]\tvalid_0's multi_logloss: 0.821986\n",
      "[86]\tvalid_0's multi_logloss: 0.821953\n",
      "[87]\tvalid_0's multi_logloss: 0.821722\n",
      "[88]\tvalid_0's multi_logloss: 0.821543\n",
      "[89]\tvalid_0's multi_logloss: 0.821408\n",
      "[90]\tvalid_0's multi_logloss: 0.821313\n",
      "[91]\tvalid_0's multi_logloss: 0.821416\n",
      "[92]\tvalid_0's multi_logloss: 0.820971\n",
      "[93]\tvalid_0's multi_logloss: 0.820791\n",
      "[94]\tvalid_0's multi_logloss: 0.820557\n",
      "[95]\tvalid_0's multi_logloss: 0.820226\n",
      "[96]\tvalid_0's multi_logloss: 0.820044\n",
      "[97]\tvalid_0's multi_logloss: 0.819827\n",
      "[98]\tvalid_0's multi_logloss: 0.819404\n",
      "[99]\tvalid_0's multi_logloss: 0.819393\n",
      "[100]\tvalid_0's multi_logloss: 0.819325\n",
      "[101]\tvalid_0's multi_logloss: 0.81921\n",
      "[102]\tvalid_0's multi_logloss: 0.819381\n",
      "[103]\tvalid_0's multi_logloss: 0.819383\n",
      "[104]\tvalid_0's multi_logloss: 0.819118\n",
      "[105]\tvalid_0's multi_logloss: 0.818937\n",
      "[106]\tvalid_0's multi_logloss: 0.818774\n",
      "[107]\tvalid_0's multi_logloss: 0.818807\n",
      "[108]\tvalid_0's multi_logloss: 0.818723\n",
      "[109]\tvalid_0's multi_logloss: 0.818886\n",
      "[110]\tvalid_0's multi_logloss: 0.818767\n",
      "[111]\tvalid_0's multi_logloss: 0.818832\n",
      "[112]\tvalid_0's multi_logloss: 0.818768\n",
      "[113]\tvalid_0's multi_logloss: 0.818561\n",
      "[114]\tvalid_0's multi_logloss: 0.818637\n",
      "[115]\tvalid_0's multi_logloss: 0.8185\n",
      "[116]\tvalid_0's multi_logloss: 0.818314\n",
      "[117]\tvalid_0's multi_logloss: 0.818235\n",
      "[118]\tvalid_0's multi_logloss: 0.818215\n",
      "[119]\tvalid_0's multi_logloss: 0.818015\n",
      "[120]\tvalid_0's multi_logloss: 0.818105\n",
      "[121]\tvalid_0's multi_logloss: 0.817966\n",
      "[122]\tvalid_0's multi_logloss: 0.817894\n",
      "[123]\tvalid_0's multi_logloss: 0.817587\n",
      "[124]\tvalid_0's multi_logloss: 0.817499\n",
      "[125]\tvalid_0's multi_logloss: 0.817386\n",
      "[126]\tvalid_0's multi_logloss: 0.817303\n",
      "[127]\tvalid_0's multi_logloss: 0.81739\n",
      "[128]\tvalid_0's multi_logloss: 0.817411\n",
      "[129]\tvalid_0's multi_logloss: 0.817387\n",
      "[130]\tvalid_0's multi_logloss: 0.817349\n",
      "[131]\tvalid_0's multi_logloss: 0.817435\n",
      "[132]\tvalid_0's multi_logloss: 0.817424\n",
      "[133]\tvalid_0's multi_logloss: 0.817413\n",
      "[134]\tvalid_0's multi_logloss: 0.817493\n",
      "[135]\tvalid_0's multi_logloss: 0.817377\n",
      "[136]\tvalid_0's multi_logloss: 0.81738\n",
      "[137]\tvalid_0's multi_logloss: 0.817454\n",
      "[138]\tvalid_0's multi_logloss: 0.817259\n",
      "[139]\tvalid_0's multi_logloss: 0.81714\n",
      "[140]\tvalid_0's multi_logloss: 0.816845\n",
      "[141]\tvalid_0's multi_logloss: 0.816691\n",
      "[142]\tvalid_0's multi_logloss: 0.816643\n",
      "[143]\tvalid_0's multi_logloss: 0.816532\n",
      "[144]\tvalid_0's multi_logloss: 0.81637\n",
      "[145]\tvalid_0's multi_logloss: 0.81613\n",
      "[146]\tvalid_0's multi_logloss: 0.815985\n",
      "[147]\tvalid_0's multi_logloss: 0.816012\n",
      "[148]\tvalid_0's multi_logloss: 0.816032\n",
      "[149]\tvalid_0's multi_logloss: 0.81607\n",
      "[150]\tvalid_0's multi_logloss: 0.815985\n",
      "[151]\tvalid_0's multi_logloss: 0.816081\n",
      "[152]\tvalid_0's multi_logloss: 0.815991\n",
      "[153]\tvalid_0's multi_logloss: 0.815889\n",
      "[154]\tvalid_0's multi_logloss: 0.815904\n",
      "[155]\tvalid_0's multi_logloss: 0.815886\n",
      "[156]\tvalid_0's multi_logloss: 0.815752\n",
      "[157]\tvalid_0's multi_logloss: 0.815806\n",
      "[158]\tvalid_0's multi_logloss: 0.815742\n",
      "[159]\tvalid_0's multi_logloss: 0.815868\n",
      "[160]\tvalid_0's multi_logloss: 0.815914\n",
      "[161]\tvalid_0's multi_logloss: 0.815892\n",
      "[162]\tvalid_0's multi_logloss: 0.815838\n",
      "[163]\tvalid_0's multi_logloss: 0.815685\n",
      "[164]\tvalid_0's multi_logloss: 0.815754\n",
      "[165]\tvalid_0's multi_logloss: 0.815742\n",
      "[166]\tvalid_0's multi_logloss: 0.815818\n",
      "[167]\tvalid_0's multi_logloss: 0.815877\n",
      "[168]\tvalid_0's multi_logloss: 0.815856\n",
      "[169]\tvalid_0's multi_logloss: 0.816004\n",
      "[170]\tvalid_0's multi_logloss: 0.816022\n",
      "[171]\tvalid_0's multi_logloss: 0.816048\n",
      "[172]\tvalid_0's multi_logloss: 0.8161\n",
      "[173]\tvalid_0's multi_logloss: 0.816062\n",
      "[174]\tvalid_0's multi_logloss: 0.816022\n",
      "[175]\tvalid_0's multi_logloss: 0.815977\n",
      "[176]\tvalid_0's multi_logloss: 0.81602\n",
      "[177]\tvalid_0's multi_logloss: 0.816042\n",
      "[178]\tvalid_0's multi_logloss: 0.816101\n",
      "[179]\tvalid_0's multi_logloss: 0.816191\n",
      "[180]\tvalid_0's multi_logloss: 0.816228\n",
      "[181]\tvalid_0's multi_logloss: 0.8162\n",
      "[182]\tvalid_0's multi_logloss: 0.816205\n",
      "[183]\tvalid_0's multi_logloss: 0.816312\n",
      "[184]\tvalid_0's multi_logloss: 0.816287\n",
      "[185]\tvalid_0's multi_logloss: 0.816403\n",
      "[186]\tvalid_0's multi_logloss: 0.816519\n",
      "[187]\tvalid_0's multi_logloss: 0.816444\n",
      "[188]\tvalid_0's multi_logloss: 0.816421\n",
      "[189]\tvalid_0's multi_logloss: 0.816298\n",
      "[190]\tvalid_0's multi_logloss: 0.816265\n",
      "[191]\tvalid_0's multi_logloss: 0.816343\n",
      "[192]\tvalid_0's multi_logloss: 0.816423\n",
      "[193]\tvalid_0's multi_logloss: 0.816623\n",
      "[194]\tvalid_0's multi_logloss: 0.81658\n",
      "[195]\tvalid_0's multi_logloss: 0.816574\n",
      "[196]\tvalid_0's multi_logloss: 0.816605\n",
      "[197]\tvalid_0's multi_logloss: 0.816762\n",
      "[198]\tvalid_0's multi_logloss: 0.816645\n",
      "[199]\tvalid_0's multi_logloss: 0.816797\n",
      "[200]\tvalid_0's multi_logloss: 0.816738\n",
      "[201]\tvalid_0's multi_logloss: 0.816695\n",
      "[202]\tvalid_0's multi_logloss: 0.816749\n",
      "[203]\tvalid_0's multi_logloss: 0.816764\n",
      "[204]\tvalid_0's multi_logloss: 0.816815\n",
      "[205]\tvalid_0's multi_logloss: 0.816856\n",
      "[206]\tvalid_0's multi_logloss: 0.816949\n",
      "[207]\tvalid_0's multi_logloss: 0.816955\n",
      "[208]\tvalid_0's multi_logloss: 0.816852\n",
      "[209]\tvalid_0's multi_logloss: 0.816964\n",
      "[210]\tvalid_0's multi_logloss: 0.817072\n",
      "[211]\tvalid_0's multi_logloss: 0.817013\n",
      "[212]\tvalid_0's multi_logloss: 0.817144\n",
      "[213]\tvalid_0's multi_logloss: 0.817175\n",
      "[214]\tvalid_0's multi_logloss: 0.817195\n",
      "[215]\tvalid_0's multi_logloss: 0.817245\n",
      "[216]\tvalid_0's multi_logloss: 0.817418\n",
      "[217]\tvalid_0's multi_logloss: 0.817516\n",
      "[218]\tvalid_0's multi_logloss: 0.817628\n",
      "[219]\tvalid_0's multi_logloss: 0.817768\n",
      "[220]\tvalid_0's multi_logloss: 0.817828\n",
      "[221]\tvalid_0's multi_logloss: 0.81796\n",
      "[222]\tvalid_0's multi_logloss: 0.818176\n",
      "[223]\tvalid_0's multi_logloss: 0.8182\n",
      "[224]\tvalid_0's multi_logloss: 0.818271\n",
      "[225]\tvalid_0's multi_logloss: 0.818258\n",
      "[226]\tvalid_0's multi_logloss: 0.818392\n",
      "[227]\tvalid_0's multi_logloss: 0.818549\n",
      "[228]\tvalid_0's multi_logloss: 0.818574\n",
      "[229]\tvalid_0's multi_logloss: 0.818664\n",
      "[230]\tvalid_0's multi_logloss: 0.818693\n",
      "[231]\tvalid_0's multi_logloss: 0.818632\n",
      "[232]\tvalid_0's multi_logloss: 0.818626\n",
      "[233]\tvalid_0's multi_logloss: 0.818638\n",
      "[234]\tvalid_0's multi_logloss: 0.818587\n",
      "[235]\tvalid_0's multi_logloss: 0.818552\n",
      "[236]\tvalid_0's multi_logloss: 0.818431\n",
      "[237]\tvalid_0's multi_logloss: 0.818532\n",
      "[238]\tvalid_0's multi_logloss: 0.818426\n",
      "[239]\tvalid_0's multi_logloss: 0.818505\n",
      "[240]\tvalid_0's multi_logloss: 0.818419\n",
      "[241]\tvalid_0's multi_logloss: 0.81839\n",
      "[242]\tvalid_0's multi_logloss: 0.818433\n",
      "[243]\tvalid_0's multi_logloss: 0.818293\n",
      "[244]\tvalid_0's multi_logloss: 0.818186\n",
      "[245]\tvalid_0's multi_logloss: 0.818149\n",
      "[246]\tvalid_0's multi_logloss: 0.818069\n",
      "[247]\tvalid_0's multi_logloss: 0.81819\n",
      "[248]\tvalid_0's multi_logloss: 0.818154\n",
      "[249]\tvalid_0's multi_logloss: 0.818171\n",
      "[250]\tvalid_0's multi_logloss: 0.818223\n",
      "[251]\tvalid_0's multi_logloss: 0.818296\n",
      "[252]\tvalid_0's multi_logloss: 0.818262\n",
      "[253]\tvalid_0's multi_logloss: 0.818232\n",
      "[254]\tvalid_0's multi_logloss: 0.818197\n",
      "[255]\tvalid_0's multi_logloss: 0.818272\n",
      "[256]\tvalid_0's multi_logloss: 0.818405\n",
      "[257]\tvalid_0's multi_logloss: 0.818496\n",
      "[258]\tvalid_0's multi_logloss: 0.818531\n",
      "[259]\tvalid_0's multi_logloss: 0.818558\n",
      "[260]\tvalid_0's multi_logloss: 0.818767\n",
      "[261]\tvalid_0's multi_logloss: 0.81879\n",
      "[262]\tvalid_0's multi_logloss: 0.818777\n",
      "[263]\tvalid_0's multi_logloss: 0.818814\n",
      "Early stopping, best iteration is:\n",
      "[163]\tvalid_0's multi_logloss: 0.815685\n"
     ]
    }
   ],
   "source": [
    "lgb = LGBMClassifier(n_estimators=1000)\n",
    "evals = [(X_val, Y_val)]\n",
    "lgb.fit(X_train, Y_train, early_stopping_rounds=100, eval_metric=\"logloss\", eval_set=evals, verbose=True)\n",
    "predict = lgb.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "logloss = log_loss(y_val_onehot, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8156847808255132"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "logloss"
   ]
  },
  {
   "source": [
    "## 6) TabNet"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, stratify=train_y, test_size=0.18, random_state=10086)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/17 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "340eef1d072d4db19b4dd8035eb6b259"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "'''\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "features = [col for col in train_x.columns]\n",
    "\n",
    "categorical_columns = []\n",
    "categorical_dims = {}\n",
    "\n",
    "for col in tqdm(train_x.columns):\n",
    "    l_enc = preprocessing.LabelEncoder()\n",
    "    l_enc.fit_transform(train_x[col].values)\n",
    "    categorical_columns.append(col)\n",
    "    categorical_dims[col] = len(l_enc.classes_)\n",
    "\n",
    "cat_idxs = [i for i, f in enumerate(features) if f in categorical_columns]\n",
    "cat_dims = [categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]\n",
    "'''"
   ]
  },
  {
   "source": [
    "### Hyperparameter 정리\n",
    "#### * https://github.com/dreamquark-ai/tabnet 및 https://arxiv.org/pdf/1908.07442.pdf 를 참고함. 더 자세한 내용이나 추가 Hyperparameter는 여기로\n",
    "- 모델 파라미터\n",
    "* <code>n_d</code> = decision prediction layer의 층. 값이 클수록 featrue특징을 더 잘 잡아내지만, 오버피팅될 확률이 높음. 8~64값\n",
    "* <code>n_a</code> = Width of the attention embedding for each mask. 논문에서는 n_d == n_a가 되도록 추천함.\n",
    "* <code>n_steps</code> = decision step. 정보를 담은 feature들이 많다면 높은 값을 주는 것이 좋지만, 너무 높으면 성능을 떨어뜨리고 오버피팅될 확률이 높음. 3~10\n",
    "* <code>gamma</code> = This is the coefficient for feature reusage in the masks(?). 한번 선택된 feature가 다시 사용될지를 결정하는 hyperparameter. 논문에서는 성능에 큰 역할을 하고, n_steps가 높을수록 gamma값도 높이는 것을 추천. 1.0~2.0\n",
    "* <code>momentum</code> = 배치 정규화 모멘텀 값. 0.01~0.4\n",
    "* <code>lambda_sparse</code> = sparsity loss coefficient. 이것도 잘... 적당히 높이면 학습에 도움을 준다고 나와있음\n",
    "* <code>optimizer_fn</code> = 신경망 최적화 함수. Adam을 주로 사용\n",
    "* <code>optimizer_params</code> = optimizer_fn에 넣어줄 parameter. optimizer_fn이 Adam이라면 초기 학습률을 parameter로 받음. 기본값이 dict(lr=2e-2)\n",
    "* <code>scheduler_fn</code> = 학습률 scheduler. 여기서는 단계별로 학습률을 감쇠시키는 StepLR사용\n",
    "* <code>scheduler_params</code> = scheduler_fn에 넣어줄 parameter. 여기서는 감쇠율 gamma와 몇 단계 후 감쇠시킬지 정하는 step_size를 설정\n",
    "* <code>device_name</code> = 'cpu'로 설정하면 cpu, 'cuda'로 설정하면 nvidia gpu사용. gpu 사용하려면 cuda 11.1버전 필요(그 이상 버전은 아직 지원 안하는듯)\n",
    "* <code>mask_type</code> = featrue선택에 사용하는 masking function(?). sparsemax 또는 entmax 둘 중 하나 사용.\n",
    "- Fit 파라미터\n",
    "* <code>max_epochs</code> = 최대 epoch\n",
    "* <code>patience</code> = 지정해준 patience만큼의 epoch동안, 학습의 향상이 이루어지지 않으면, 학습을 중단하고 제일 결과가 잘 나온 것의 가중치로 설정. 0으로 하면 조기 종료 없이 계속 진행\n",
    "* <code>loss_fn</code> = loss function 지정\n",
    "* <code>batch_size</code> = 메모리 크기가 되는 한 크게 잡아주는게 좋다고 하는데 batch 크기도 logloss에 영향을 주니 잘 조절하는것이 좋을듯\n",
    "* <code>virtual_batch_size</code> = ghost batch normalization(?)에 사용된다고 함. 작은 값이 좋고, batch_size값을 나눌 수 있어야 함.\n",
    "* <code>num_workers</code> = GPU연산 시 사용할 cpu코어 개수를 설정한다는데... 오히려 설정하니까 더 느림. CPU보다 GPU가 훨씬 좋으면 어느정도 설정해주는 게 좋아보임\n",
    "* <code>drop_last</code> = 맨 마지막, 남는 배치를 쓸지 안쓸지 결정"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* **주의. 한번 실행에 시간 소요가 너무 김(약 1시간)**\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device used : cuda\n",
      "epoch 0  | loss: 3.81459 | val_0_logloss: 5.64735 |  0:00:01s\n",
      "epoch 1  | loss: 1.28557 | val_0_logloss: 2.12134 |  0:00:03s\n",
      "epoch 2  | loss: 1.03773 | val_0_logloss: 1.38402 |  0:00:05s\n",
      "epoch 3  | loss: 0.98534 | val_0_logloss: 1.11979 |  0:00:07s\n",
      "epoch 4  | loss: 0.93837 | val_0_logloss: 1.02961 |  0:00:09s\n",
      "epoch 5  | loss: 0.9175  | val_0_logloss: 0.98219 |  0:00:11s\n",
      "epoch 6  | loss: 0.90652 | val_0_logloss: 0.99071 |  0:00:13s\n",
      "epoch 7  | loss: 0.89724 | val_0_logloss: 0.99378 |  0:00:15s\n",
      "epoch 8  | loss: 0.89369 | val_0_logloss: 0.98137 |  0:00:17s\n",
      "epoch 9  | loss: 0.89046 | val_0_logloss: 0.96116 |  0:00:19s\n",
      "epoch 10 | loss: 0.88661 | val_0_logloss: 0.94896 |  0:00:21s\n",
      "epoch 11 | loss: 0.8849  | val_0_logloss: 0.93632 |  0:00:23s\n",
      "epoch 12 | loss: 0.88414 | val_0_logloss: 0.92281 |  0:00:25s\n",
      "epoch 13 | loss: 0.8811  | val_0_logloss: 0.91622 |  0:00:27s\n",
      "epoch 14 | loss: 0.87958 | val_0_logloss: 0.91203 |  0:00:29s\n",
      "epoch 15 | loss: 0.88089 | val_0_logloss: 0.9036  |  0:00:30s\n",
      "epoch 16 | loss: 0.87841 | val_0_logloss: 0.89869 |  0:00:32s\n",
      "epoch 17 | loss: 0.8763  | val_0_logloss: 0.89261 |  0:00:34s\n",
      "epoch 18 | loss: 0.87468 | val_0_logloss: 0.88652 |  0:00:35s\n",
      "epoch 19 | loss: 0.87431 | val_0_logloss: 0.8823  |  0:00:37s\n",
      "epoch 20 | loss: 0.87203 | val_0_logloss: 0.88086 |  0:00:38s\n",
      "epoch 21 | loss: 0.87203 | val_0_logloss: 0.87962 |  0:00:40s\n",
      "epoch 22 | loss: 0.86825 | val_0_logloss: 0.87701 |  0:00:41s\n",
      "epoch 23 | loss: 0.86756 | val_0_logloss: 0.8721  |  0:00:43s\n",
      "epoch 24 | loss: 0.86673 | val_0_logloss: 0.87102 |  0:00:44s\n",
      "epoch 25 | loss: 0.86476 | val_0_logloss: 0.87053 |  0:00:46s\n",
      "epoch 26 | loss: 0.86531 | val_0_logloss: 0.86902 |  0:00:47s\n",
      "epoch 27 | loss: 0.86207 | val_0_logloss: 0.8693  |  0:00:49s\n",
      "epoch 28 | loss: 0.86175 | val_0_logloss: 0.86791 |  0:00:51s\n",
      "epoch 29 | loss: 0.86089 | val_0_logloss: 0.86632 |  0:00:52s\n",
      "epoch 30 | loss: 0.86053 | val_0_logloss: 0.8715  |  0:00:54s\n",
      "epoch 31 | loss: 0.85931 | val_0_logloss: 0.86281 |  0:00:56s\n",
      "epoch 32 | loss: 0.85617 | val_0_logloss: 0.86289 |  0:00:57s\n",
      "epoch 33 | loss: 0.85773 | val_0_logloss: 0.86257 |  0:00:59s\n",
      "epoch 34 | loss: 0.85621 | val_0_logloss: 0.86101 |  0:01:01s\n",
      "epoch 35 | loss: 0.85491 | val_0_logloss: 0.86181 |  0:01:02s\n",
      "epoch 36 | loss: 0.85406 | val_0_logloss: 0.85867 |  0:01:05s\n",
      "epoch 37 | loss: 0.85071 | val_0_logloss: 0.86214 |  0:01:07s\n",
      "epoch 38 | loss: 0.85183 | val_0_logloss: 0.8626  |  0:01:09s\n",
      "epoch 39 | loss: 0.85033 | val_0_logloss: 0.86298 |  0:01:12s\n",
      "epoch 40 | loss: 0.85013 | val_0_logloss: 0.86835 |  0:01:14s\n",
      "epoch 41 | loss: 0.85094 | val_0_logloss: 0.86579 |  0:01:16s\n",
      "epoch 42 | loss: 0.85037 | val_0_logloss: 0.8602  |  0:01:18s\n",
      "epoch 43 | loss: 0.84864 | val_0_logloss: 0.8635  |  0:01:21s\n",
      "epoch 44 | loss: 0.84882 | val_0_logloss: 0.86394 |  0:01:23s\n",
      "epoch 45 | loss: 0.84797 | val_0_logloss: 0.85897 |  0:01:25s\n",
      "epoch 46 | loss: 0.8482  | val_0_logloss: 0.86516 |  0:01:27s\n",
      "epoch 47 | loss: 0.84893 | val_0_logloss: 0.86593 |  0:01:28s\n",
      "epoch 48 | loss: 0.84823 | val_0_logloss: 0.8641  |  0:01:30s\n",
      "epoch 49 | loss: 0.84807 | val_0_logloss: 0.8597  |  0:01:31s\n",
      "epoch 50 | loss: 0.84752 | val_0_logloss: 0.86177 |  0:01:33s\n",
      "epoch 51 | loss: 0.84704 | val_0_logloss: 0.86472 |  0:01:35s\n",
      "epoch 52 | loss: 0.84658 | val_0_logloss: 0.85726 |  0:01:36s\n",
      "epoch 53 | loss: 0.84573 | val_0_logloss: 0.85575 |  0:01:38s\n",
      "epoch 54 | loss: 0.84557 | val_0_logloss: 0.85605 |  0:01:40s\n",
      "epoch 55 | loss: 0.845   | val_0_logloss: 0.85234 |  0:01:42s\n",
      "epoch 56 | loss: 0.84355 | val_0_logloss: 0.84988 |  0:01:43s\n",
      "epoch 57 | loss: 0.84549 | val_0_logloss: 0.85491 |  0:01:45s\n",
      "epoch 58 | loss: 0.84499 | val_0_logloss: 0.8545  |  0:01:47s\n",
      "epoch 59 | loss: 0.84451 | val_0_logloss: 0.84793 |  0:01:49s\n",
      "epoch 60 | loss: 0.84364 | val_0_logloss: 0.85127 |  0:01:50s\n",
      "epoch 61 | loss: 0.8438  | val_0_logloss: 0.85133 |  0:01:52s\n",
      "epoch 62 | loss: 0.84289 | val_0_logloss: 0.85059 |  0:01:54s\n",
      "epoch 63 | loss: 0.84394 | val_0_logloss: 0.84632 |  0:01:56s\n",
      "epoch 64 | loss: 0.84319 | val_0_logloss: 0.84528 |  0:01:58s\n",
      "epoch 65 | loss: 0.84291 | val_0_logloss: 0.84585 |  0:02:00s\n",
      "epoch 66 | loss: 0.84164 | val_0_logloss: 0.84646 |  0:02:01s\n",
      "epoch 67 | loss: 0.8427  | val_0_logloss: 0.84645 |  0:02:03s\n",
      "epoch 68 | loss: 0.8407  | val_0_logloss: 0.84804 |  0:02:04s\n",
      "epoch 69 | loss: 0.84338 | val_0_logloss: 0.84693 |  0:02:06s\n",
      "epoch 70 | loss: 0.84207 | val_0_logloss: 0.8436  |  0:02:07s\n",
      "epoch 71 | loss: 0.84353 | val_0_logloss: 0.84663 |  0:02:09s\n",
      "epoch 72 | loss: 0.84179 | val_0_logloss: 0.8465  |  0:02:10s\n",
      "epoch 73 | loss: 0.84136 | val_0_logloss: 0.84516 |  0:02:12s\n",
      "epoch 74 | loss: 0.84098 | val_0_logloss: 0.84442 |  0:02:14s\n",
      "epoch 75 | loss: 0.84187 | val_0_logloss: 0.84676 |  0:02:15s\n",
      "epoch 76 | loss: 0.84009 | val_0_logloss: 0.84347 |  0:02:17s\n",
      "epoch 77 | loss: 0.84123 | val_0_logloss: 0.84499 |  0:02:18s\n",
      "epoch 78 | loss: 0.84034 | val_0_logloss: 0.84424 |  0:02:20s\n",
      "epoch 79 | loss: 0.83956 | val_0_logloss: 0.84537 |  0:02:21s\n",
      "epoch 80 | loss: 0.84032 | val_0_logloss: 0.84811 |  0:02:23s\n",
      "epoch 81 | loss: 0.83898 | val_0_logloss: 0.84316 |  0:02:24s\n",
      "epoch 82 | loss: 0.83793 | val_0_logloss: 0.84097 |  0:02:26s\n",
      "epoch 83 | loss: 0.83827 | val_0_logloss: 0.84467 |  0:02:27s\n",
      "epoch 84 | loss: 0.83739 | val_0_logloss: 0.84347 |  0:02:29s\n",
      "epoch 85 | loss: 0.83823 | val_0_logloss: 0.84195 |  0:02:30s\n",
      "epoch 86 | loss: 0.83872 | val_0_logloss: 0.84272 |  0:02:32s\n",
      "epoch 87 | loss: 0.83793 | val_0_logloss: 0.84495 |  0:02:34s\n",
      "epoch 88 | loss: 0.83777 | val_0_logloss: 0.84223 |  0:02:35s\n",
      "epoch 89 | loss: 0.83759 | val_0_logloss: 0.8415  |  0:02:37s\n",
      "epoch 90 | loss: 0.83682 | val_0_logloss: 0.84203 |  0:02:38s\n",
      "epoch 91 | loss: 0.83555 | val_0_logloss: 0.84181 |  0:02:40s\n",
      "epoch 92 | loss: 0.83576 | val_0_logloss: 0.84281 |  0:02:41s\n",
      "epoch 93 | loss: 0.83738 | val_0_logloss: 0.84066 |  0:02:43s\n",
      "epoch 94 | loss: 0.83581 | val_0_logloss: 0.84035 |  0:02:44s\n",
      "epoch 95 | loss: 0.83539 | val_0_logloss: 0.84392 |  0:02:46s\n",
      "epoch 96 | loss: 0.83555 | val_0_logloss: 0.84247 |  0:02:48s\n",
      "epoch 97 | loss: 0.83583 | val_0_logloss: 0.84265 |  0:02:50s\n",
      "epoch 98 | loss: 0.83475 | val_0_logloss: 0.84055 |  0:02:52s\n",
      "epoch 99 | loss: 0.83394 | val_0_logloss: 0.84037 |  0:02:53s\n",
      "epoch 100| loss: 0.83386 | val_0_logloss: 0.84207 |  0:02:55s\n",
      "epoch 101| loss: 0.83314 | val_0_logloss: 0.84021 |  0:02:57s\n",
      "epoch 102| loss: 0.83435 | val_0_logloss: 0.83913 |  0:02:59s\n",
      "epoch 103| loss: 0.8323  | val_0_logloss: 0.841   |  0:03:01s\n",
      "epoch 104| loss: 0.83328 | val_0_logloss: 0.83857 |  0:03:03s\n",
      "epoch 105| loss: 0.83322 | val_0_logloss: 0.83579 |  0:03:04s\n",
      "epoch 106| loss: 0.83328 | val_0_logloss: 0.83673 |  0:03:06s\n",
      "epoch 107| loss: 0.83359 | val_0_logloss: 0.83949 |  0:03:08s\n",
      "epoch 108| loss: 0.83277 | val_0_logloss: 0.83802 |  0:03:09s\n",
      "epoch 109| loss: 0.83138 | val_0_logloss: 0.83872 |  0:03:11s\n",
      "epoch 110| loss: 0.83121 | val_0_logloss: 0.83847 |  0:03:13s\n",
      "epoch 111| loss: 0.83087 | val_0_logloss: 0.83699 |  0:03:15s\n",
      "epoch 112| loss: 0.83116 | val_0_logloss: 0.83675 |  0:03:16s\n",
      "epoch 113| loss: 0.83045 | val_0_logloss: 0.83684 |  0:03:18s\n",
      "epoch 114| loss: 0.82946 | val_0_logloss: 0.8404  |  0:03:20s\n",
      "epoch 115| loss: 0.8301  | val_0_logloss: 0.84221 |  0:03:21s\n",
      "epoch 116| loss: 0.82973 | val_0_logloss: 0.83781 |  0:03:23s\n",
      "epoch 117| loss: 0.82954 | val_0_logloss: 0.83797 |  0:03:25s\n",
      "epoch 118| loss: 0.83004 | val_0_logloss: 0.83733 |  0:03:26s\n",
      "epoch 119| loss: 0.82959 | val_0_logloss: 0.83732 |  0:03:28s\n",
      "epoch 120| loss: 0.82935 | val_0_logloss: 0.83771 |  0:03:29s\n",
      "epoch 121| loss: 0.82914 | val_0_logloss: 0.83835 |  0:03:31s\n",
      "epoch 122| loss: 0.82888 | val_0_logloss: 0.83918 |  0:03:33s\n",
      "epoch 123| loss: 0.83026 | val_0_logloss: 0.84004 |  0:03:34s\n",
      "epoch 124| loss: 0.82899 | val_0_logloss: 0.84024 |  0:03:36s\n",
      "epoch 125| loss: 0.83004 | val_0_logloss: 0.84063 |  0:03:37s\n",
      "epoch 126| loss: 0.82963 | val_0_logloss: 0.83943 |  0:03:39s\n",
      "epoch 127| loss: 0.82874 | val_0_logloss: 0.83865 |  0:03:41s\n",
      "epoch 128| loss: 0.82802 | val_0_logloss: 0.83726 |  0:03:42s\n",
      "epoch 129| loss: 0.82846 | val_0_logloss: 0.8374  |  0:03:44s\n",
      "epoch 130| loss: 0.82703 | val_0_logloss: 0.83719 |  0:03:46s\n",
      "epoch 131| loss: 0.8265  | val_0_logloss: 0.83718 |  0:03:48s\n",
      "epoch 132| loss: 0.82613 | val_0_logloss: 0.83556 |  0:03:50s\n",
      "epoch 133| loss: 0.82455 | val_0_logloss: 0.83671 |  0:03:52s\n",
      "epoch 134| loss: 0.82545 | val_0_logloss: 0.83788 |  0:03:54s\n",
      "epoch 135| loss: 0.82561 | val_0_logloss: 0.83925 |  0:03:55s\n",
      "epoch 136| loss: 0.8251  | val_0_logloss: 0.8387  |  0:03:57s\n",
      "epoch 137| loss: 0.82443 | val_0_logloss: 0.83843 |  0:03:58s\n",
      "epoch 138| loss: 0.82503 | val_0_logloss: 0.83856 |  0:04:00s\n",
      "epoch 139| loss: 0.82498 | val_0_logloss: 0.83862 |  0:04:02s\n",
      "epoch 140| loss: 0.82553 | val_0_logloss: 0.83766 |  0:04:04s\n",
      "epoch 141| loss: 0.82473 | val_0_logloss: 0.83565 |  0:04:06s\n",
      "epoch 142| loss: 0.82519 | val_0_logloss: 0.83662 |  0:04:08s\n",
      "epoch 143| loss: 0.82545 | val_0_logloss: 0.83692 |  0:04:10s\n",
      "epoch 144| loss: 0.82594 | val_0_logloss: 0.83762 |  0:04:12s\n",
      "epoch 145| loss: 0.82623 | val_0_logloss: 0.83776 |  0:04:14s\n",
      "epoch 146| loss: 0.82554 | val_0_logloss: 0.83765 |  0:04:16s\n",
      "epoch 147| loss: 0.82569 | val_0_logloss: 0.83844 |  0:04:18s\n",
      "epoch 148| loss: 0.82363 | val_0_logloss: 0.83664 |  0:04:20s\n",
      "epoch 149| loss: 0.82345 | val_0_logloss: 0.83501 |  0:04:21s\n",
      "epoch 150| loss: 0.82401 | val_0_logloss: 0.83528 |  0:04:23s\n",
      "epoch 151| loss: 0.82354 | val_0_logloss: 0.83562 |  0:04:25s\n",
      "epoch 152| loss: 0.82248 | val_0_logloss: 0.83565 |  0:04:27s\n",
      "epoch 153| loss: 0.82235 | val_0_logloss: 0.8357  |  0:04:29s\n",
      "epoch 154| loss: 0.82355 | val_0_logloss: 0.83485 |  0:04:31s\n",
      "epoch 155| loss: 0.82443 | val_0_logloss: 0.83545 |  0:04:33s\n",
      "epoch 156| loss: 0.82277 | val_0_logloss: 0.83604 |  0:04:35s\n",
      "epoch 157| loss: 0.82336 | val_0_logloss: 0.83287 |  0:04:37s\n",
      "epoch 158| loss: 0.82111 | val_0_logloss: 0.83263 |  0:04:38s\n",
      "epoch 159| loss: 0.82243 | val_0_logloss: 0.83612 |  0:04:40s\n",
      "epoch 160| loss: 0.82069 | val_0_logloss: 0.83444 |  0:04:42s\n",
      "epoch 161| loss: 0.82119 | val_0_logloss: 0.83339 |  0:04:44s\n",
      "epoch 162| loss: 0.82072 | val_0_logloss: 0.83608 |  0:04:46s\n",
      "epoch 163| loss: 0.8215  | val_0_logloss: 0.83723 |  0:04:48s\n",
      "epoch 164| loss: 0.82073 | val_0_logloss: 0.83649 |  0:04:50s\n",
      "epoch 165| loss: 0.82058 | val_0_logloss: 0.83563 |  0:04:52s\n",
      "epoch 166| loss: 0.82051 | val_0_logloss: 0.8347  |  0:04:54s\n",
      "epoch 167| loss: 0.82075 | val_0_logloss: 0.83393 |  0:04:55s\n",
      "epoch 168| loss: 0.82262 | val_0_logloss: 0.8345  |  0:04:57s\n",
      "epoch 169| loss: 0.82255 | val_0_logloss: 0.83493 |  0:04:59s\n",
      "epoch 170| loss: 0.8232  | val_0_logloss: 0.83406 |  0:05:01s\n",
      "epoch 171| loss: 0.82332 | val_0_logloss: 0.83263 |  0:05:03s\n",
      "epoch 172| loss: 0.82077 | val_0_logloss: 0.83187 |  0:05:04s\n",
      "epoch 173| loss: 0.82172 | val_0_logloss: 0.83062 |  0:05:07s\n",
      "epoch 174| loss: 0.81949 | val_0_logloss: 0.83103 |  0:05:09s\n",
      "epoch 175| loss: 0.81979 | val_0_logloss: 0.83042 |  0:05:11s\n",
      "epoch 176| loss: 0.81846 | val_0_logloss: 0.83069 |  0:05:13s\n",
      "epoch 177| loss: 0.81941 | val_0_logloss: 0.83015 |  0:05:15s\n",
      "epoch 178| loss: 0.81726 | val_0_logloss: 0.83161 |  0:05:16s\n",
      "epoch 179| loss: 0.81798 | val_0_logloss: 0.83156 |  0:05:19s\n",
      "epoch 180| loss: 0.81509 | val_0_logloss: 0.83269 |  0:05:22s\n",
      "epoch 181| loss: 0.81653 | val_0_logloss: 0.83316 |  0:05:24s\n",
      "epoch 182| loss: 0.81459 | val_0_logloss: 0.83333 |  0:05:25s\n",
      "epoch 183| loss: 0.81327 | val_0_logloss: 0.83419 |  0:05:27s\n",
      "epoch 184| loss: 0.81411 | val_0_logloss: 0.83364 |  0:05:29s\n",
      "epoch 185| loss: 0.81468 | val_0_logloss: 0.83559 |  0:05:31s\n",
      "epoch 186| loss: 0.81479 | val_0_logloss: 0.83611 |  0:05:32s\n",
      "epoch 187| loss: 0.81461 | val_0_logloss: 0.83537 |  0:05:34s\n",
      "epoch 188| loss: 0.81425 | val_0_logloss: 0.83308 |  0:05:35s\n",
      "epoch 189| loss: 0.81355 | val_0_logloss: 0.83211 |  0:05:37s\n",
      "epoch 190| loss: 0.8142  | val_0_logloss: 0.83127 |  0:05:39s\n",
      "epoch 191| loss: 0.8142  | val_0_logloss: 0.83209 |  0:05:41s\n",
      "epoch 192| loss: 0.81446 | val_0_logloss: 0.83233 |  0:05:42s\n",
      "epoch 193| loss: 0.81492 | val_0_logloss: 0.8339  |  0:05:44s\n",
      "epoch 194| loss: 0.81285 | val_0_logloss: 0.83596 |  0:05:47s\n",
      "epoch 195| loss: 0.81361 | val_0_logloss: 0.83667 |  0:05:48s\n",
      "epoch 196| loss: 0.81581 | val_0_logloss: 0.83613 |  0:05:50s\n",
      "epoch 197| loss: 0.81748 | val_0_logloss: 0.83373 |  0:05:52s\n",
      "epoch 198| loss: 0.81409 | val_0_logloss: 0.83172 |  0:05:54s\n",
      "epoch 199| loss: 0.81485 | val_0_logloss: 0.83159 |  0:05:56s\n",
      "epoch 200| loss: 0.81399 | val_0_logloss: 0.83113 |  0:05:58s\n",
      "epoch 201| loss: 0.81406 | val_0_logloss: 0.83202 |  0:05:59s\n",
      "epoch 202| loss: 0.81707 | val_0_logloss: 0.8318  |  0:06:01s\n",
      "epoch 203| loss: 0.8129  | val_0_logloss: 0.82945 |  0:06:03s\n",
      "epoch 204| loss: 0.81235 | val_0_logloss: 0.83074 |  0:06:05s\n",
      "epoch 205| loss: 0.81121 | val_0_logloss: 0.83162 |  0:06:06s\n",
      "epoch 206| loss: 0.81173 | val_0_logloss: 0.83267 |  0:06:08s\n",
      "epoch 207| loss: 0.81054 | val_0_logloss: 0.83348 |  0:06:10s\n",
      "epoch 208| loss: 0.81001 | val_0_logloss: 0.83439 |  0:06:11s\n",
      "epoch 209| loss: 0.80958 | val_0_logloss: 0.83479 |  0:06:13s\n",
      "epoch 210| loss: 0.80963 | val_0_logloss: 0.83406 |  0:06:15s\n",
      "epoch 211| loss: 0.80909 | val_0_logloss: 0.8337  |  0:06:17s\n",
      "epoch 212| loss: 0.80825 | val_0_logloss: 0.83286 |  0:06:19s\n",
      "epoch 213| loss: 0.80788 | val_0_logloss: 0.83183 |  0:06:21s\n",
      "epoch 214| loss: 0.80786 | val_0_logloss: 0.8316  |  0:06:23s\n",
      "epoch 215| loss: 0.80874 | val_0_logloss: 0.83376 |  0:06:25s\n",
      "epoch 216| loss: 0.8099  | val_0_logloss: 0.83573 |  0:06:27s\n",
      "epoch 217| loss: 0.81005 | val_0_logloss: 0.83578 |  0:06:29s\n",
      "epoch 218| loss: 0.80854 | val_0_logloss: 0.83421 |  0:06:31s\n",
      "epoch 219| loss: 0.80972 | val_0_logloss: 0.83469 |  0:06:33s\n",
      "epoch 220| loss: 0.80941 | val_0_logloss: 0.83307 |  0:06:35s\n",
      "epoch 221| loss: 0.80843 | val_0_logloss: 0.83118 |  0:06:36s\n",
      "epoch 222| loss: 0.80686 | val_0_logloss: 0.82963 |  0:06:39s\n",
      "epoch 223| loss: 0.80715 | val_0_logloss: 0.8293  |  0:06:41s\n",
      "epoch 224| loss: 0.80599 | val_0_logloss: 0.82955 |  0:06:43s\n",
      "epoch 225| loss: 0.80614 | val_0_logloss: 0.83039 |  0:06:45s\n",
      "epoch 226| loss: 0.80351 | val_0_logloss: 0.83003 |  0:06:47s\n",
      "epoch 227| loss: 0.80587 | val_0_logloss: 0.83031 |  0:06:48s\n",
      "epoch 228| loss: 0.80555 | val_0_logloss: 0.83062 |  0:06:50s\n",
      "epoch 229| loss: 0.80328 | val_0_logloss: 0.83121 |  0:06:51s\n",
      "epoch 230| loss: 0.80322 | val_0_logloss: 0.8316  |  0:06:54s\n",
      "epoch 231| loss: 0.80262 | val_0_logloss: 0.83273 |  0:06:56s\n",
      "epoch 232| loss: 0.80273 | val_0_logloss: 0.83086 |  0:06:58s\n",
      "epoch 233| loss: 0.8025  | val_0_logloss: 0.83133 |  0:07:00s\n",
      "epoch 234| loss: 0.80358 | val_0_logloss: 0.83168 |  0:07:02s\n",
      "epoch 235| loss: 0.80011 | val_0_logloss: 0.83263 |  0:07:03s\n",
      "epoch 236| loss: 0.80229 | val_0_logloss: 0.83411 |  0:07:05s\n",
      "epoch 237| loss: 0.79933 | val_0_logloss: 0.83495 |  0:07:07s\n",
      "epoch 238| loss: 0.79899 | val_0_logloss: 0.83564 |  0:07:09s\n",
      "epoch 239| loss: 0.79793 | val_0_logloss: 0.83461 |  0:07:10s\n",
      "epoch 240| loss: 0.79778 | val_0_logloss: 0.8342  |  0:07:12s\n",
      "epoch 241| loss: 0.79797 | val_0_logloss: 0.83406 |  0:07:14s\n",
      "epoch 242| loss: 0.7968  | val_0_logloss: 0.83445 |  0:07:15s\n",
      "epoch 243| loss: 0.79579 | val_0_logloss: 0.83423 |  0:07:17s\n",
      "epoch 244| loss: 0.79426 | val_0_logloss: 0.83294 |  0:07:18s\n",
      "epoch 245| loss: 0.79441 | val_0_logloss: 0.8302  |  0:07:20s\n",
      "epoch 246| loss: 0.7938  | val_0_logloss: 0.83049 |  0:07:22s\n",
      "epoch 247| loss: 0.79489 | val_0_logloss: 0.83022 |  0:07:23s\n",
      "epoch 248| loss: 0.79418 | val_0_logloss: 0.83135 |  0:07:25s\n",
      "epoch 249| loss: 0.7933  | val_0_logloss: 0.83059 |  0:07:27s\n",
      "epoch 250| loss: 0.79331 | val_0_logloss: 0.83206 |  0:07:28s\n",
      "epoch 251| loss: 0.79151 | val_0_logloss: 0.83325 |  0:07:30s\n",
      "epoch 252| loss: 0.79164 | val_0_logloss: 0.83431 |  0:07:33s\n",
      "epoch 253| loss: 0.79137 | val_0_logloss: 0.83422 |  0:07:35s\n",
      "epoch 254| loss: 0.79156 | val_0_logloss: 0.8329  |  0:07:37s\n",
      "epoch 255| loss: 0.79198 | val_0_logloss: 0.83231 |  0:07:39s\n",
      "epoch 256| loss: 0.78928 | val_0_logloss: 0.83175 |  0:07:41s\n",
      "epoch 257| loss: 0.79067 | val_0_logloss: 0.83366 |  0:07:43s\n",
      "epoch 258| loss: 0.79048 | val_0_logloss: 0.83617 |  0:07:45s\n",
      "epoch 259| loss: 0.79087 | val_0_logloss: 0.83634 |  0:07:47s\n",
      "epoch 260| loss: 0.79208 | val_0_logloss: 0.83487 |  0:07:48s\n",
      "epoch 261| loss: 0.7894  | val_0_logloss: 0.83249 |  0:07:50s\n",
      "epoch 262| loss: 0.78994 | val_0_logloss: 0.83371 |  0:07:52s\n",
      "epoch 263| loss: 0.79015 | val_0_logloss: 0.83044 |  0:07:54s\n",
      "epoch 264| loss: 0.78844 | val_0_logloss: 0.82991 |  0:07:56s\n",
      "epoch 265| loss: 0.78778 | val_0_logloss: 0.83058 |  0:07:58s\n",
      "epoch 266| loss: 0.78808 | val_0_logloss: 0.83272 |  0:08:00s\n",
      "epoch 267| loss: 0.78797 | val_0_logloss: 0.83351 |  0:08:02s\n",
      "epoch 268| loss: 0.78568 | val_0_logloss: 0.83464 |  0:08:04s\n",
      "epoch 269| loss: 0.78489 | val_0_logloss: 0.83314 |  0:08:05s\n",
      "epoch 270| loss: 0.78497 | val_0_logloss: 0.83079 |  0:08:08s\n",
      "epoch 271| loss: 0.78441 | val_0_logloss: 0.83068 |  0:08:11s\n",
      "epoch 272| loss: 0.78233 | val_0_logloss: 0.83185 |  0:08:14s\n",
      "epoch 273| loss: 0.7867  | val_0_logloss: 0.83034 |  0:08:17s\n",
      "epoch 274| loss: 0.78569 | val_0_logloss: 0.83229 |  0:08:20s\n",
      "epoch 275| loss: 0.78385 | val_0_logloss: 0.8319  |  0:08:22s\n",
      "epoch 276| loss: 0.78353 | val_0_logloss: 0.8313  |  0:08:25s\n",
      "epoch 277| loss: 0.78464 | val_0_logloss: 0.83314 |  0:08:28s\n",
      "epoch 278| loss: 0.78309 | val_0_logloss: 0.83316 |  0:08:30s\n",
      "epoch 279| loss: 0.78236 | val_0_logloss: 0.83085 |  0:08:32s\n",
      "epoch 280| loss: 0.78442 | val_0_logloss: 0.83119 |  0:08:33s\n",
      "epoch 281| loss: 0.78083 | val_0_logloss: 0.83079 |  0:08:35s\n",
      "epoch 282| loss: 0.7822  | val_0_logloss: 0.8308  |  0:08:37s\n",
      "epoch 283| loss: 0.78339 | val_0_logloss: 0.83052 |  0:08:39s\n",
      "\n",
      "Early stopping occurred at epoch 283 with best_epoch = 223 and best_val_0_logloss = 0.8293\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.multitask import TabNetMultiTaskClassifier\n",
    "\n",
    "clf = TabNetMultiTaskClassifier(\n",
    "    n_d=16, n_a=16, n_steps=5,\n",
    "    lambda_sparse=1e-2,\n",
    "    gamma = 1.3,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    scheduler_params = {\"gamma\": 0.95, \"step_size\": 20},\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    mask_type='entmax',\n",
    "    device_name='cuda'\n",
    ")\n",
    "clf.fit(\n",
    "    X_train = X_train.values, y_train = np.array(Y_train).reshape(Y_train.shape[0],1),\n",
    "    eval_set = [(X_val.values, np.array(Y_val).reshape(Y_val.shape[0],1))],\n",
    "    loss_fn = \n",
    "    max_epochs=300,\n",
    "    patience=60,\n",
    "    batch_size=8192,\n",
    "    virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x275398d7460>]"
      ]
     },
     "metadata": {},
     "execution_count": 51
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 378.465625 248.518125\" width=\"378.465625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-05-11T01:13:41.756956</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.4.1, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 378.465625 248.518125 \r\nL 378.465625 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 371.265625 224.64 \r\nL 371.265625 7.2 \r\nL 36.465625 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m6e8954b22a\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.683807\" xlink:href=\"#m6e8954b22a\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(48.502557 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 4250 \r\nQ 1547 4250 1301 3770 \r\nQ 1056 3291 1056 2328 \r\nQ 1056 1369 1301 889 \r\nQ 1547 409 2034 409 \r\nQ 2525 409 2770 889 \r\nQ 3016 1369 3016 2328 \r\nQ 3016 3291 2770 3770 \r\nQ 2525 4250 2034 4250 \r\nz\r\nM 2034 4750 \r\nQ 2819 4750 3233 4129 \r\nQ 3647 3509 3647 2328 \r\nQ 3647 1150 3233 529 \r\nQ 2819 -91 2034 -91 \r\nQ 1250 -91 836 529 \r\nQ 422 1150 422 2328 \r\nQ 422 3509 836 4129 \r\nQ 1250 4750 2034 4750 \r\nz\r\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"106.425468\" xlink:href=\"#m6e8954b22a\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 50 -->\r\n      <g transform=\"translate(100.062968 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 691 4666 \r\nL 3169 4666 \r\nL 3169 4134 \r\nL 1269 4134 \r\nL 1269 2991 \r\nQ 1406 3038 1543 3061 \r\nQ 1681 3084 1819 3084 \r\nQ 2600 3084 3056 2656 \r\nQ 3513 2228 3513 1497 \r\nQ 3513 744 3044 326 \r\nQ 2575 -91 1722 -91 \r\nQ 1428 -91 1123 -41 \r\nQ 819 9 494 109 \r\nL 494 744 \r\nQ 775 591 1075 516 \r\nQ 1375 441 1709 441 \r\nQ 2250 441 2565 725 \r\nQ 2881 1009 2881 1497 \r\nQ 2881 1984 2565 2268 \r\nQ 2250 2553 1709 2553 \r\nQ 1456 2553 1204 2497 \r\nQ 953 2441 691 2322 \r\nL 691 4666 \r\nz\r\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"161.167129\" xlink:href=\"#m6e8954b22a\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 100 -->\r\n      <g transform=\"translate(151.623379 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 794 531 \r\nL 1825 531 \r\nL 1825 4091 \r\nL 703 3866 \r\nL 703 4441 \r\nL 1819 4666 \r\nL 2450 4666 \r\nL 2450 531 \r\nL 3481 531 \r\nL 3481 0 \r\nL 794 0 \r\nL 794 531 \r\nz\r\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"215.90879\" xlink:href=\"#m6e8954b22a\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 150 -->\r\n      <g transform=\"translate(206.36504 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"270.650452\" xlink:href=\"#m6e8954b22a\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 200 -->\r\n      <g transform=\"translate(261.106702 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 1228 531 \r\nL 3431 531 \r\nL 3431 0 \r\nL 469 0 \r\nL 469 531 \r\nQ 828 903 1448 1529 \r\nQ 2069 2156 2228 2338 \r\nQ 2531 2678 2651 2914 \r\nQ 2772 3150 2772 3378 \r\nQ 2772 3750 2511 3984 \r\nQ 2250 4219 1831 4219 \r\nQ 1534 4219 1204 4116 \r\nQ 875 4013 500 3803 \r\nL 500 4441 \r\nQ 881 4594 1212 4672 \r\nQ 1544 4750 1819 4750 \r\nQ 2544 4750 2975 4387 \r\nQ 3406 4025 3406 3419 \r\nQ 3406 3131 3298 2873 \r\nQ 3191 2616 2906 2266 \r\nQ 2828 2175 2409 1742 \r\nQ 1991 1309 1228 531 \r\nz\r\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"325.392113\" xlink:href=\"#m6e8954b22a\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 250 -->\r\n      <g transform=\"translate(315.848363 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m22428ae0b9\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m22428ae0b9\" y=\"196.958514\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0.80 -->\r\n      <g transform=\"translate(7.2 200.757733)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 684 794 \r\nL 1344 794 \r\nL 1344 0 \r\nL 684 0 \r\nL 684 794 \r\nz\r\n\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\r\n        <path d=\"M 2034 2216 \r\nQ 1584 2216 1326 1975 \r\nQ 1069 1734 1069 1313 \r\nQ 1069 891 1326 650 \r\nQ 1584 409 2034 409 \r\nQ 2484 409 2743 651 \r\nQ 3003 894 3003 1313 \r\nQ 3003 1734 2745 1975 \r\nQ 2488 2216 2034 2216 \r\nz\r\nM 1403 2484 \r\nQ 997 2584 770 2862 \r\nQ 544 3141 544 3541 \r\nQ 544 4100 942 4425 \r\nQ 1341 4750 2034 4750 \r\nQ 2731 4750 3128 4425 \r\nQ 3525 4100 3525 3541 \r\nQ 3525 3141 3298 2862 \r\nQ 3072 2584 2669 2484 \r\nQ 3125 2378 3379 2068 \r\nQ 3634 1759 3634 1313 \r\nQ 3634 634 3220 271 \r\nQ 2806 -91 2034 -91 \r\nQ 1263 -91 848 271 \r\nQ 434 634 434 1313 \r\nQ 434 1759 690 2068 \r\nQ 947 2378 1403 2484 \r\nz\r\nM 1172 3481 \r\nQ 1172 3119 1398 2916 \r\nQ 1625 2713 2034 2713 \r\nQ 2441 2713 2670 2916 \r\nQ 2900 3119 2900 3481 \r\nQ 2900 3844 2670 4047 \r\nQ 2441 4250 2034 4250 \r\nQ 1625 4250 1398 4047 \r\nQ 1172 3844 1172 3481 \r\nz\r\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m22428ae0b9\" y=\"150.545599\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.85 -->\r\n      <g transform=\"translate(7.2 154.344818)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m22428ae0b9\" y=\"104.132684\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.90 -->\r\n      <g transform=\"translate(7.2 107.931903)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 703 97 \r\nL 703 672 \r\nQ 941 559 1184 500 \r\nQ 1428 441 1663 441 \r\nQ 2288 441 2617 861 \r\nQ 2947 1281 2994 2138 \r\nQ 2813 1869 2534 1725 \r\nQ 2256 1581 1919 1581 \r\nQ 1219 1581 811 2004 \r\nQ 403 2428 403 3163 \r\nQ 403 3881 828 4315 \r\nQ 1253 4750 1959 4750 \r\nQ 2769 4750 3195 4129 \r\nQ 3622 3509 3622 2328 \r\nQ 3622 1225 3098 567 \r\nQ 2575 -91 1691 -91 \r\nQ 1453 -91 1209 -44 \r\nQ 966 3 703 97 \r\nz\r\nM 1959 2075 \r\nQ 2384 2075 2632 2365 \r\nQ 2881 2656 2881 3163 \r\nQ 2881 3666 2632 3958 \r\nQ 2384 4250 1959 4250 \r\nQ 1534 4250 1286 3958 \r\nQ 1038 3666 1038 3163 \r\nQ 1038 2656 1286 2365 \r\nQ 1534 2075 1959 2075 \r\nz\r\n\" id=\"DejaVuSans-39\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-39\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m22428ae0b9\" y=\"57.71977\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.95 -->\r\n      <g transform=\"translate(7.2 61.518988)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-39\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m22428ae0b9\" y=\"11.306855\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 1.00 -->\r\n      <g transform=\"translate(7.2 15.106074)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_12\">\r\n    <path clip-path=\"url(#paca74df3fa)\" d=\"M 51.683807 87.888312 \r\nL 52.77864 98.08216 \r\nL 53.873473 106.691575 \r\nL 57.157973 116.559969 \r\nL 58.252806 118.151891 \r\nL 59.347639 118.851791 \r\nL 60.442473 121.676411 \r\nL 61.537306 123.085144 \r\nL 62.632139 121.867924 \r\nL 63.726972 124.171822 \r\nL 64.821806 126.128344 \r\nL 65.916639 127.634106 \r\nL 67.011472 127.981881 \r\nL 68.106305 130.096808 \r\nL 69.201138 130.098405 \r\nL 70.295972 133.606555 \r\nL 72.485638 135.019338 \r\nL 73.580471 136.845268 \r\nL 74.675305 136.330347 \r\nL 75.770138 139.342518 \r\nL 76.864971 139.634268 \r\nL 77.959804 140.440129 \r\nL 79.054637 140.773241 \r\nL 80.149471 141.901286 \r\nL 81.244304 144.81769 \r\nL 82.339137 143.365592 \r\nL 83.43397 144.784432 \r\nL 84.528804 145.986261 \r\nL 85.623637 146.774718 \r\nL 86.71847 149.886625 \r\nL 87.813303 148.843271 \r\nL 88.908136 150.242682 \r\nL 90.00297 150.421028 \r\nL 91.097803 149.673851 \r\nL 92.192636 150.20659 \r\nL 93.287469 151.809869 \r\nL 94.382303 151.645357 \r\nL 95.477136 152.42926 \r\nL 96.571969 152.21668 \r\nL 97.666802 151.537409 \r\nL 98.761635 152.18913 \r\nL 99.856469 152.33632 \r\nL 102.046135 153.291437 \r\nL 103.140968 153.718057 \r\nL 104.235802 154.509541 \r\nL 105.330635 154.657943 \r\nL 106.425468 155.190399 \r\nL 107.520301 156.532439 \r\nL 108.615134 154.728755 \r\nL 110.804801 155.642619 \r\nL 111.899634 156.449474 \r\nL 112.994467 156.303642 \r\nL 114.089301 157.147751 \r\nL 115.184134 156.167533 \r\nL 116.278967 156.863478 \r\nL 117.3738 157.130905 \r\nL 118.468634 158.304263 \r\nL 119.563467 157.324176 \r\nL 120.6583 159.174253 \r\nL 121.753133 156.688976 \r\nL 122.847966 157.906212 \r\nL 123.9428 156.554299 \r\nL 125.037633 158.165719 \r\nL 127.227299 158.915862 \r\nL 128.322133 158.092216 \r\nL 129.416966 159.742124 \r\nL 130.511799 158.686731 \r\nL 132.701465 160.236369 \r\nL 133.796299 159.529839 \r\nL 134.891132 160.774514 \r\nL 135.985965 161.749499 \r\nL 137.080798 161.429994 \r\nL 138.175632 162.249767 \r\nL 139.270465 161.469046 \r\nL 140.365298 161.016003 \r\nL 141.460131 161.752817 \r\nL 143.649798 162.060912 \r\nL 144.744631 162.783192 \r\nL 145.839464 163.963253 \r\nL 146.934297 163.767316 \r\nL 148.029131 162.255908 \r\nL 149.123964 163.713591 \r\nL 150.218797 164.107884 \r\nL 151.31363 163.958372 \r\nL 152.408463 163.696451 \r\nL 153.503297 164.701836 \r\nL 154.59813 165.457141 \r\nL 155.692963 165.524966 \r\nL 156.787796 166.198795 \r\nL 157.88263 165.074711 \r\nL 158.977463 166.980229 \r\nL 160.072296 166.066455 \r\nL 161.167129 166.125007 \r\nL 162.261962 166.065931 \r\nL 163.356796 165.775405 \r\nL 164.451629 166.541704 \r\nL 165.546462 167.826194 \r\nL 166.641295 167.987587 \r\nL 167.736129 168.304095 \r\nL 168.830962 168.03124 \r\nL 169.925795 168.697047 \r\nL 171.020628 169.60977 \r\nL 172.115461 169.018652 \r\nL 173.210295 169.361881 \r\nL 174.305128 169.536021 \r\nL 175.399961 169.076605 \r\nL 176.494794 169.491261 \r\nL 179.779294 170.151303 \r\nL 180.874127 168.869122 \r\nL 181.968961 170.048387 \r\nL 183.063794 169.070573 \r\nL 184.158627 169.456533 \r\nL 185.25346 170.276036 \r\nL 186.348293 170.952428 \r\nL 187.443127 170.543345 \r\nL 188.53796 171.871558 \r\nL 189.632793 172.361501 \r\nL 190.727626 172.700626 \r\nL 191.82246 174.168211 \r\nL 192.917293 173.332302 \r\nL 194.012126 173.183943 \r\nL 195.106959 173.657258 \r\nL 196.201792 174.281816 \r\nL 197.296626 173.727275 \r\nL 198.391459 173.774027 \r\nL 199.486292 173.256302 \r\nL 200.581125 174.007046 \r\nL 201.675959 173.573347 \r\nL 202.770792 173.330083 \r\nL 203.865625 172.8774 \r\nL 204.960458 172.609516 \r\nL 206.055291 173.248216 \r\nL 207.150125 173.109417 \r\nL 208.244958 175.023662 \r\nL 209.339791 175.193613 \r\nL 210.434624 174.670001 \r\nL 211.529458 175.109341 \r\nL 212.624291 176.094711 \r\nL 213.719124 176.20763 \r\nL 214.813957 175.099296 \r\nL 215.90879 174.282493 \r\nL 217.003624 175.820368 \r\nL 218.098457 175.273241 \r\nL 219.19329 177.367521 \r\nL 220.288123 176.141948 \r\nL 221.382957 177.753365 \r\nL 222.47779 177.291247 \r\nL 223.572623 177.723324 \r\nL 224.667456 177.002499 \r\nL 225.762289 177.713822 \r\nL 227.951956 177.920558 \r\nL 229.046789 177.698711 \r\nL 230.141622 175.964137 \r\nL 231.236456 176.023786 \r\nL 232.331289 175.425189 \r\nL 233.426122 175.308945 \r\nL 234.520955 177.677791 \r\nL 235.615789 176.800053 \r\nL 236.710622 178.867945 \r\nL 237.805455 178.591695 \r\nL 238.900288 179.820303 \r\nL 239.995121 178.942132 \r\nL 241.089955 180.94064 \r\nL 242.184788 180.270117 \r\nL 243.279621 182.952095 \r\nL 244.374454 181.613652 \r\nL 245.469288 183.412286 \r\nL 246.564121 184.644599 \r\nL 247.658954 183.863122 \r\nL 248.753787 183.335848 \r\nL 249.84862 183.231657 \r\nL 250.943454 183.39877 \r\nL 252.038287 183.731172 \r\nL 253.13312 184.385035 \r\nL 254.227953 183.779859 \r\nL 255.322787 183.78028 \r\nL 256.41762 183.535486 \r\nL 257.512453 183.108064 \r\nL 258.607286 185.026716 \r\nL 259.702119 184.325155 \r\nL 260.796953 182.285648 \r\nL 261.891786 180.733537 \r\nL 262.986619 183.879666 \r\nL 264.081452 183.178006 \r\nL 265.176286 183.974556 \r\nL 266.271119 183.90878 \r\nL 267.365952 181.115021 \r\nL 268.460785 184.988162 \r\nL 269.555618 185.496421 \r\nL 270.650452 186.550892 \r\nL 271.745285 186.071252 \r\nL 272.840118 187.178174 \r\nL 275.029785 188.065355 \r\nL 276.124618 188.021097 \r\nL 277.219451 188.522361 \r\nL 278.314284 189.29641 \r\nL 279.409117 189.640414 \r\nL 280.503951 189.662665 \r\nL 281.598784 188.850082 \r\nL 282.693617 187.766467 \r\nL 283.78845 187.628204 \r\nL 284.883284 189.034958 \r\nL 285.978117 187.936509 \r\nL 287.07295 188.224966 \r\nL 288.167783 189.131331 \r\nL 289.262616 190.591292 \r\nL 290.35745 190.325326 \r\nL 291.452283 191.402347 \r\nL 292.547116 191.261458 \r\nL 293.641949 193.698908 \r\nL 294.736783 191.512039 \r\nL 295.831616 191.802061 \r\nL 296.926449 193.912826 \r\nL 298.021282 193.973577 \r\nL 299.116116 194.522948 \r\nL 300.210949 194.421447 \r\nL 301.305782 194.640981 \r\nL 302.400615 193.633724 \r\nL 303.495448 196.852434 \r\nL 304.590282 194.829278 \r\nL 305.685115 197.583472 \r\nL 306.779948 197.898642 \r\nL 307.874781 198.880873 \r\nL 308.969615 199.015201 \r\nL 310.064448 198.84388 \r\nL 312.254114 200.864485 \r\nL 313.348947 202.28439 \r\nL 314.443781 202.147009 \r\nL 315.538614 202.709372 \r\nL 316.633447 201.703829 \r\nL 317.72828 202.364678 \r\nL 318.823114 203.17533 \r\nL 319.917947 203.166223 \r\nL 321.01278 204.83691 \r\nL 322.107613 204.722777 \r\nL 323.202446 204.973082 \r\nL 324.29728 204.793031 \r\nL 325.392113 204.400702 \r\nL 326.486946 206.909648 \r\nL 327.581779 205.621426 \r\nL 328.676613 205.796352 \r\nL 329.771446 205.437397 \r\nL 330.866279 204.308484 \r\nL 331.961112 206.79621 \r\nL 333.055945 206.292386 \r\nL 334.150779 206.097254 \r\nL 335.245612 207.691559 \r\nL 336.340445 208.302673 \r\nL 337.435278 208.026559 \r\nL 338.530112 208.127135 \r\nL 339.624945 210.247944 \r\nL 340.719778 210.984491 \r\nL 341.814611 210.914231 \r\nL 342.909444 211.42559 \r\nL 344.004278 213.359638 \r\nL 345.099111 209.302874 \r\nL 346.193944 210.240487 \r\nL 347.288777 211.948658 \r\nL 348.383611 212.249334 \r\nL 349.478444 211.217316 \r\nL 350.573277 212.652475 \r\nL 351.66811 213.332056 \r\nL 352.762944 211.4185 \r\nL 353.857777 214.756364 \r\nL 356.047443 212.372477 \r\nL 356.047443 212.372477 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_13\">\r\n    <path clip-path=\"url(#paca74df3fa)\" d=\"M 51.683807 27.841103 \r\nL 52.77864 19.925871 \r\nL 53.873473 17.083636 \r\nL 54.968306 28.595824 \r\nL 56.06314 47.360313 \r\nL 58.252806 70.414868 \r\nL 59.347639 82.95539 \r\nL 60.442473 89.077773 \r\nL 61.537306 92.964938 \r\nL 62.632139 100.791577 \r\nL 63.726972 105.348417 \r\nL 65.916639 116.64266 \r\nL 67.011472 120.565731 \r\nL 68.106305 121.902583 \r\nL 69.201138 123.049513 \r\nL 70.295972 125.477027 \r\nL 71.390805 130.027129 \r\nL 72.485638 131.03447 \r\nL 73.580471 131.49295 \r\nL 74.675305 132.89139 \r\nL 75.770138 132.633366 \r\nL 76.864971 133.92327 \r\nL 77.959804 135.396733 \r\nL 79.054637 130.592049 \r\nL 80.149471 138.65087 \r\nL 81.244304 138.57677 \r\nL 82.339137 138.874857 \r\nL 83.43397 140.327647 \r\nL 84.528804 139.579332 \r\nL 85.623637 142.499114 \r\nL 86.71847 139.272491 \r\nL 88.908136 138.498579 \r\nL 90.00297 133.508366 \r\nL 91.097803 135.889637 \r\nL 92.192636 141.074885 \r\nL 93.287469 138.017966 \r\nL 94.382303 137.604263 \r\nL 95.477136 142.220555 \r\nL 96.571969 136.470882 \r\nL 97.666802 135.757939 \r\nL 98.761635 137.458087 \r\nL 99.856469 141.545281 \r\nL 100.951302 139.619884 \r\nL 102.046135 136.881059 \r\nL 103.140968 143.803477 \r\nL 104.235802 145.209729 \r\nL 105.330635 144.932199 \r\nL 106.425468 148.373642 \r\nL 107.520301 150.659068 \r\nL 108.615134 145.987765 \r\nL 109.709968 146.367993 \r\nL 110.804801 152.468862 \r\nL 111.899634 149.368354 \r\nL 112.994467 149.3112 \r\nL 114.089301 150.001753 \r\nL 115.184134 153.961531 \r\nL 116.278967 154.93065 \r\nL 118.468634 153.831145 \r\nL 119.563467 153.840925 \r\nL 120.6583 152.360505 \r\nL 121.753133 153.395401 \r\nL 122.847966 156.484509 \r\nL 123.9428 153.669787 \r\nL 125.037633 153.793433 \r\nL 126.132466 155.03826 \r\nL 127.227299 155.722543 \r\nL 128.322133 153.551393 \r\nL 129.416966 156.60325 \r\nL 130.511799 155.194606 \r\nL 131.606632 155.893059 \r\nL 132.701465 154.847011 \r\nL 133.796299 152.297164 \r\nL 134.891132 156.892458 \r\nL 135.985965 158.928722 \r\nL 137.080798 155.496314 \r\nL 138.175632 156.603219 \r\nL 139.270465 158.020704 \r\nL 140.365298 157.301503 \r\nL 141.460131 155.232771 \r\nL 142.554964 157.756246 \r\nL 143.649798 158.43456 \r\nL 144.744631 157.945829 \r\nL 145.839464 158.149526 \r\nL 146.934297 157.216795 \r\nL 148.029131 159.21134 \r\nL 149.123964 159.499997 \r\nL 150.218797 156.191973 \r\nL 151.31363 157.537323 \r\nL 152.408463 157.367274 \r\nL 153.503297 159.316997 \r\nL 154.59813 159.484967 \r\nL 155.692963 157.908579 \r\nL 156.787796 159.633208 \r\nL 157.88263 160.634116 \r\nL 158.977463 158.898535 \r\nL 160.072296 161.158842 \r\nL 161.167129 163.739033 \r\nL 162.261962 162.864419 \r\nL 163.356796 160.300999 \r\nL 164.451629 161.668756 \r\nL 165.546462 161.020405 \r\nL 166.641295 161.252391 \r\nL 167.736129 162.620433 \r\nL 168.830962 162.84914 \r\nL 169.925795 162.760669 \r\nL 171.020628 159.460988 \r\nL 172.115461 157.778868 \r\nL 173.210295 161.856946 \r\nL 174.305128 161.710711 \r\nL 175.399961 162.304287 \r\nL 176.494794 162.311993 \r\nL 177.589628 161.955948 \r\nL 178.684461 161.363981 \r\nL 180.874127 159.788284 \r\nL 181.968961 159.602862 \r\nL 183.063794 159.244747 \r\nL 184.158627 160.360964 \r\nL 185.25346 161.081435 \r\nL 186.348293 162.376133 \r\nL 187.443127 162.246082 \r\nL 188.53796 162.437463 \r\nL 189.632793 162.441267 \r\nL 190.727626 163.947826 \r\nL 192.917293 161.794793 \r\nL 194.012126 160.528862 \r\nL 195.106959 161.033928 \r\nL 196.201792 161.281951 \r\nL 198.391459 161.111474 \r\nL 199.486292 161.995965 \r\nL 200.581125 163.869592 \r\nL 201.675959 162.969308 \r\nL 202.770792 162.691435 \r\nL 203.865625 162.038288 \r\nL 204.960458 161.907368 \r\nL 206.055291 162.014152 \r\nL 207.150125 161.273734 \r\nL 209.339791 164.456582 \r\nL 211.529458 163.897883 \r\nL 213.719124 163.820668 \r\nL 214.813957 164.612857 \r\nL 217.003624 163.508028 \r\nL 218.098457 166.450807 \r\nL 219.19329 166.6658 \r\nL 220.288123 163.428123 \r\nL 221.382957 164.989708 \r\nL 222.47779 165.966601 \r\nL 223.572623 163.468017 \r\nL 224.667456 162.396808 \r\nL 226.857123 163.885845 \r\nL 227.951956 164.746171 \r\nL 229.046789 165.465139 \r\nL 230.141622 164.930781 \r\nL 231.236456 164.529934 \r\nL 232.331289 165.343988 \r\nL 233.426122 166.669272 \r\nL 234.520955 167.378242 \r\nL 235.615789 168.5334 \r\nL 236.710622 168.158993 \r\nL 237.805455 168.718559 \r\nL 238.900288 168.473565 \r\nL 239.995121 168.970428 \r\nL 241.089955 167.613066 \r\nL 242.184788 167.664069 \r\nL 243.279621 166.617214 \r\nL 244.374454 166.172837 \r\nL 245.469288 166.021224 \r\nL 246.564121 165.218956 \r\nL 247.658954 165.731077 \r\nL 248.753787 163.924467 \r\nL 249.84862 163.43618 \r\nL 250.943454 164.126242 \r\nL 252.038287 166.250268 \r\nL 254.227953 167.934649 \r\nL 255.322787 167.173807 \r\nL 256.41762 166.948649 \r\nL 257.512453 165.490592 \r\nL 258.607286 163.577323 \r\nL 259.702119 162.918396 \r\nL 260.796953 163.41807 \r\nL 261.891786 165.646421 \r\nL 262.986619 167.511884 \r\nL 264.081452 167.63757 \r\nL 265.176286 168.062208 \r\nL 266.271119 167.237295 \r\nL 267.365952 167.443681 \r\nL 268.460785 169.621854 \r\nL 269.555618 168.425143 \r\nL 270.650452 167.607218 \r\nL 271.745285 166.630912 \r\nL 273.934951 165.03629 \r\nL 275.029785 164.660273 \r\nL 276.124618 165.340719 \r\nL 277.219451 165.679069 \r\nL 278.314284 166.460435 \r\nL 279.409117 167.416469 \r\nL 280.503951 167.627884 \r\nL 282.693617 163.794424 \r\nL 283.78845 163.742888 \r\nL 284.883284 165.201637 \r\nL 285.978117 164.759303 \r\nL 287.07295 166.262244 \r\nL 288.167783 168.015287 \r\nL 289.262616 169.458742 \r\nL 290.35745 169.760634 \r\nL 291.452283 169.52621 \r\nL 292.547116 168.744406 \r\nL 293.641949 169.086348 \r\nL 295.831616 168.53571 \r\nL 296.926449 167.988785 \r\nL 298.021282 167.628475 \r\nL 299.116116 166.574185 \r\nL 300.210949 168.313252 \r\nL 302.400615 167.546797 \r\nL 303.495448 166.665652 \r\nL 304.590282 165.294072 \r\nL 305.685115 164.511983 \r\nL 306.779948 163.874685 \r\nL 307.874781 164.834647 \r\nL 308.969615 165.21224 \r\nL 310.064448 165.343675 \r\nL 311.159281 164.982649 \r\nL 312.254114 165.188045 \r\nL 313.348947 166.381503 \r\nL 314.443781 168.922479 \r\nL 315.538614 168.658375 \r\nL 316.633447 168.902489 \r\nL 317.72828 167.858454 \r\nL 318.823114 168.564254 \r\nL 319.917947 167.201017 \r\nL 322.107613 165.109602 \r\nL 323.202446 165.191332 \r\nL 324.29728 166.414721 \r\nL 326.486946 167.487584 \r\nL 327.581779 165.709947 \r\nL 328.676613 163.383735 \r\nL 329.771446 163.227463 \r\nL 330.866279 164.593017 \r\nL 331.961112 166.802867 \r\nL 333.055945 165.666541 \r\nL 334.150779 168.702771 \r\nL 335.245612 169.198268 \r\nL 336.340445 168.574471 \r\nL 337.435278 166.58576 \r\nL 338.530112 165.854575 \r\nL 339.624945 164.802145 \r\nL 340.719778 166.195382 \r\nL 341.814611 168.375733 \r\nL 342.909444 168.478078 \r\nL 344.004278 167.396047 \r\nL 345.099111 168.793926 \r\nL 346.193944 166.989691 \r\nL 347.288777 167.34945 \r\nL 348.383611 167.90246 \r\nL 349.478444 166.199969 \r\nL 350.573277 166.176486 \r\nL 351.66811 168.321653 \r\nL 352.762944 168.003853 \r\nL 353.857777 168.373295 \r\nL 354.95261 168.370924 \r\nL 356.047443 168.627386 \r\nL 356.047443 168.627386 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 36.465625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 371.265625 224.64 \r\nL 371.265625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 371.265625 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 36.465625 7.2 \r\nL 371.265625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"paca74df3fa\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"36.465625\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "#clf.history['loss']\n",
    "plt.plot(clf.history['loss'][5:])\n",
    "plt.plot(clf.history['val_0_logloss'][5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = clf.predict_proba(X_val.values)\n",
    "predict = np.array(predict)\n",
    "predict = pd.DataFrame(predict.reshape(predict.shape[1],3), columns=[0.0, 1.0, 2.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8292999066046411"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "log_loss(y_val_onehot, predict)"
   ]
  },
  {
   "source": [
    "* n_d=8, n_a=8, n_steps=5, lambda_sparse=1e-1, gamma = 1.3, scheduler_params = {\"gamma\": 0.95, \"step_size\": 20} = **0.8380463815163395**\n",
    "* n_d=8, n_a=8, n_steps=5, lambda_sparse=1e-2, gamma = 1.3, scheduler_params = {\"gamma\": 0.95, \"step_size\": 20} = **0.8327229226728547**\n",
    "* n_d=8, n_a=8, n_steps=5, lambda_sparse=1e-3, gamma = 1.3, scheduler_params = {\"gamma\": 0.95, \"step_size\": 20} = **0.8373788992342593**\n",
    "* n_d=16, n_a=16, n_steps=5, lambda_sparse=1e-2, gamma = 1.3, scheduler_params = {\"gamma\": 0.95, \"step_size\": 20} = **0.8265579702642575**\n",
    "* 나머지 hyperparameter에 대해서는 logloss가 잘 나오지 않았음. 추후 이것저것 더 해볼 예정"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.00882648, 0.03877237, 0.04557721, 0.11152691, 0.01798807,\n",
       "       0.0165193 , 0.03604957, 0.07089741, 0.02188411, 0.04779797,\n",
       "       0.08276292, 0.02145794, 0.07950223, 0.05027626, 0.03550007,\n",
       "       0.07715966, 0.23750152])"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "source": [
    "### 6_1) unsupervised pretraining test\n",
    "* 논문에서 unsupervised training시 효과가 더 좋다고 나와있고, TabNet을 구현한 github repo에서도 해당 기능을 만들어놨는데, 아직까지는 별로 효과가 없어보임."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device used : cuda\n",
      "epoch 0  | loss: 33618089.82115| val_0_unsup_loss: 14424186.0|  0:00:02s\n",
      "epoch 1  | loss: 2333663.33207| val_0_unsup_loss: 3216637.25|  0:00:04s\n",
      "epoch 2  | loss: 889025.13432| val_0_unsup_loss: 1832241.375|  0:00:07s\n",
      "epoch 3  | loss: 234650.33455| val_0_unsup_loss: 3065469.5|  0:00:09s\n",
      "epoch 4  | loss: 135668.29539| val_0_unsup_loss: 2716994.0|  0:00:11s\n",
      "epoch 5  | loss: 76708.91889| val_0_unsup_loss: 5317295.0|  0:00:14s\n",
      "epoch 6  | loss: 61302.79769| val_0_unsup_loss: 6522936.0|  0:00:16s\n",
      "epoch 7  | loss: 46479.54278| val_0_unsup_loss: 8887041.0|  0:00:19s\n",
      "epoch 8  | loss: 59282.07907| val_0_unsup_loss: 13044147.0|  0:00:21s\n",
      "epoch 9  | loss: 80179.28277| val_0_unsup_loss: 11875471.0|  0:00:23s\n",
      "epoch 10 | loss: 85045.88515| val_0_unsup_loss: 25205670.0|  0:00:26s\n",
      "epoch 11 | loss: 60058.42732| val_0_unsup_loss: 23481978.0|  0:00:28s\n",
      "epoch 12 | loss: 57738.88439| val_0_unsup_loss: 15810113.0|  0:00:30s\n",
      "\n",
      "Early stopping occurred at epoch 12 with best_epoch = 2 and best_val_0_unsup_loss = 1832241.375\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cuda\n",
      "Loading weights from unsupervised pretraining\n",
      "epoch 0  | loss: 1.45015 | val_0_logloss: 1.5953  |  0:00:01s\n",
      "epoch 1  | loss: 0.89808 | val_0_logloss: 1.04891 |  0:00:03s\n",
      "epoch 2  | loss: 0.88465 | val_0_logloss: 0.99275 |  0:00:05s\n",
      "epoch 3  | loss: 0.8768  | val_0_logloss: 0.95187 |  0:00:07s\n",
      "epoch 4  | loss: 0.8693  | val_0_logloss: 0.92826 |  0:00:09s\n",
      "epoch 5  | loss: 0.86274 | val_0_logloss: 0.88435 |  0:00:11s\n",
      "epoch 6  | loss: 0.85916 | val_0_logloss: 0.87779 |  0:00:13s\n",
      "epoch 7  | loss: 0.85708 | val_0_logloss: 0.8764  |  0:00:14s\n",
      "epoch 8  | loss: 0.85575 | val_0_logloss: 0.8605  |  0:00:16s\n",
      "epoch 9  | loss: 0.85496 | val_0_logloss: 0.85847 |  0:00:18s\n",
      "epoch 10 | loss: 0.85256 | val_0_logloss: 0.85326 |  0:00:20s\n",
      "epoch 11 | loss: 0.85147 | val_0_logloss: 0.8525  |  0:00:22s\n",
      "epoch 12 | loss: 0.8508  | val_0_logloss: 0.84719 |  0:00:24s\n",
      "epoch 13 | loss: 0.8511  | val_0_logloss: 0.84723 |  0:00:26s\n",
      "epoch 14 | loss: 0.85002 | val_0_logloss: 0.84846 |  0:00:28s\n",
      "epoch 15 | loss: 0.85018 | val_0_logloss: 0.84988 |  0:00:30s\n",
      "epoch 16 | loss: 0.85212 | val_0_logloss: 0.85336 |  0:00:31s\n",
      "epoch 17 | loss: 0.85088 | val_0_logloss: 0.8474  |  0:00:33s\n",
      "epoch 18 | loss: 0.8477  | val_0_logloss: 0.8456  |  0:00:35s\n",
      "epoch 19 | loss: 0.84859 | val_0_logloss: 0.84781 |  0:00:37s\n",
      "epoch 20 | loss: 0.84965 | val_0_logloss: 0.84786 |  0:00:39s\n",
      "epoch 21 | loss: 0.84937 | val_0_logloss: 0.84779 |  0:00:41s\n",
      "epoch 22 | loss: 0.8482  | val_0_logloss: 0.84525 |  0:00:43s\n",
      "epoch 23 | loss: 0.84757 | val_0_logloss: 0.84395 |  0:00:45s\n",
      "epoch 24 | loss: 0.84878 | val_0_logloss: 0.84472 |  0:00:46s\n",
      "epoch 25 | loss: 0.84625 | val_0_logloss: 0.84538 |  0:00:48s\n",
      "epoch 26 | loss: 0.84527 | val_0_logloss: 0.84491 |  0:00:50s\n",
      "epoch 27 | loss: 0.84469 | val_0_logloss: 0.84391 |  0:00:52s\n",
      "epoch 28 | loss: 0.84404 | val_0_logloss: 0.84181 |  0:00:54s\n",
      "epoch 29 | loss: 0.84535 | val_0_logloss: 0.84376 |  0:00:56s\n",
      "epoch 30 | loss: 0.84585 | val_0_logloss: 0.84295 |  0:00:57s\n",
      "epoch 31 | loss: 0.84433 | val_0_logloss: 0.84213 |  0:00:59s\n",
      "epoch 32 | loss: 0.84349 | val_0_logloss: 0.84034 |  0:01:01s\n",
      "epoch 33 | loss: 0.84289 | val_0_logloss: 0.84145 |  0:01:03s\n",
      "epoch 34 | loss: 0.845   | val_0_logloss: 0.83948 |  0:01:05s\n",
      "epoch 35 | loss: 0.84549 | val_0_logloss: 0.84003 |  0:01:07s\n",
      "epoch 36 | loss: 0.84591 | val_0_logloss: 0.84163 |  0:01:09s\n",
      "epoch 37 | loss: 0.84525 | val_0_logloss: 0.83968 |  0:01:11s\n",
      "epoch 38 | loss: 0.84445 | val_0_logloss: 0.84028 |  0:01:12s\n",
      "epoch 39 | loss: 0.84356 | val_0_logloss: 0.84138 |  0:01:14s\n",
      "epoch 40 | loss: 0.8449  | val_0_logloss: 0.8386  |  0:01:16s\n",
      "epoch 41 | loss: 0.84701 | val_0_logloss: 0.84035 |  0:01:18s\n",
      "epoch 42 | loss: 0.84522 | val_0_logloss: 0.84197 |  0:01:20s\n",
      "epoch 43 | loss: 0.84627 | val_0_logloss: 0.8408  |  0:01:22s\n",
      "epoch 44 | loss: 0.84515 | val_0_logloss: 0.84038 |  0:01:24s\n",
      "epoch 45 | loss: 0.84509 | val_0_logloss: 0.84163 |  0:01:25s\n",
      "epoch 46 | loss: 0.84405 | val_0_logloss: 0.84038 |  0:01:27s\n",
      "epoch 47 | loss: 0.84303 | val_0_logloss: 0.8405  |  0:01:29s\n",
      "epoch 48 | loss: 0.84351 | val_0_logloss: 0.84213 |  0:01:31s\n",
      "epoch 49 | loss: 0.84418 | val_0_logloss: 0.84002 |  0:01:33s\n",
      "epoch 50 | loss: 0.84364 | val_0_logloss: 0.84028 |  0:01:35s\n",
      "epoch 51 | loss: 0.84292 | val_0_logloss: 0.83863 |  0:01:37s\n",
      "epoch 52 | loss: 0.8432  | val_0_logloss: 0.83993 |  0:01:39s\n",
      "epoch 53 | loss: 0.84338 | val_0_logloss: 0.84002 |  0:01:40s\n",
      "epoch 54 | loss: 0.84384 | val_0_logloss: 0.84234 |  0:01:42s\n",
      "epoch 55 | loss: 0.84473 | val_0_logloss: 0.84216 |  0:01:44s\n",
      "epoch 56 | loss: 0.84474 | val_0_logloss: 0.84158 |  0:01:46s\n",
      "epoch 57 | loss: 0.84369 | val_0_logloss: 0.84179 |  0:01:48s\n",
      "epoch 58 | loss: 0.84375 | val_0_logloss: 0.84041 |  0:01:50s\n",
      "epoch 59 | loss: 0.84308 | val_0_logloss: 0.83939 |  0:01:51s\n",
      "epoch 60 | loss: 0.84266 | val_0_logloss: 0.8401  |  0:01:53s\n",
      "epoch 61 | loss: 0.84233 | val_0_logloss: 0.83906 |  0:01:55s\n",
      "epoch 62 | loss: 0.84173 | val_0_logloss: 0.84052 |  0:01:57s\n",
      "epoch 63 | loss: 0.842   | val_0_logloss: 0.8394  |  0:01:59s\n",
      "epoch 64 | loss: 0.84204 | val_0_logloss: 0.83916 |  0:02:01s\n",
      "epoch 65 | loss: 0.84152 | val_0_logloss: 0.83822 |  0:02:02s\n",
      "epoch 66 | loss: 0.84152 | val_0_logloss: 0.83861 |  0:02:04s\n",
      "epoch 67 | loss: 0.84096 | val_0_logloss: 0.83889 |  0:02:06s\n",
      "epoch 68 | loss: 0.84106 | val_0_logloss: 0.83927 |  0:02:08s\n",
      "epoch 69 | loss: 0.84146 | val_0_logloss: 0.83784 |  0:02:10s\n",
      "epoch 70 | loss: 0.84122 | val_0_logloss: 0.83866 |  0:02:12s\n",
      "epoch 71 | loss: 0.8415  | val_0_logloss: 0.83779 |  0:02:14s\n",
      "epoch 72 | loss: 0.84082 | val_0_logloss: 0.83766 |  0:02:15s\n",
      "epoch 73 | loss: 0.84055 | val_0_logloss: 0.83798 |  0:02:17s\n",
      "epoch 74 | loss: 0.84041 | val_0_logloss: 0.83858 |  0:02:19s\n",
      "epoch 75 | loss: 0.84114 | val_0_logloss: 0.83866 |  0:02:21s\n",
      "epoch 76 | loss: 0.84157 | val_0_logloss: 0.83764 |  0:02:23s\n",
      "epoch 77 | loss: 0.84174 | val_0_logloss: 0.83776 |  0:02:25s\n",
      "epoch 78 | loss: 0.84156 | val_0_logloss: 0.83915 |  0:02:26s\n",
      "epoch 79 | loss: 0.84168 | val_0_logloss: 0.83835 |  0:02:28s\n",
      "epoch 80 | loss: 0.84109 | val_0_logloss: 0.83791 |  0:02:30s\n",
      "epoch 81 | loss: 0.84161 | val_0_logloss: 0.8386  |  0:02:32s\n",
      "epoch 82 | loss: 0.83992 | val_0_logloss: 0.83792 |  0:02:34s\n",
      "epoch 83 | loss: 0.84035 | val_0_logloss: 0.8373  |  0:02:36s\n",
      "epoch 84 | loss: 0.84003 | val_0_logloss: 0.8375  |  0:02:38s\n",
      "epoch 85 | loss: 0.84173 | val_0_logloss: 0.83723 |  0:02:39s\n",
      "epoch 86 | loss: 0.84085 | val_0_logloss: 0.83692 |  0:02:41s\n",
      "epoch 87 | loss: 0.84071 | val_0_logloss: 0.83655 |  0:02:43s\n",
      "epoch 88 | loss: 0.84041 | val_0_logloss: 0.83689 |  0:02:45s\n",
      "epoch 89 | loss: 0.83921 | val_0_logloss: 0.83686 |  0:02:47s\n",
      "epoch 90 | loss: 0.83882 | val_0_logloss: 0.83635 |  0:02:49s\n",
      "epoch 91 | loss: 0.83876 | val_0_logloss: 0.83735 |  0:02:51s\n",
      "epoch 92 | loss: 0.83927 | val_0_logloss: 0.83802 |  0:02:52s\n",
      "epoch 93 | loss: 0.83848 | val_0_logloss: 0.83723 |  0:02:54s\n",
      "epoch 94 | loss: 0.8393  | val_0_logloss: 0.83819 |  0:02:56s\n",
      "epoch 95 | loss: 0.83831 | val_0_logloss: 0.83678 |  0:02:58s\n",
      "epoch 96 | loss: 0.83902 | val_0_logloss: 0.83676 |  0:03:00s\n",
      "epoch 97 | loss: 0.83827 | val_0_logloss: 0.83776 |  0:03:02s\n",
      "epoch 98 | loss: 0.8381  | val_0_logloss: 0.83738 |  0:03:04s\n",
      "epoch 99 | loss: 0.83751 | val_0_logloss: 0.83835 |  0:03:06s\n",
      "epoch 100| loss: 0.83695 | val_0_logloss: 0.83866 |  0:03:08s\n",
      "epoch 101| loss: 0.8378  | val_0_logloss: 0.83677 |  0:03:09s\n",
      "epoch 102| loss: 0.83741 | val_0_logloss: 0.83794 |  0:03:11s\n",
      "epoch 103| loss: 0.83766 | val_0_logloss: 0.83756 |  0:03:13s\n",
      "epoch 104| loss: 0.83662 | val_0_logloss: 0.83584 |  0:03:15s\n",
      "epoch 105| loss: 0.83701 | val_0_logloss: 0.83674 |  0:03:17s\n",
      "epoch 106| loss: 0.83624 | val_0_logloss: 0.8374  |  0:03:19s\n",
      "epoch 107| loss: 0.83826 | val_0_logloss: 0.83733 |  0:03:21s\n",
      "epoch 108| loss: 0.83763 | val_0_logloss: 0.83647 |  0:03:22s\n",
      "epoch 109| loss: 0.83686 | val_0_logloss: 0.83564 |  0:03:24s\n",
      "epoch 110| loss: 0.83872 | val_0_logloss: 0.8377  |  0:03:26s\n",
      "epoch 111| loss: 0.83809 | val_0_logloss: 0.83644 |  0:03:28s\n",
      "epoch 112| loss: 0.83859 | val_0_logloss: 0.83594 |  0:03:30s\n",
      "epoch 113| loss: 0.83682 | val_0_logloss: 0.83521 |  0:03:32s\n",
      "epoch 114| loss: 0.83656 | val_0_logloss: 0.835   |  0:03:34s\n",
      "epoch 115| loss: 0.83636 | val_0_logloss: 0.83475 |  0:03:35s\n",
      "epoch 116| loss: 0.83513 | val_0_logloss: 0.83395 |  0:03:37s\n",
      "epoch 117| loss: 0.83554 | val_0_logloss: 0.8346  |  0:03:39s\n",
      "epoch 118| loss: 0.83495 | val_0_logloss: 0.83427 |  0:03:41s\n",
      "epoch 119| loss: 0.83464 | val_0_logloss: 0.83463 |  0:03:43s\n",
      "epoch 120| loss: 0.83413 | val_0_logloss: 0.83443 |  0:03:45s\n",
      "epoch 121| loss: 0.83339 | val_0_logloss: 0.83515 |  0:03:47s\n",
      "epoch 122| loss: 0.83436 | val_0_logloss: 0.8355  |  0:03:48s\n",
      "epoch 123| loss: 0.83442 | val_0_logloss: 0.83483 |  0:03:50s\n",
      "epoch 124| loss: 0.83228 | val_0_logloss: 0.83552 |  0:03:52s\n",
      "epoch 125| loss: 0.83295 | val_0_logloss: 0.83475 |  0:03:54s\n",
      "epoch 126| loss: 0.83398 | val_0_logloss: 0.83357 |  0:03:56s\n",
      "epoch 127| loss: 0.8346  | val_0_logloss: 0.83536 |  0:03:58s\n",
      "epoch 128| loss: 0.83429 | val_0_logloss: 0.83474 |  0:04:00s\n",
      "epoch 129| loss: 0.83283 | val_0_logloss: 0.83416 |  0:04:01s\n",
      "epoch 130| loss: 0.83287 | val_0_logloss: 0.83284 |  0:04:03s\n",
      "epoch 131| loss: 0.83184 | val_0_logloss: 0.83376 |  0:04:05s\n",
      "epoch 132| loss: 0.83456 | val_0_logloss: 0.83486 |  0:04:07s\n",
      "epoch 133| loss: 0.83434 | val_0_logloss: 0.83753 |  0:04:09s\n",
      "epoch 134| loss: 0.83378 | val_0_logloss: 0.83628 |  0:04:11s\n",
      "epoch 135| loss: 0.83349 | val_0_logloss: 0.83467 |  0:04:13s\n",
      "epoch 136| loss: 0.83301 | val_0_logloss: 0.83403 |  0:04:14s\n",
      "epoch 137| loss: 0.83328 | val_0_logloss: 0.83326 |  0:04:16s\n",
      "epoch 138| loss: 0.83181 | val_0_logloss: 0.83426 |  0:04:18s\n",
      "epoch 139| loss: 0.83192 | val_0_logloss: 0.83424 |  0:04:20s\n",
      "epoch 140| loss: 0.83119 | val_0_logloss: 0.83398 |  0:04:22s\n",
      "epoch 141| loss: 0.83156 | val_0_logloss: 0.83427 |  0:04:24s\n",
      "epoch 142| loss: 0.83131 | val_0_logloss: 0.83349 |  0:04:26s\n",
      "epoch 143| loss: 0.83076 | val_0_logloss: 0.83632 |  0:04:27s\n",
      "epoch 144| loss: 0.83083 | val_0_logloss: 0.83641 |  0:04:29s\n",
      "epoch 145| loss: 0.83185 | val_0_logloss: 0.8362  |  0:04:31s\n",
      "epoch 146| loss: 0.83087 | val_0_logloss: 0.83649 |  0:04:33s\n",
      "epoch 147| loss: 0.83101 | val_0_logloss: 0.8376  |  0:04:35s\n",
      "epoch 148| loss: 0.83024 | val_0_logloss: 0.83784 |  0:04:37s\n",
      "epoch 149| loss: 0.82971 | val_0_logloss: 0.83562 |  0:04:39s\n",
      "epoch 150| loss: 0.82852 | val_0_logloss: 0.83617 |  0:04:40s\n",
      "epoch 151| loss: 0.8302  | val_0_logloss: 0.83676 |  0:04:42s\n",
      "epoch 152| loss: 0.83002 | val_0_logloss: 0.83525 |  0:04:44s\n",
      "epoch 153| loss: 0.82969 | val_0_logloss: 0.83286 |  0:04:46s\n",
      "epoch 154| loss: 0.82871 | val_0_logloss: 0.83336 |  0:04:48s\n",
      "epoch 155| loss: 0.82887 | val_0_logloss: 0.83464 |  0:04:50s\n",
      "epoch 156| loss: 0.82766 | val_0_logloss: 0.83356 |  0:04:52s\n",
      "epoch 157| loss: 0.82757 | val_0_logloss: 0.83437 |  0:04:54s\n",
      "epoch 158| loss: 0.82696 | val_0_logloss: 0.83347 |  0:04:55s\n",
      "epoch 159| loss: 0.82739 | val_0_logloss: 0.83673 |  0:04:57s\n",
      "epoch 160| loss: 0.82736 | val_0_logloss: 0.83533 |  0:04:59s\n",
      "epoch 161| loss: 0.82869 | val_0_logloss: 0.83511 |  0:05:01s\n",
      "epoch 162| loss: 0.82762 | val_0_logloss: 0.83576 |  0:05:03s\n",
      "epoch 163| loss: 0.82899 | val_0_logloss: 0.83227 |  0:05:05s\n",
      "epoch 164| loss: 0.82607 | val_0_logloss: 0.83222 |  0:05:07s\n",
      "epoch 165| loss: 0.82618 | val_0_logloss: 0.83234 |  0:05:08s\n",
      "epoch 166| loss: 0.82509 | val_0_logloss: 0.8318  |  0:05:10s\n",
      "epoch 167| loss: 0.82701 | val_0_logloss: 0.83278 |  0:05:12s\n",
      "epoch 168| loss: 0.82573 | val_0_logloss: 0.83379 |  0:05:14s\n",
      "epoch 169| loss: 0.82708 | val_0_logloss: 0.83543 |  0:05:16s\n",
      "epoch 170| loss: 0.82791 | val_0_logloss: 0.83341 |  0:05:18s\n",
      "epoch 171| loss: 0.82653 | val_0_logloss: 0.83371 |  0:05:19s\n",
      "epoch 172| loss: 0.82449 | val_0_logloss: 0.83255 |  0:05:21s\n",
      "epoch 173| loss: 0.82406 | val_0_logloss: 0.83227 |  0:05:23s\n",
      "epoch 174| loss: 0.82334 | val_0_logloss: 0.83414 |  0:05:25s\n",
      "epoch 175| loss: 0.82371 | val_0_logloss: 0.83546 |  0:05:27s\n",
      "epoch 176| loss: 0.8262  | val_0_logloss: 0.83364 |  0:05:29s\n",
      "epoch 177| loss: 0.82522 | val_0_logloss: 0.8339  |  0:05:31s\n",
      "epoch 178| loss: 0.82362 | val_0_logloss: 0.83463 |  0:05:32s\n",
      "epoch 179| loss: 0.82692 | val_0_logloss: 0.83347 |  0:05:34s\n",
      "epoch 180| loss: 0.82758 | val_0_logloss: 0.83364 |  0:05:36s\n",
      "epoch 181| loss: 0.82821 | val_0_logloss: 0.83188 |  0:05:38s\n",
      "epoch 182| loss: 0.82939 | val_0_logloss: 0.8317  |  0:05:40s\n",
      "epoch 183| loss: 0.82731 | val_0_logloss: 0.83201 |  0:05:42s\n",
      "epoch 184| loss: 0.82539 | val_0_logloss: 0.83282 |  0:05:44s\n",
      "epoch 185| loss: 0.82595 | val_0_logloss: 0.83303 |  0:05:45s\n",
      "epoch 186| loss: 0.82477 | val_0_logloss: 0.83236 |  0:05:47s\n",
      "epoch 187| loss: 0.82436 | val_0_logloss: 0.83223 |  0:05:49s\n",
      "epoch 188| loss: 0.82519 | val_0_logloss: 0.83499 |  0:05:51s\n",
      "epoch 189| loss: 0.82613 | val_0_logloss: 0.83331 |  0:05:53s\n",
      "epoch 190| loss: 0.82461 | val_0_logloss: 0.83266 |  0:05:55s\n",
      "epoch 191| loss: 0.82456 | val_0_logloss: 0.83355 |  0:05:56s\n",
      "epoch 192| loss: 0.82461 | val_0_logloss: 0.83305 |  0:05:58s\n",
      "epoch 193| loss: 0.82421 | val_0_logloss: 0.83264 |  0:06:00s\n",
      "epoch 194| loss: 0.82341 | val_0_logloss: 0.83125 |  0:06:02s\n",
      "epoch 195| loss: 0.8222  | val_0_logloss: 0.8342  |  0:06:04s\n",
      "epoch 196| loss: 0.82261 | val_0_logloss: 0.83259 |  0:06:06s\n",
      "epoch 197| loss: 0.82391 | val_0_logloss: 0.83234 |  0:06:08s\n",
      "epoch 198| loss: 0.82266 | val_0_logloss: 0.83102 |  0:06:09s\n",
      "epoch 199| loss: 0.82162 | val_0_logloss: 0.8327  |  0:06:11s\n",
      "epoch 200| loss: 0.82251 | val_0_logloss: 0.83261 |  0:06:13s\n",
      "epoch 201| loss: 0.82408 | val_0_logloss: 0.83164 |  0:06:15s\n",
      "epoch 202| loss: 0.82203 | val_0_logloss: 0.82955 |  0:06:17s\n",
      "epoch 203| loss: 0.82215 | val_0_logloss: 0.82996 |  0:06:19s\n",
      "epoch 204| loss: 0.8224  | val_0_logloss: 0.83018 |  0:06:21s\n",
      "epoch 205| loss: 0.82496 | val_0_logloss: 0.83072 |  0:06:23s\n",
      "epoch 206| loss: 0.82389 | val_0_logloss: 0.8324  |  0:06:24s\n",
      "epoch 207| loss: 0.82231 | val_0_logloss: 0.83212 |  0:06:26s\n",
      "epoch 208| loss: 0.82093 | val_0_logloss: 0.83241 |  0:06:28s\n",
      "epoch 209| loss: 0.82128 | val_0_logloss: 0.83314 |  0:06:30s\n",
      "epoch 210| loss: 0.82219 | val_0_logloss: 0.83478 |  0:06:32s\n",
      "epoch 211| loss: 0.82209 | val_0_logloss: 0.83243 |  0:06:34s\n",
      "epoch 212| loss: 0.82047 | val_0_logloss: 0.83133 |  0:06:35s\n",
      "epoch 213| loss: 0.82148 | val_0_logloss: 0.83164 |  0:06:37s\n",
      "epoch 214| loss: 0.82208 | val_0_logloss: 0.83311 |  0:06:39s\n",
      "epoch 215| loss: 0.82107 | val_0_logloss: 0.83265 |  0:06:41s\n",
      "epoch 216| loss: 0.82157 | val_0_logloss: 0.82903 |  0:06:43s\n",
      "epoch 217| loss: 0.82159 | val_0_logloss: 0.82877 |  0:06:45s\n",
      "epoch 218| loss: 0.81956 | val_0_logloss: 0.82991 |  0:06:47s\n",
      "epoch 219| loss: 0.82045 | val_0_logloss: 0.83043 |  0:06:48s\n",
      "epoch 220| loss: 0.81953 | val_0_logloss: 0.83107 |  0:06:50s\n",
      "epoch 221| loss: 0.81746 | val_0_logloss: 0.82987 |  0:06:52s\n",
      "epoch 222| loss: 0.81794 | val_0_logloss: 0.82987 |  0:06:54s\n",
      "epoch 223| loss: 0.81765 | val_0_logloss: 0.83015 |  0:06:56s\n",
      "epoch 224| loss: 0.81798 | val_0_logloss: 0.83025 |  0:06:58s\n",
      "epoch 225| loss: 0.81742 | val_0_logloss: 0.83187 |  0:07:00s\n",
      "epoch 226| loss: 0.81712 | val_0_logloss: 0.83347 |  0:07:01s\n",
      "epoch 227| loss: 0.81914 | val_0_logloss: 0.83119 |  0:07:03s\n",
      "epoch 228| loss: 0.81962 | val_0_logloss: 0.83143 |  0:07:05s\n",
      "epoch 229| loss: 0.819   | val_0_logloss: 0.83299 |  0:07:07s\n",
      "epoch 230| loss: 0.81782 | val_0_logloss: 0.83402 |  0:07:09s\n",
      "epoch 231| loss: 0.81769 | val_0_logloss: 0.83165 |  0:07:11s\n",
      "epoch 232| loss: 0.81658 | val_0_logloss: 0.83019 |  0:07:12s\n",
      "epoch 233| loss: 0.81464 | val_0_logloss: 0.82916 |  0:07:14s\n",
      "epoch 234| loss: 0.81482 | val_0_logloss: 0.83325 |  0:07:16s\n",
      "epoch 235| loss: 0.81742 | val_0_logloss: 0.83242 |  0:07:18s\n",
      "epoch 236| loss: 0.81599 | val_0_logloss: 0.83004 |  0:07:20s\n",
      "epoch 237| loss: 0.8149  | val_0_logloss: 0.83008 |  0:07:22s\n",
      "epoch 238| loss: 0.81409 | val_0_logloss: 0.82971 |  0:07:24s\n",
      "epoch 239| loss: 0.8125  | val_0_logloss: 0.83067 |  0:07:26s\n",
      "epoch 240| loss: 0.81356 | val_0_logloss: 0.82868 |  0:07:27s\n",
      "epoch 241| loss: 0.81358 | val_0_logloss: 0.82912 |  0:07:29s\n",
      "epoch 242| loss: 0.81231 | val_0_logloss: 0.82905 |  0:07:31s\n",
      "epoch 243| loss: 0.81384 | val_0_logloss: 0.82808 |  0:07:33s\n",
      "epoch 244| loss: 0.81189 | val_0_logloss: 0.82839 |  0:07:35s\n",
      "epoch 245| loss: 0.81445 | val_0_logloss: 0.83099 |  0:07:37s\n",
      "epoch 246| loss: 0.81313 | val_0_logloss: 0.83029 |  0:07:39s\n",
      "epoch 247| loss: 0.81261 | val_0_logloss: 0.82964 |  0:07:40s\n",
      "epoch 248| loss: 0.81314 | val_0_logloss: 0.82958 |  0:07:42s\n",
      "epoch 249| loss: 0.81342 | val_0_logloss: 0.82972 |  0:07:44s\n",
      "epoch 250| loss: 0.81267 | val_0_logloss: 0.8277  |  0:07:46s\n",
      "epoch 251| loss: 0.81387 | val_0_logloss: 0.82844 |  0:07:48s\n",
      "epoch 252| loss: 0.81279 | val_0_logloss: 0.82752 |  0:07:50s\n",
      "epoch 253| loss: 0.81201 | val_0_logloss: 0.83001 |  0:07:52s\n",
      "epoch 254| loss: 0.81221 | val_0_logloss: 0.82948 |  0:07:54s\n",
      "epoch 255| loss: 0.81239 | val_0_logloss: 0.82915 |  0:07:55s\n",
      "epoch 256| loss: 0.81283 | val_0_logloss: 0.82939 |  0:07:57s\n",
      "epoch 257| loss: 0.81355 | val_0_logloss: 0.83202 |  0:07:59s\n",
      "epoch 258| loss: 0.81167 | val_0_logloss: 0.83498 |  0:08:01s\n",
      "epoch 259| loss: 0.81282 | val_0_logloss: 0.83349 |  0:08:03s\n",
      "epoch 260| loss: 0.81271 | val_0_logloss: 0.83216 |  0:08:05s\n",
      "epoch 261| loss: 0.81116 | val_0_logloss: 0.83125 |  0:08:07s\n",
      "epoch 262| loss: 0.80963 | val_0_logloss: 0.83235 |  0:08:10s\n",
      "epoch 263| loss: 0.80928 | val_0_logloss: 0.83178 |  0:08:12s\n",
      "epoch 264| loss: 0.80905 | val_0_logloss: 0.83277 |  0:08:14s\n",
      "epoch 265| loss: 0.80932 | val_0_logloss: 0.83189 |  0:08:16s\n",
      "epoch 266| loss: 0.8078  | val_0_logloss: 0.83384 |  0:08:18s\n",
      "epoch 267| loss: 0.80924 | val_0_logloss: 0.83243 |  0:08:20s\n",
      "epoch 268| loss: 0.80604 | val_0_logloss: 0.83255 |  0:08:22s\n",
      "epoch 269| loss: 0.8082  | val_0_logloss: 0.83293 |  0:08:24s\n",
      "epoch 270| loss: 0.80987 | val_0_logloss: 0.83352 |  0:08:26s\n",
      "epoch 271| loss: 0.80855 | val_0_logloss: 0.83436 |  0:08:28s\n",
      "epoch 272| loss: 0.80992 | val_0_logloss: 0.83344 |  0:08:29s\n",
      "epoch 273| loss: 0.80845 | val_0_logloss: 0.8336  |  0:08:31s\n",
      "epoch 274| loss: 0.8092  | val_0_logloss: 0.83352 |  0:08:33s\n",
      "epoch 275| loss: 0.8079  | val_0_logloss: 0.83282 |  0:08:35s\n",
      "epoch 276| loss: 0.80605 | val_0_logloss: 0.83288 |  0:08:37s\n",
      "epoch 277| loss: 0.80864 | val_0_logloss: 0.83264 |  0:08:39s\n",
      "epoch 278| loss: 0.80701 | val_0_logloss: 0.83053 |  0:08:41s\n",
      "epoch 279| loss: 0.80546 | val_0_logloss: 0.82987 |  0:08:42s\n",
      "epoch 280| loss: 0.80571 | val_0_logloss: 0.82889 |  0:08:44s\n",
      "epoch 281| loss: 0.80508 | val_0_logloss: 0.83151 |  0:08:46s\n",
      "epoch 282| loss: 0.80401 | val_0_logloss: 0.83268 |  0:08:48s\n",
      "epoch 283| loss: 0.80733 | val_0_logloss: 0.83207 |  0:08:50s\n",
      "epoch 284| loss: 0.80585 | val_0_logloss: 0.83139 |  0:08:52s\n",
      "epoch 285| loss: 0.80448 | val_0_logloss: 0.83154 |  0:08:54s\n",
      "epoch 286| loss: 0.80644 | val_0_logloss: 0.83165 |  0:08:55s\n",
      "epoch 287| loss: 0.80772 | val_0_logloss: 0.83301 |  0:08:57s\n",
      "epoch 288| loss: 0.80633 | val_0_logloss: 0.83459 |  0:08:59s\n",
      "epoch 289| loss: 0.80583 | val_0_logloss: 0.83418 |  0:09:01s\n",
      "epoch 290| loss: 0.80682 | val_0_logloss: 0.83376 |  0:09:03s\n",
      "epoch 291| loss: 0.80498 | val_0_logloss: 0.83267 |  0:09:05s\n",
      "epoch 292| loss: 0.80454 | val_0_logloss: 0.8323  |  0:09:07s\n",
      "epoch 293| loss: 0.80483 | val_0_logloss: 0.83306 |  0:09:08s\n",
      "epoch 294| loss: 0.80443 | val_0_logloss: 0.83134 |  0:09:10s\n",
      "epoch 295| loss: 0.80366 | val_0_logloss: 0.83362 |  0:09:12s\n",
      "epoch 296| loss: 0.80394 | val_0_logloss: 0.83358 |  0:09:14s\n",
      "epoch 297| loss: 0.80351 | val_0_logloss: 0.8337  |  0:09:16s\n",
      "epoch 298| loss: 0.80175 | val_0_logloss: 0.83304 |  0:09:18s\n",
      "epoch 299| loss: 0.80477 | val_0_logloss: 0.83251 |  0:09:20s\n",
      "Stop training because you reached max_epochs = 300 with best_epoch = 252 and best_val_0_logloss = 0.82752\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "from pytorch_tabnet.multitask import TabNetMultiTaskClassifier\n",
    "\n",
    "unsupervised_model = TabNetPretrainer(\n",
    "    n_d=16, n_a=16, n_steps=5,\n",
    "    lambda_sparse=1e-2,\n",
    "    gamma = 1.3,\n",
    "    optimizer_fn = torch.optim.Adam,\n",
    "    optimizer_params = dict(lr=2e-2),\n",
    "    scheduler_params = {\"gamma\": 0.9, \"step_size\": 20},\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    mask_type='entmax',\n",
    "    device_name='cuda'\n",
    ")\n",
    "\n",
    "unsupervised_model.fit(\n",
    "    X_train = X_train.values,\n",
    "    eval_set = [X_val.values],\n",
    "    pretraining_ratio=0.8\n",
    ")\n",
    "\n",
    "clf = TabNetMultiTaskClassifier(\n",
    "    n_d=16, n_a=16, n_steps=5,\n",
    "    lambda_sparse=1e-2,\n",
    "    gamma = 1.3,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=1e-2),\n",
    "    scheduler_params = {\"gamma\": 0.9, \"step_size\": 20},\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    mask_type='entmax',\n",
    "    device_name='cuda'\n",
    ")\n",
    "\n",
    "clf.fit(\n",
    "    X_train = X_train.values, y_train = np.array(Y_train).reshape(Y_train.shape[0],1),\n",
    "    eval_set = [(X_val.values, np.array(Y_val).reshape(Y_val.shape[0],1))],\n",
    "    max_epochs=300,\n",
    "    patience=50,\n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    from_unsupervised=unsupervised_model,\n",
    "    drop_last=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8275176201233926"
      ]
     },
     "metadata": {},
     "execution_count": 18
    }
   ],
   "source": [
    "predict = clf.predict_proba(X_val.values)\n",
    "predict = pd.DataFrame(np.reshape(np.array(predict), (np.array(predict).shape[1],3)), columns=[0.0, 1.0, 2.0])\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "log_loss(y_val_onehot, predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1c9a651fd00>]"
      ]
     },
     "metadata": {},
     "execution_count": 19
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 379.002696 248.518125\" width=\"379.002696pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-05-11T19:26:08.155068</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.4.1, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 379.002696 248.518125 \r\nL 379.002696 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 371.265625 224.64 \r\nL 371.265625 7.2 \r\nL 36.465625 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m2d2ee22692\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.683807\" xlink:href=\"#m2d2ee22692\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(48.502557 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 4250 \r\nQ 1547 4250 1301 3770 \r\nQ 1056 3291 1056 2328 \r\nQ 1056 1369 1301 889 \r\nQ 1547 409 2034 409 \r\nQ 2525 409 2770 889 \r\nQ 3016 1369 3016 2328 \r\nQ 3016 3291 2770 3770 \r\nQ 2525 4250 2034 4250 \r\nz\r\nM 2034 4750 \r\nQ 2819 4750 3233 4129 \r\nQ 3647 3509 3647 2328 \r\nQ 3647 1150 3233 529 \r\nQ 2819 -91 2034 -91 \r\nQ 1250 -91 836 529 \r\nQ 422 1150 422 2328 \r\nQ 422 3509 836 4129 \r\nQ 1250 4750 2034 4750 \r\nz\r\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"103.44633\" xlink:href=\"#m2d2ee22692\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 50 -->\r\n      <g transform=\"translate(97.08383 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 691 4666 \r\nL 3169 4666 \r\nL 3169 4134 \r\nL 1269 4134 \r\nL 1269 2991 \r\nQ 1406 3038 1543 3061 \r\nQ 1681 3084 1819 3084 \r\nQ 2600 3084 3056 2656 \r\nQ 3513 2228 3513 1497 \r\nQ 3513 744 3044 326 \r\nQ 2575 -91 1722 -91 \r\nQ 1428 -91 1123 -41 \r\nQ 819 9 494 109 \r\nL 494 744 \r\nQ 775 591 1075 516 \r\nQ 1375 441 1709 441 \r\nQ 2250 441 2565 725 \r\nQ 2881 1009 2881 1497 \r\nQ 2881 1984 2565 2268 \r\nQ 2250 2553 1709 2553 \r\nQ 1456 2553 1204 2497 \r\nQ 953 2441 691 2322 \r\nL 691 4666 \r\nz\r\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"155.208853\" xlink:href=\"#m2d2ee22692\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 100 -->\r\n      <g transform=\"translate(145.665103 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 794 531 \r\nL 1825 531 \r\nL 1825 4091 \r\nL 703 3866 \r\nL 703 4441 \r\nL 1819 4666 \r\nL 2450 4666 \r\nL 2450 531 \r\nL 3481 531 \r\nL 3481 0 \r\nL 794 0 \r\nL 794 531 \r\nz\r\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"206.971376\" xlink:href=\"#m2d2ee22692\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 150 -->\r\n      <g transform=\"translate(197.427626 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"258.7339\" xlink:href=\"#m2d2ee22692\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 200 -->\r\n      <g transform=\"translate(249.19015 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 1228 531 \r\nL 3431 531 \r\nL 3431 0 \r\nL 469 0 \r\nL 469 531 \r\nQ 828 903 1448 1529 \r\nQ 2069 2156 2228 2338 \r\nQ 2531 2678 2651 2914 \r\nQ 2772 3150 2772 3378 \r\nQ 2772 3750 2511 3984 \r\nQ 2250 4219 1831 4219 \r\nQ 1534 4219 1204 4116 \r\nQ 875 4013 500 3803 \r\nL 500 4441 \r\nQ 881 4594 1212 4672 \r\nQ 1544 4750 1819 4750 \r\nQ 2544 4750 2975 4387 \r\nQ 3406 4025 3406 3419 \r\nQ 3406 3131 3298 2873 \r\nQ 3191 2616 2906 2266 \r\nQ 2828 2175 2409 1742 \r\nQ 1991 1309 1228 531 \r\nz\r\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"310.496423\" xlink:href=\"#m2d2ee22692\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 250 -->\r\n      <g transform=\"translate(300.952673 239.238437)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_7\">\r\n     <g id=\"line2d_7\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"362.258946\" xlink:href=\"#m2d2ee22692\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 300 -->\r\n      <g transform=\"translate(352.715196 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2597 2516 \r\nQ 3050 2419 3304 2112 \r\nQ 3559 1806 3559 1356 \r\nQ 3559 666 3084 287 \r\nQ 2609 -91 1734 -91 \r\nQ 1441 -91 1130 -33 \r\nQ 819 25 488 141 \r\nL 488 750 \r\nQ 750 597 1062 519 \r\nQ 1375 441 1716 441 \r\nQ 2309 441 2620 675 \r\nQ 2931 909 2931 1356 \r\nQ 2931 1769 2642 2001 \r\nQ 2353 2234 1838 2234 \r\nL 1294 2234 \r\nL 1294 2753 \r\nL 1863 2753 \r\nQ 2328 2753 2575 2939 \r\nQ 2822 3125 2822 3475 \r\nQ 2822 3834 2567 4026 \r\nQ 2313 4219 1838 4219 \r\nQ 1578 4219 1281 4162 \r\nQ 984 4106 628 3988 \r\nL 628 4550 \r\nQ 988 4650 1302 4700 \r\nQ 1616 4750 1894 4750 \r\nQ 2613 4750 3031 4423 \r\nQ 3450 4097 3450 3541 \r\nQ 3450 3153 3228 2886 \r\nQ 3006 2619 2597 2516 \r\nz\r\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-33\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_8\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"mb4b11a9a18\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mb4b11a9a18\" y=\"218.949132\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.80 -->\r\n      <g transform=\"translate(7.2 222.74835)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 684 794 \r\nL 1344 794 \r\nL 1344 0 \r\nL 684 0 \r\nL 684 794 \r\nz\r\n\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\r\n        <path d=\"M 2034 2216 \r\nQ 1584 2216 1326 1975 \r\nQ 1069 1734 1069 1313 \r\nQ 1069 891 1326 650 \r\nQ 1584 409 2034 409 \r\nQ 2484 409 2743 651 \r\nQ 3003 894 3003 1313 \r\nQ 3003 1734 2745 1975 \r\nQ 2488 2216 2034 2216 \r\nz\r\nM 1403 2484 \r\nQ 997 2584 770 2862 \r\nQ 544 3141 544 3541 \r\nQ 544 4100 942 4425 \r\nQ 1341 4750 2034 4750 \r\nQ 2731 4750 3128 4425 \r\nQ 3525 4100 3525 3541 \r\nQ 3525 3141 3298 2862 \r\nQ 3072 2584 2669 2484 \r\nQ 3125 2378 3379 2068 \r\nQ 3634 1759 3634 1313 \r\nQ 3634 634 3220 271 \r\nQ 2806 -91 2034 -91 \r\nQ 1263 -91 848 271 \r\nQ 434 634 434 1313 \r\nQ 434 1759 690 2068 \r\nQ 947 2378 1403 2484 \r\nz\r\nM 1172 3481 \r\nQ 1172 3119 1398 2916 \r\nQ 1625 2713 2034 2713 \r\nQ 2441 2713 2670 2916 \r\nQ 2900 3119 2900 3481 \r\nQ 2900 3844 2670 4047 \r\nQ 2441 4250 2034 4250 \r\nQ 1625 4250 1398 4047 \r\nQ 1172 3844 1172 3481 \r\nz\r\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mb4b11a9a18\" y=\"171.086034\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.82 -->\r\n      <g transform=\"translate(7.2 174.885252)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-32\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mb4b11a9a18\" y=\"123.222936\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.84 -->\r\n      <g transform=\"translate(7.2 127.022154)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2419 4116 \r\nL 825 1625 \r\nL 2419 1625 \r\nL 2419 4116 \r\nz\r\nM 2253 4666 \r\nL 3047 4666 \r\nL 3047 1625 \r\nL 3713 1625 \r\nL 3713 1100 \r\nL 3047 1100 \r\nL 3047 0 \r\nL 2419 0 \r\nL 2419 1100 \r\nL 313 1100 \r\nL 313 1709 \r\nL 2253 4666 \r\nz\r\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-34\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mb4b11a9a18\" y=\"75.359838\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.86 -->\r\n      <g transform=\"translate(7.2 79.159056)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2113 2584 \r\nQ 1688 2584 1439 2293 \r\nQ 1191 2003 1191 1497 \r\nQ 1191 994 1439 701 \r\nQ 1688 409 2113 409 \r\nQ 2538 409 2786 701 \r\nQ 3034 994 3034 1497 \r\nQ 3034 2003 2786 2293 \r\nQ 2538 2584 2113 2584 \r\nz\r\nM 3366 4563 \r\nL 3366 3988 \r\nQ 3128 4100 2886 4159 \r\nQ 2644 4219 2406 4219 \r\nQ 1781 4219 1451 3797 \r\nQ 1122 3375 1075 2522 \r\nQ 1259 2794 1537 2939 \r\nQ 1816 3084 2150 3084 \r\nQ 2853 3084 3261 2657 \r\nQ 3669 2231 3669 1497 \r\nQ 3669 778 3244 343 \r\nQ 2819 -91 2113 -91 \r\nQ 1303 -91 875 529 \r\nQ 447 1150 447 2328 \r\nQ 447 3434 972 4092 \r\nQ 1497 4750 2381 4750 \r\nQ 2619 4750 2861 4703 \r\nQ 3103 4656 3366 4563 \r\nz\r\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-36\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_12\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#mb4b11a9a18\" y=\"27.49674\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_12\">\r\n      <!-- 0.88 -->\r\n      <g transform=\"translate(7.2 31.295958)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-38\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_13\">\r\n    <path clip-path=\"url(#p5f4b5bf5fa)\" d=\"M 51.683807 68.800567 \r\nL 52.719057 77.361957 \r\nL 53.754308 82.340974 \r\nL 54.789558 85.534093 \r\nL 55.824809 87.421099 \r\nL 56.860059 93.162944 \r\nL 57.89531 95.770931 \r\nL 58.93056 97.388339 \r\nL 59.965811 96.650868 \r\nL 61.001061 99.237352 \r\nL 62.036311 98.868762 \r\nL 63.071562 94.210766 \r\nL 64.106812 97.190961 \r\nL 65.142063 104.803176 \r\nL 66.177313 102.672439 \r\nL 67.212564 100.13142 \r\nL 68.247814 100.796787 \r\nL 69.283065 103.607974 \r\nL 70.318315 105.095585 \r\nL 71.353566 102.21523 \r\nL 72.388816 108.270679 \r\nL 73.424067 110.619039 \r\nL 75.494567 113.563113 \r\nL 76.529818 110.430159 \r\nL 77.565068 109.212673 \r\nL 78.600319 112.851258 \r\nL 79.635569 114.871941 \r\nL 80.67082 116.297197 \r\nL 81.70607 111.250815 \r\nL 83.776571 109.079809 \r\nL 84.811822 110.664325 \r\nL 85.847072 112.561773 \r\nL 86.882323 114.695535 \r\nL 87.917573 111.504208 \r\nL 88.952824 106.447934 \r\nL 89.988074 110.724471 \r\nL 91.023324 108.21272 \r\nL 92.058575 110.886424 \r\nL 93.093825 111.03822 \r\nL 95.164326 115.960583 \r\nL 96.199577 114.813622 \r\nL 97.234827 113.230674 \r\nL 98.270078 114.502241 \r\nL 99.305328 116.244536 \r\nL 100.340579 115.560979 \r\nL 101.375829 115.124312 \r\nL 102.41108 114.035906 \r\nL 103.44633 111.895368 \r\nL 104.48158 111.879239 \r\nL 105.516831 114.387425 \r\nL 106.552081 114.250598 \r\nL 107.587332 115.860654 \r\nL 108.622582 116.864362 \r\nL 109.657833 117.653592 \r\nL 110.693083 119.093513 \r\nL 111.728334 118.435401 \r\nL 112.763584 118.349611 \r\nL 113.798835 119.583303 \r\nL 114.834085 119.595586 \r\nL 115.869336 120.918894 \r\nL 116.904586 120.692858 \r\nL 117.939837 119.738242 \r\nL 118.975087 120.29909 \r\nL 120.010337 119.629355 \r\nL 121.045588 121.264292 \r\nL 122.080838 121.898338 \r\nL 123.116089 122.230546 \r\nL 124.151339 120.4962 \r\nL 125.18659 119.473673 \r\nL 126.22184 119.057234 \r\nL 127.257091 119.490546 \r\nL 128.292341 119.191372 \r\nL 129.327592 120.618309 \r\nL 130.362842 119.36993 \r\nL 131.398093 123.410883 \r\nL 132.433343 122.393236 \r\nL 133.468593 123.145976 \r\nL 134.503844 119.071226 \r\nL 135.539094 121.19351 \r\nL 136.574345 121.522911 \r\nL 137.609595 122.237191 \r\nL 138.644846 125.117724 \r\nL 139.680096 126.05194 \r\nL 140.715347 126.191581 \r\nL 141.750597 124.966988 \r\nL 142.785848 126.865029 \r\nL 143.821098 124.89659 \r\nL 144.856349 127.258149 \r\nL 145.891599 125.571046 \r\nL 146.926849 127.357194 \r\nL 147.9621 127.77979 \r\nL 150.032601 130.531504 \r\nL 151.067851 128.477987 \r\nL 152.103102 129.412222 \r\nL 153.138352 128.821921 \r\nL 154.173603 131.30004 \r\nL 155.208853 130.375405 \r\nL 156.244104 132.229446 \r\nL 157.279354 127.390033 \r\nL 158.314605 128.890711 \r\nL 159.349855 130.743477 \r\nL 160.385106 126.279282 \r\nL 161.420356 127.785413 \r\nL 162.455606 126.597271 \r\nL 163.490857 130.843452 \r\nL 165.561358 131.930318 \r\nL 166.596608 134.875421 \r\nL 167.631859 133.890135 \r\nL 168.667109 135.299727 \r\nL 169.70236 136.038617 \r\nL 170.73761 137.273518 \r\nL 171.772861 139.036821 \r\nL 172.808111 136.711421 \r\nL 173.843362 136.575585 \r\nL 174.878612 141.703839 \r\nL 175.913862 140.104329 \r\nL 176.949113 137.634339 \r\nL 177.984363 136.140788 \r\nL 179.019614 136.88494 \r\nL 180.054864 140.389014 \r\nL 181.090115 140.284983 \r\nL 182.125365 142.754608 \r\nL 183.160616 136.246336 \r\nL 184.195866 136.775596 \r\nL 185.231117 138.101476 \r\nL 186.266367 138.805043 \r\nL 187.301618 139.962212 \r\nL 188.336868 139.302439 \r\nL 189.372119 142.833756 \r\nL 190.407369 142.564909 \r\nL 191.442619 144.317391 \r\nL 192.47787 143.428375 \r\nL 193.51312 144.011639 \r\nL 194.548371 145.329952 \r\nL 195.583621 145.157222 \r\nL 196.618872 142.730395 \r\nL 197.654122 145.078928 \r\nL 198.689373 144.74039 \r\nL 199.724623 146.588741 \r\nL 200.759874 147.859379 \r\nL 201.795124 150.68579 \r\nL 202.830375 146.668153 \r\nL 203.865625 147.097033 \r\nL 204.900875 147.905631 \r\nL 205.936126 150.24032 \r\nL 206.971376 149.854309 \r\nL 208.006627 152.762714 \r\nL 209.041877 152.97212 \r\nL 210.077128 154.440889 \r\nL 211.112378 153.396449 \r\nL 212.147629 153.471893 \r\nL 213.182879 150.283291 \r\nL 214.21813 152.848964 \r\nL 215.25338 149.57587 \r\nL 216.288631 156.555819 \r\nL 217.323881 156.301491 \r\nL 218.359131 158.916765 \r\nL 219.394382 154.310638 \r\nL 220.429632 157.383701 \r\nL 221.464883 154.150807 \r\nL 222.500133 152.144626 \r\nL 223.535384 155.458293 \r\nL 224.570634 160.34293 \r\nL 225.605885 161.359494 \r\nL 226.641135 163.102236 \r\nL 227.676386 162.201065 \r\nL 228.711636 156.240842 \r\nL 229.746887 158.591348 \r\nL 230.782137 162.412467 \r\nL 231.817388 154.519769 \r\nL 233.887888 151.429853 \r\nL 234.923139 148.61176 \r\nL 236.99364 158.177617 \r\nL 238.02889 156.854496 \r\nL 239.064141 159.668506 \r\nL 240.099391 160.640215 \r\nL 241.134642 158.668195 \r\nL 242.169892 156.420336 \r\nL 243.205143 160.04781 \r\nL 244.240393 160.167727 \r\nL 245.275644 160.0433 \r\nL 246.310894 161.022428 \r\nL 247.346144 162.925868 \r\nL 248.381395 165.82179 \r\nL 249.416645 164.851847 \r\nL 250.451896 161.723444 \r\nL 251.487146 164.71955 \r\nL 252.522397 167.211932 \r\nL 253.557647 165.090293 \r\nL 254.592898 161.311983 \r\nL 255.628148 166.228479 \r\nL 256.663399 165.937012 \r\nL 257.698649 165.341819 \r\nL 258.7339 159.21166 \r\nL 259.76915 161.785023 \r\nL 260.804401 165.546283 \r\nL 261.839651 168.870391 \r\nL 262.874901 168.031303 \r\nL 263.910152 165.849307 \r\nL 264.945402 166.072584 \r\nL 265.980653 169.95068 \r\nL 267.015903 167.545011 \r\nL 268.051154 166.119365 \r\nL 269.086404 168.530218 \r\nL 270.121655 167.333703 \r\nL 271.156905 167.291939 \r\nL 272.192156 172.141717 \r\nL 273.227406 170.011056 \r\nL 274.262657 172.213909 \r\nL 275.297907 177.175046 \r\nL 276.333157 176.012843 \r\nL 277.368408 176.700312 \r\nL 278.403658 175.929344 \r\nL 279.438909 177.24933 \r\nL 280.474159 177.966392 \r\nL 281.50941 173.135589 \r\nL 282.54466 172.001821 \r\nL 283.579911 173.479471 \r\nL 284.615161 176.313497 \r\nL 285.650412 176.624092 \r\nL 286.685662 179.26696 \r\nL 287.720913 183.911417 \r\nL 288.756163 183.480629 \r\nL 289.791413 177.251945 \r\nL 290.826664 180.676725 \r\nL 291.861914 183.282925 \r\nL 292.897165 185.233911 \r\nL 293.932415 189.025979 \r\nL 294.967666 186.508479 \r\nL 296.002916 186.45451 \r\nL 297.038167 189.485898 \r\nL 298.073417 185.835516 \r\nL 299.108668 190.498115 \r\nL 300.143918 184.365744 \r\nL 301.179169 187.533689 \r\nL 302.214419 188.763737 \r\nL 303.24967 187.512857 \r\nL 304.28492 186.82555 \r\nL 305.32017 188.617674 \r\nL 306.355421 185.767375 \r\nL 307.390671 188.347925 \r\nL 308.425922 190.2078 \r\nL 310.496423 189.308641 \r\nL 311.531673 188.234749 \r\nL 312.566924 186.521003 \r\nL 313.602174 191.027626 \r\nL 314.637425 188.260167 \r\nL 315.672675 188.527694 \r\nL 317.743176 195.893399 \r\nL 318.778426 196.736315 \r\nL 319.813677 197.297465 \r\nL 320.848927 196.646072 \r\nL 321.884178 200.283134 \r\nL 322.919428 196.837918 \r\nL 323.954679 204.48543 \r\nL 324.989929 199.31562 \r\nL 326.02518 195.321497 \r\nL 327.06043 198.492232 \r\nL 328.095681 195.202633 \r\nL 329.130931 198.735722 \r\nL 330.166182 196.930635 \r\nL 331.201432 200.039136 \r\nL 332.236683 204.481288 \r\nL 333.271933 198.261473 \r\nL 335.342434 205.889421 \r\nL 336.377684 205.285659 \r\nL 337.412935 206.78237 \r\nL 338.448185 209.353157 \r\nL 339.483436 201.411204 \r\nL 341.553937 208.220408 \r\nL 342.589187 203.529099 \r\nL 343.624438 200.484916 \r\nL 344.659688 203.792956 \r\nL 345.694939 204.995413 \r\nL 346.730189 202.628018 \r\nL 347.765439 207.0206 \r\nL 348.80069 208.083189 \r\nL 349.83594 207.378889 \r\nL 350.871191 208.354828 \r\nL 351.906441 210.182605 \r\nL 352.941692 209.51997 \r\nL 353.976942 210.552322 \r\nL 355.012193 214.756364 \r\nL 356.047443 207.526387 \r\nL 356.047443 207.526387 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_14\">\r\n    <path clip-path=\"url(#p5f4b5bf5fa)\" d=\"M 51.683807 17.083636 \r\nL 52.719057 32.78044 \r\nL 53.754308 36.103729 \r\nL 54.789558 74.168641 \r\nL 55.824809 79.020104 \r\nL 56.860059 91.489674 \r\nL 57.89531 93.31107 \r\nL 58.93056 106.011397 \r\nL 59.965811 105.922818 \r\nL 61.001061 102.984294 \r\nL 62.036311 99.577367 \r\nL 63.071562 91.244951 \r\nL 64.106812 105.522365 \r\nL 65.142063 109.826748 \r\nL 66.177313 104.540464 \r\nL 67.212564 104.405376 \r\nL 68.247814 104.578458 \r\nL 69.283065 110.665295 \r\nL 70.318315 113.761873 \r\nL 71.353566 111.92598 \r\nL 72.388816 110.341827 \r\nL 73.424067 111.460968 \r\nL 74.459317 113.860383 \r\nL 75.494567 118.887548 \r\nL 76.529818 114.231023 \r\nL 78.600319 118.12519 \r\nL 79.635569 122.406164 \r\nL 80.67082 119.741758 \r\nL 81.70607 124.457933 \r\nL 82.741321 123.152357 \r\nL 83.776571 119.312613 \r\nL 84.811822 123.995723 \r\nL 85.847072 122.547514 \r\nL 86.882323 119.922764 \r\nL 87.917573 126.565028 \r\nL 89.988074 118.502097 \r\nL 91.023324 121.296473 \r\nL 92.058575 122.314882 \r\nL 93.093825 119.330295 \r\nL 94.129076 122.320435 \r\nL 95.164326 122.027015 \r\nL 96.199577 118.113811 \r\nL 97.234827 123.165557 \r\nL 98.270078 122.555391 \r\nL 99.305328 126.490949 \r\nL 100.340579 123.386122 \r\nL 101.375829 123.185079 \r\nL 102.41108 117.624753 \r\nL 103.44633 118.05903 \r\nL 104.48158 119.437756 \r\nL 105.516831 118.9505 \r\nL 106.552081 122.248334 \r\nL 107.587332 124.693055 \r\nL 108.622582 122.982992 \r\nL 109.657833 125.483727 \r\nL 110.693083 121.97583 \r\nL 111.728334 124.648367 \r\nL 112.763584 125.240234 \r\nL 113.798835 127.485392 \r\nL 114.834085 126.544739 \r\nL 115.869336 125.885865 \r\nL 116.904586 124.973512 \r\nL 117.939837 128.397089 \r\nL 118.975087 126.441515 \r\nL 120.010337 128.513229 \r\nL 121.045588 128.82728 \r\nL 122.080838 128.059084 \r\nL 123.116089 126.622739 \r\nL 124.151339 126.421204 \r\nL 125.18659 128.881937 \r\nL 126.22184 128.594455 \r\nL 127.257091 125.260488 \r\nL 128.292341 127.160612 \r\nL 129.327592 128.216709 \r\nL 130.362842 126.568684 \r\nL 132.433343 129.693367 \r\nL 133.468593 129.201008 \r\nL 135.539094 130.590945 \r\nL 136.574345 131.469683 \r\nL 137.609595 130.675092 \r\nL 138.644846 130.74518 \r\nL 139.680096 131.946142 \r\nL 140.715347 129.568014 \r\nL 141.750597 127.963762 \r\nL 142.785848 129.857443 \r\nL 143.821098 127.563933 \r\nL 144.856349 130.91706 \r\nL 145.891599 130.976611 \r\nL 146.926849 128.577056 \r\nL 147.9621 129.485648 \r\nL 148.99735 127.171555 \r\nL 150.032601 126.433538 \r\nL 151.067851 130.961779 \r\nL 152.103102 128.156469 \r\nL 153.138352 129.066024 \r\nL 154.173603 133.182122 \r\nL 155.208853 131.034437 \r\nL 156.244104 129.436871 \r\nL 157.279354 129.601424 \r\nL 159.349855 133.647362 \r\nL 160.385106 128.72911 \r\nL 161.420356 131.732666 \r\nL 162.455606 132.935474 \r\nL 163.490857 134.693309 \r\nL 165.561358 135.780724 \r\nL 166.596608 137.697951 \r\nL 167.631859 136.145511 \r\nL 168.667109 136.93356 \r\nL 169.70236 136.072246 \r\nL 170.73761 136.561561 \r\nL 171.772861 134.835777 \r\nL 172.808111 133.989868 \r\nL 173.843362 135.589145 \r\nL 174.878612 133.947451 \r\nL 175.913862 135.792467 \r\nL 176.949113 138.606584 \r\nL 177.984363 134.321206 \r\nL 180.054864 137.210524 \r\nL 181.090115 140.364065 \r\nL 182.125365 138.164944 \r\nL 183.160616 135.529599 \r\nL 184.195866 129.129507 \r\nL 185.231117 132.132826 \r\nL 186.266367 135.986115 \r\nL 187.301618 137.518668 \r\nL 188.336868 139.344796 \r\nL 189.372119 136.951141 \r\nL 190.407369 137.017836 \r\nL 191.442619 137.634872 \r\nL 192.47787 136.935646 \r\nL 193.51312 138.809746 \r\nL 194.548371 132.025247 \r\nL 195.583621 131.814678 \r\nL 196.618872 132.309306 \r\nL 197.654122 131.617942 \r\nL 198.689373 128.962691 \r\nL 199.724623 128.401666 \r\nL 200.759874 133.716563 \r\nL 202.830375 130.980041 \r\nL 203.865625 134.58229 \r\nL 204.900875 140.3185 \r\nL 205.936126 139.10426 \r\nL 206.971376 136.04194 \r\nL 208.006627 138.629039 \r\nL 209.041877 136.694949 \r\nL 210.077128 138.857064 \r\nL 211.112378 131.040447 \r\nL 212.147629 134.39928 \r\nL 213.182879 134.92421 \r\nL 214.21813 133.360953 \r\nL 215.25338 141.725709 \r\nL 216.288631 141.831381 \r\nL 217.323881 141.558513 \r\nL 218.359131 142.856795 \r\nL 220.429632 138.076422 \r\nL 221.464883 134.152047 \r\nL 222.500133 139.005431 \r\nL 223.535384 138.277391 \r\nL 224.570634 141.059365 \r\nL 225.605885 141.730684 \r\nL 226.641135 137.248733 \r\nL 227.676386 134.090498 \r\nL 228.711636 138.431619 \r\nL 229.746887 137.82257 \r\nL 230.782137 136.07167 \r\nL 231.817388 138.858119 \r\nL 232.852638 138.438538 \r\nL 233.887888 142.64906 \r\nL 234.923139 143.093487 \r\nL 235.958389 142.338857 \r\nL 236.99364 140.410181 \r\nL 238.02889 139.897799 \r\nL 239.064141 141.517761 \r\nL 240.099391 141.827475 \r\nL 241.134642 135.204346 \r\nL 242.169892 139.242868 \r\nL 243.205143 140.7813 \r\nL 244.240393 138.670194 \r\nL 245.275644 139.866438 \r\nL 246.310894 140.846388 \r\nL 247.346144 144.166715 \r\nL 248.381395 137.099288 \r\nL 249.416645 140.952974 \r\nL 250.451896 141.561507 \r\nL 251.487146 144.720717 \r\nL 252.522397 140.703335 \r\nL 253.557647 140.907733 \r\nL 254.592898 143.236096 \r\nL 255.628148 148.220255 \r\nL 256.663399 147.239916 \r\nL 257.698649 146.725786 \r\nL 258.7339 145.434494 \r\nL 259.76915 141.410087 \r\nL 260.804401 142.0771 \r\nL 261.839651 141.385896 \r\nL 262.874901 139.634494 \r\nL 263.910152 135.715649 \r\nL 264.945402 141.346908 \r\nL 265.980653 143.97488 \r\nL 267.015903 143.236519 \r\nL 268.051154 139.706964 \r\nL 269.086404 140.808232 \r\nL 270.121655 149.482478 \r\nL 271.156905 150.102158 \r\nL 272.192156 147.374774 \r\nL 273.227406 146.131169 \r\nL 274.262657 144.60013 \r\nL 275.297907 147.46207 \r\nL 276.333157 147.470873 \r\nL 277.368408 146.788102 \r\nL 278.403658 146.55525 \r\nL 280.474159 138.84865 \r\nL 281.50941 144.315165 \r\nL 282.54466 143.731892 \r\nL 283.579911 139.994819 \r\nL 284.615161 137.528505 \r\nL 285.650412 143.196946 \r\nL 286.685662 146.692395 \r\nL 287.720913 149.153478 \r\nL 288.756163 139.378096 \r\nL 289.791413 141.367615 \r\nL 290.826664 147.057627 \r\nL 291.861914 146.971325 \r\nL 292.897165 147.853959 \r\nL 293.932415 145.544391 \r\nL 294.967666 150.310431 \r\nL 296.002916 149.267505 \r\nL 297.038167 149.434383 \r\nL 298.073417 151.753933 \r\nL 299.108668 151.016059 \r\nL 300.143918 144.782301 \r\nL 302.214419 148.026668 \r\nL 303.24967 148.163621 \r\nL 304.28492 147.829325 \r\nL 305.32017 152.649048 \r\nL 306.355421 150.897141 \r\nL 307.390671 153.095207 \r\nL 308.425922 147.140966 \r\nL 309.461172 148.396316 \r\nL 310.496423 149.183919 \r\nL 311.531673 148.602864 \r\nL 312.566924 142.320451 \r\nL 313.602174 135.22801 \r\nL 314.637425 138.810484 \r\nL 315.672675 141.977941 \r\nL 316.707926 144.162542 \r\nL 317.743176 141.534064 \r\nL 318.778426 142.905968 \r\nL 319.813677 140.517829 \r\nL 320.848927 142.626302 \r\nL 321.884178 137.959829 \r\nL 322.919428 141.335655 \r\nL 323.954679 141.043228 \r\nL 324.989929 140.149428 \r\nL 326.02518 138.734161 \r\nL 327.06043 136.714088 \r\nL 328.095681 138.91431 \r\nL 329.130931 138.531376 \r\nL 330.166182 138.719212 \r\nL 331.201432 140.395061 \r\nL 332.236683 140.261652 \r\nL 333.271933 140.832832 \r\nL 334.307183 145.886085 \r\nL 335.342434 147.472145 \r\nL 336.377684 149.812874 \r\nL 337.412935 143.53665 \r\nL 338.448185 140.739397 \r\nL 339.483436 142.190113 \r\nL 340.518686 143.83391 \r\nL 342.589187 143.198591 \r\nL 343.624438 139.93957 \r\nL 344.659688 136.163751 \r\nL 346.730189 138.153593 \r\nL 347.765439 140.764862 \r\nL 348.80069 141.640966 \r\nL 349.83594 139.836427 \r\nL 350.871191 143.9585 \r\nL 351.906441 138.482863 \r\nL 352.941692 138.594396 \r\nL 353.976942 138.311534 \r\nL 355.012193 139.874649 \r\nL 356.047443 141.145046 \r\nL 356.047443 141.145046 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 36.465625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 371.265625 224.64 \r\nL 371.265625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 371.265625 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 36.465625 7.2 \r\nL 371.265625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p5f4b5bf5fa\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"36.465625\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.plot(clf.history['loss'][5:])\n",
    "plt.plot(clf.history['val_0_logloss'][5:])"
   ]
  },
  {
   "source": [
    "## 7) Neural Network using PyTorch(미완성)\n",
    "* PyTorch 튜토리얼 참고함\n",
    "* 보기에는 loss가 낮게 나오긴 한데 validation set에 대한 accuracy도 낮고, 실제로 predict값을 확인해보면 학습할수록 예측값이 2로 치우쳐져 있음... 이것저것 해보면서 수정할 예정\n",
    "* logloss는 아직 안뽑아봄"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, stratify=train_y, test_size=0.18, random_state=5000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "device : cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from  torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('device : {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ts = torch.from_numpy(X_train.values)\n",
    "Y_train_ts = np.reshape(Y_train.values, ((Y_train.values.shape[0], 1)))\n",
    "Y_train_ts = torch.from_numpy(Y_train_ts)\n",
    "\n",
    "X_val_ts = torch.from_numpy(X_val.values)\n",
    "Y_val_ts = np.reshape(Y_val.values, ((Y_val.values.shape[0], 1)))\n",
    "Y_val_ts = torch.from_numpy(Y_val_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train_ts, Y_train_ts)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=512, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val_ts, Y_val_ts)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "tags": [
     "outputPrepend"
    ]
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "76\n",
      "(Epoch 655) \n",
      "loss: 0.887684 val_loss : 1.213297030271433\n",
      "(Epoch 656) \n",
      "loss: 0.870018 val_loss : 1.2183430004672233\n",
      "(Epoch 657) \n",
      "loss: 0.936035 val_loss : 1.2177995177088368\n",
      "(Epoch 658) \n",
      "loss: 0.886507 val_loss : 1.2160247286915413\n",
      "(Epoch 659) \n",
      "loss: 0.824758 val_loss : 1.2236996698642488\n",
      "(Epoch 660) \n",
      "loss: 0.873892 val_loss : 1.2246747558902185\n",
      "(Epoch 661) \n",
      "loss: 0.933698 val_loss : 1.2296311681734757\n",
      "(Epoch 662) \n",
      "loss: 0.902022 val_loss : 1.2256955603828235\n",
      "(Epoch 663) \n",
      "loss: 0.850892 val_loss : 1.2321754460767065\n",
      "(Epoch 664) \n",
      "loss: 0.878497 val_loss : 1.2282762163813001\n",
      "(Epoch 665) \n",
      "loss: 0.913722 val_loss : 1.2301874634591503\n",
      "(Epoch 666) \n",
      "loss: 0.847151 val_loss : 1.233423440861855\n",
      "(Epoch 667) \n",
      "loss: 0.838624 val_loss : 1.233456729553235\n",
      "(Epoch 668) \n",
      "loss: 0.923315 val_loss : 1.241985645494106\n",
      "(Epoch 669) \n",
      "loss: 0.868964 val_loss : 1.2432437250093928\n",
      "(Epoch 670) \n",
      "loss: 0.934473 val_loss : 1.2376517234275768\n",
      "(Epoch 671) \n",
      "loss: 0.875713 val_loss : 1.2378360691010375\n",
      "(Epoch 672) \n",
      "loss: 0.900006 val_loss : 1.2415188867769373\n",
      "(Epoch 673) \n",
      "loss: 0.873941 val_loss : 1.2458203452959198\n",
      "(Epoch 674) \n",
      "loss: 0.900388 val_loss : 1.2505978492503071\n",
      "(Epoch 675) \n",
      "loss: 0.829939 val_loss : 1.2471408796879542\n",
      "(Epoch 676) \n",
      "loss: 0.882027 val_loss : 1.2451250716567779\n",
      "(Epoch 677) \n",
      "loss: 0.888080 val_loss : 1.2566382823922186\n",
      "(Epoch 678) \n",
      "loss: 0.826397 val_loss : 1.2560041134800566\n",
      "(Epoch 679) \n",
      "loss: 0.830478 val_loss : 1.2572604372898393\n",
      "(Epoch 680) \n",
      "loss: 0.880549 val_loss : 1.2572076846816362\n",
      "(Epoch 681) \n",
      "loss: 0.911756 val_loss : 1.2570336211593418\n",
      "(Epoch 682) \n",
      "loss: 0.915936 val_loss : 1.260148737526998\n",
      "(Epoch 683) \n",
      "loss: 0.866899 val_loss : 1.2659526829742433\n",
      "(Epoch 684) \n",
      "loss: 0.865165 val_loss : 1.2639626526314385\n",
      "(Epoch 685) \n",
      "loss: 0.890717 val_loss : 1.2663815096562692\n",
      "(Epoch 686) \n",
      "loss: 0.920370 val_loss : 1.26675799383909\n",
      "(Epoch 687) \n",
      "loss: 0.920246 val_loss : 1.2717687214669775\n",
      "(Epoch 688) \n",
      "loss: 0.863018 val_loss : 1.2705657512057253\n",
      "(Epoch 689) \n",
      "loss: 0.869508 val_loss : 1.2734656077274238\n",
      "(Epoch 690) \n",
      "loss: 0.848009 val_loss : 1.277364705336437\n",
      "(Epoch 691) \n",
      "loss: 0.855335 val_loss : 1.2758009146251699\n",
      "(Epoch 692) \n",
      "loss: 0.865587 val_loss : 1.2761226574987323\n",
      "(Epoch 693) \n",
      "loss: 0.899470 val_loss : 1.2797833572826935\n",
      "(Epoch 694) \n",
      "loss: 0.883940 val_loss : 1.2808392136683042\n",
      "(Epoch 695) \n",
      "loss: 0.832128 val_loss : 1.2852632951941694\n",
      "(Epoch 696) \n",
      "loss: 0.866091 val_loss : 1.2838581373193854\n",
      "(Epoch 697) \n",
      "loss: 0.840792 val_loss : 1.2866445710970436\n",
      "(Epoch 698) \n",
      "loss: 0.901936 val_loss : 1.2859915702445144\n",
      "(Epoch 699) \n",
      "loss: 0.867900 val_loss : 1.2881896528416055\n",
      "(Epoch 700) \n",
      "loss: 0.870195 val_loss : 1.2840276797216315\n",
      "(Epoch 701) \n",
      "loss: 0.865594 val_loss : 1.2949949805054317\n",
      "(Epoch 702) \n",
      "loss: 0.893192 val_loss : 1.2895194965951269\n",
      "(Epoch 703) \n",
      "loss: 0.877159 val_loss : 1.2950312938921142\n",
      "(Epoch 704) \n",
      "loss: 0.864818 val_loss : 1.2960498662017026\n",
      "(Epoch 705) \n",
      "loss: 0.878052 val_loss : 1.2981694002901751\n",
      "(Epoch 706) \n",
      "loss: 0.887266 val_loss : 1.3006726466160108\n",
      "(Epoch 707) \n",
      "loss: 0.856528 val_loss : 1.293953822273408\n",
      "(Epoch 708) \n",
      "loss: 0.832570 val_loss : 1.2966776651429721\n",
      "(Epoch 709) \n",
      "loss: 0.867661 val_loss : 1.3031202282065555\n",
      "(Epoch 710) \n",
      "loss: 0.922282 val_loss : 1.3071937826297597\n",
      "(Epoch 711) \n",
      "loss: 0.905307 val_loss : 1.305865113432753\n",
      "(Epoch 712) \n",
      "loss: 0.870016 val_loss : 1.3071313232408979\n",
      "(Epoch 713) \n",
      "loss: 0.909634 val_loss : 1.305285598431456\n",
      "(Epoch 714) \n",
      "loss: 0.878567 val_loss : 1.3167190188620221\n",
      "(Epoch 715) \n",
      "loss: 0.890246 val_loss : 1.3174493856988356\n",
      "(Epoch 716) \n",
      "loss: 0.939925 val_loss : 1.3162801093722385\n",
      "(Epoch 717) \n",
      "loss: 0.918207 val_loss : 1.3139956773239265\n",
      "(Epoch 718) \n",
      "loss: 0.892252 val_loss : 1.3198827389833578\n",
      "(Epoch 719) \n",
      "loss: 0.886357 val_loss : 1.3147319394138601\n",
      "(Epoch 720) \n",
      "loss: 0.840392 val_loss : 1.320055907365011\n",
      "(Epoch 721) \n",
      "loss: 0.917924 val_loss : 1.3212622070269204\n",
      "(Epoch 722) \n",
      "loss: 0.905233 val_loss : 1.3229286978858144\n",
      "(Epoch 723) \n",
      "loss: 0.904174 val_loss : 1.3264147467943408\n",
      "(Epoch 724) \n",
      "loss: 0.888267 val_loss : 1.3302358431355992\n",
      "(Epoch 725) \n",
      "loss: 0.854092 val_loss : 1.3286165388481528\n",
      "(Epoch 726) \n",
      "loss: 0.859311 val_loss : 1.3287053670891424\n",
      "(Epoch 727) \n",
      "loss: 0.911251 val_loss : 1.3306443025579462\n",
      "(Epoch 728) \n",
      "loss: 0.937676 val_loss : 1.3313479171687865\n",
      "(Epoch 729) \n",
      "loss: 0.829738 val_loss : 1.3348819062150865\n",
      "(Epoch 730) \n",
      "loss: 0.830305 val_loss : 1.3333902464978384\n",
      "(Epoch 731) \n",
      "loss: 0.870013 val_loss : 1.3340955453971042\n",
      "(Epoch 732) \n",
      "loss: 0.965949 val_loss : 1.3413255995530435\n",
      "(Epoch 733) \n",
      "loss: 0.865679 val_loss : 1.341564738472981\n",
      "(Epoch 734) \n",
      "loss: 0.876469 val_loss : 1.3459723603133429\n",
      "(Epoch 735) \n",
      "loss: 0.909381 val_loss : 1.3492988557499728\n",
      "(Epoch 736) \n",
      "loss: 0.838950 val_loss : 1.3422141473324782\n",
      "(Epoch 737) \n",
      "loss: 0.869330 val_loss : 1.3537009565941431\n",
      "(Epoch 738) \n",
      "loss: 0.845685 val_loss : 1.3459543313272306\n",
      "(Epoch 739) \n",
      "loss: 0.894868 val_loss : 1.347936999223078\n",
      "(Epoch 740) \n",
      "loss: 0.874307 val_loss : 1.3544193666797169\n",
      "(Epoch 741) \n",
      "loss: 0.863405 val_loss : 1.35672754510976\n",
      "(Epoch 742) \n",
      "loss: 0.903321 val_loss : 1.3605427392321718\n",
      "(Epoch 743) \n",
      "loss: 0.908128 val_loss : 1.359306408254736\n",
      "(Epoch 744) \n",
      "loss: 0.849354 val_loss : 1.3629689038910107\n",
      "(Epoch 745) \n",
      "loss: 0.907893 val_loss : 1.3619356534355793\n",
      "(Epoch 746) \n",
      "loss: 0.887462 val_loss : 1.3622116450664334\n",
      "(Epoch 747) \n",
      "loss: 0.889571 val_loss : 1.3708066898102023\n",
      "(Epoch 748) \n",
      "loss: 0.901828 val_loss : 1.3664570207836122\n",
      "(Epoch 749) \n",
      "loss: 0.839342 val_loss : 1.368965716657361\n",
      "(Epoch 750) \n",
      "loss: 0.903239 val_loss : 1.3653824753699864\n",
      "(Epoch 751) \n",
      "loss: 0.836411 val_loss : 1.3773336647902328\n",
      "(Epoch 752) \n",
      "loss: 0.903481 val_loss : 1.3702052013349606\n",
      "(Epoch 753) \n",
      "loss: 0.872990 val_loss : 1.3739281610627594\n",
      "(Epoch 754) \n",
      "loss: 0.885827 val_loss : 1.3760646996322965\n",
      "(Epoch 755) \n",
      "loss: 0.852204 val_loss : 1.3804758954302794\n",
      "(Epoch 756) \n",
      "loss: 0.892733 val_loss : 1.3812042568630418\n",
      "(Epoch 757) \n",
      "loss: 0.906909 val_loss : 1.3800297515102136\n",
      "(Epoch 758) \n",
      "loss: 0.907694 val_loss : 1.3733804670066858\n",
      "(Epoch 759) \n",
      "loss: 0.818617 val_loss : 1.3793788320048597\n",
      "(Epoch 760) \n",
      "loss: 0.814679 val_loss : 1.3830343400931129\n",
      "(Epoch 761) \n",
      "loss: 0.886872 val_loss : 1.3848015715605921\n",
      "(Epoch 762) \n",
      "loss: 0.915394 val_loss : 1.3887702544233596\n",
      "(Epoch 763) \n",
      "loss: 0.912186 val_loss : 1.3854824180869856\n",
      "(Epoch 764) \n",
      "loss: 0.864000 val_loss : 1.3956000509366786\n",
      "(Epoch 765) \n",
      "loss: 0.857478 val_loss : 1.3964126444998444\n",
      "(Epoch 766) \n",
      "loss: 0.937227 val_loss : 1.393288697549567\n",
      "(Epoch 767) \n",
      "loss: 0.860956 val_loss : 1.3955617699271348\n",
      "(Epoch 768) \n",
      "loss: 0.889146 val_loss : 1.4021213023430765\n",
      "(Epoch 769) \n",
      "loss: 0.872620 val_loss : 1.4082692898799676\n",
      "(Epoch 770) \n",
      "loss: 0.818901 val_loss : 1.3946547225109123\n",
      "(Epoch 771) \n",
      "loss: 0.887164 val_loss : 1.4034527923209774\n",
      "(Epoch 772) \n",
      "loss: 0.869821 val_loss : 1.4038634422725422\n",
      "(Epoch 773) \n",
      "loss: 0.873936 val_loss : 1.3948829512515049\n",
      "(Epoch 774) \n",
      "loss: 0.924664 val_loss : 1.405033768190341\n",
      "(Epoch 775) \n",
      "loss: 0.894381 val_loss : 1.4038963431129845\n",
      "(Epoch 776) \n",
      "loss: 0.877413 val_loss : 1.4107951207518803\n",
      "(Epoch 777) \n",
      "loss: 0.851628 val_loss : 1.4101640328544922\n",
      "(Epoch 778) \n",
      "loss: 0.867630 val_loss : 1.4124704352202022\n",
      "(Epoch 779) \n",
      "loss: 0.925813 val_loss : 1.4135797187914672\n",
      "(Epoch 780) \n",
      "loss: 0.909725 val_loss : 1.417309129950292\n",
      "(Epoch 781) \n",
      "loss: 0.859188 val_loss : 1.4141361562593502\n",
      "(Epoch 782) \n",
      "loss: 0.888429 val_loss : 1.4219612235211097\n",
      "(Epoch 783) \n",
      "loss: 0.894022 val_loss : 1.4193840213385542\n",
      "(Epoch 784) \n",
      "loss: 0.888294 val_loss : 1.417419428483657\n",
      "(Epoch 785) \n",
      "loss: 0.903850 val_loss : 1.4213805466290368\n",
      "(Epoch 786) \n",
      "loss: 0.843466 val_loss : 1.4189648964019497\n",
      "(Epoch 787) \n",
      "loss: 0.912871 val_loss : 1.4291698145212073\n",
      "(Epoch 788) \n",
      "loss: 0.866950 val_loss : 1.4285739300020608\n",
      "(Epoch 789) \n",
      "loss: 0.867626 val_loss : 1.4280087126444991\n",
      "(Epoch 790) \n",
      "loss: 0.841369 val_loss : 1.4297428460364077\n",
      "(Epoch 791) \n",
      "loss: 0.914071 val_loss : 1.4291414973013659\n",
      "(Epoch 792) \n",
      "loss: 0.886090 val_loss : 1.422052447783666\n",
      "(Epoch 793) \n",
      "loss: 0.824214 val_loss : 1.4293727074190874\n",
      "(Epoch 794) \n",
      "loss: 0.873517 val_loss : 1.4396847199164937\n",
      "(Epoch 795) \n",
      "loss: 0.877381 val_loss : 1.443546847717629\n",
      "(Epoch 796) \n",
      "loss: 0.917792 val_loss : 1.4390147450084063\n",
      "(Epoch 797) \n",
      "loss: 0.942398 val_loss : 1.4290425352522647\n",
      "(Epoch 798) \n",
      "loss: 0.816754 val_loss : 1.440046977659906\n",
      "(Epoch 799) \n",
      "loss: 0.868996 val_loss : 1.4430191414158733\n",
      "(Epoch 800) \n",
      "loss: 0.864851 val_loss : 1.4431090460715066\n",
      "(Epoch 801) \n",
      "loss: 0.872739 val_loss : 1.4470133320292566\n",
      "(Epoch 802) \n",
      "loss: 0.914968 val_loss : 1.4480596891081778\n",
      "(Epoch 803) \n",
      "loss: 0.910875 val_loss : 1.4498938329544144\n",
      "(Epoch 804) \n",
      "loss: 0.887604 val_loss : 1.451085255299135\n",
      "(Epoch 805) \n",
      "loss: 0.856137 val_loss : 1.443246689289457\n",
      "(Epoch 806) \n",
      "loss: 0.890483 val_loss : 1.4529865721592434\n",
      "(Epoch 807) \n",
      "loss: 0.908529 val_loss : 1.4580880527043876\n",
      "(Epoch 808) \n",
      "loss: 0.878834 val_loss : 1.4543033838279564\n",
      "(Epoch 809) \n",
      "loss: 0.831470 val_loss : 1.4548685902509348\n",
      "(Epoch 810) \n",
      "loss: 0.894020 val_loss : 1.4505293173615659\n",
      "(Epoch 811) \n",
      "loss: 0.839504 val_loss : 1.462056083937585\n",
      "(Epoch 812) \n",
      "loss: 0.942673 val_loss : 1.4686741329367063\n",
      "(Epoch 813) \n",
      "loss: 0.802095 val_loss : 1.4645928380322788\n",
      "(Epoch 814) \n",
      "loss: 0.887475 val_loss : 1.4577635253412724\n",
      "(Epoch 815) \n",
      "loss: 0.870536 val_loss : 1.4702153443397334\n",
      "(Epoch 816) \n",
      "loss: 0.897379 val_loss : 1.4654406173156413\n",
      "(Epoch 817) \n",
      "loss: 0.876254 val_loss : 1.469166947786707\n",
      "(Epoch 818) \n",
      "loss: 0.816087 val_loss : 1.4675691737608303\n",
      "(Epoch 819) \n",
      "loss: 0.864344 val_loss : 1.475953680073278\n",
      "(Epoch 820) \n",
      "loss: 0.863848 val_loss : 1.4677196498471194\n",
      "(Epoch 821) \n",
      "loss: 0.877109 val_loss : 1.4723019316095254\n",
      "(Epoch 822) \n",
      "loss: 0.884438 val_loss : 1.4717470855111336\n",
      "(Epoch 823) \n",
      "loss: 0.857108 val_loss : 1.4833193449146531\n",
      "(Epoch 824) \n",
      "loss: 0.898074 val_loss : 1.4773613069344043\n",
      "(Epoch 825) \n",
      "loss: 0.872496 val_loss : 1.4826180367249695\n",
      "(Epoch 826) \n",
      "loss: 0.847082 val_loss : 1.4836400362035405\n",
      "(Epoch 827) \n",
      "loss: 0.893343 val_loss : 1.484461164897879\n",
      "(Epoch 828) \n",
      "loss: 0.853414 val_loss : 1.4774964697395403\n",
      "(Epoch 829) \n",
      "loss: 0.928550 val_loss : 1.4845426781167694\n",
      "(Epoch 830) \n",
      "loss: 0.878592 val_loss : 1.4895630917787668\n",
      "(Epoch 831) \n",
      "loss: 0.885223 val_loss : 1.4881299549237168\n",
      "(Epoch 832) \n",
      "loss: 0.921131 val_loss : 1.4883558971689046\n",
      "(Epoch 833) \n",
      "loss: 0.882243 val_loss : 1.4938403966768772\n",
      "(Epoch 834) \n",
      "loss: 0.893163 val_loss : 1.4938542619960644\n",
      "(Epoch 835) \n",
      "loss: 0.925799 val_loss : 1.4982096796236946\n",
      "(Epoch 836) \n",
      "loss: 0.852365 val_loss : 1.496178134827597\n",
      "(Epoch 837) \n",
      "loss: 0.846912 val_loss : 1.4955574067740462\n",
      "(Epoch 838) \n",
      "loss: 0.893756 val_loss : 1.5039765354060426\n",
      "(Epoch 839) \n",
      "loss: 0.922532 val_loss : 1.4997377976520052\n",
      "(Epoch 840) \n",
      "loss: 0.900250 val_loss : 1.4912523586713164\n",
      "(Epoch 841) \n",
      "loss: 0.863880 val_loss : 1.4973875817815236\n",
      "(Epoch 842) \n",
      "loss: 0.910144 val_loss : 1.5031982713472518\n",
      "(Epoch 843) \n",
      "loss: 0.869631 val_loss : 1.5101544991346656\n",
      "(Epoch 844) \n",
      "loss: 0.918538 val_loss : 1.50523454275311\n",
      "(Epoch 845) \n",
      "loss: 0.896940 val_loss : 1.5103131701508878\n",
      "(Epoch 846) \n",
      "loss: 0.874303 val_loss : 1.5122924068449914\n",
      "(Epoch 847) \n",
      "loss: 0.876935 val_loss : 1.5115251315379779\n",
      "(Epoch 848) \n",
      "loss: 0.892505 val_loss : 1.507271161300101\n",
      "(Epoch 849) \n",
      "loss: 0.810154 val_loss : 1.5156064758247771\n",
      "(Epoch 850) \n",
      "loss: 0.886631 val_loss : 1.5136044013585133\n",
      "(Epoch 851) \n",
      "loss: 0.929700 val_loss : 1.512828492272802\n",
      "(Epoch 852) \n",
      "loss: 0.853292 val_loss : 1.5199232068117063\n",
      "(Epoch 853) \n",
      "loss: 0.863935 val_loss : 1.5180930730203306\n",
      "(Epoch 854) \n",
      "loss: 0.887466 val_loss : 1.5265958737788377\n",
      "(Epoch 855) \n",
      "loss: 0.840580 val_loss : 1.525265373735079\n",
      "(Epoch 856) \n",
      "loss: 0.877214 val_loss : 1.523828344559742\n",
      "(Epoch 857) \n",
      "loss: 0.855231 val_loss : 1.5223941859385197\n",
      "(Epoch 858) \n",
      "loss: 0.899922 val_loss : 1.521693421129437\n",
      "(Epoch 859) \n",
      "loss: 0.873226 val_loss : 1.5249566742798577\n",
      "(Epoch 860) \n",
      "loss: 0.835806 val_loss : 1.5297069724931245\n",
      "(Epoch 861) \n",
      "loss: 0.882978 val_loss : 1.535865878159753\n",
      "(Epoch 862) \n",
      "loss: 0.885097 val_loss : 1.5322154781081616\n",
      "(Epoch 863) \n",
      "loss: 0.865270 val_loss : 1.5296913767702847\n",
      "(Epoch 864) \n",
      "loss: 0.836819 val_loss : 1.5271981409443907\n",
      "(Epoch 865) \n",
      "loss: 0.842475 val_loss : 1.5324257379687245\n",
      "(Epoch 866) \n",
      "loss: 0.907113 val_loss : 1.5351890576943739\n",
      "(Epoch 867) \n",
      "loss: 0.857175 val_loss : 1.5395834513752447\n",
      "(Epoch 868) \n",
      "loss: 0.880307 val_loss : 1.5348117874144205\n",
      "(Epoch 869) \n",
      "loss: 0.853127 val_loss : 1.540738747611754\n",
      "(Epoch 870) \n",
      "loss: 0.859945 val_loss : 1.5366345333984572\n",
      "(Epoch 871) \n",
      "loss: 0.880869 val_loss : 1.542895600177575\n",
      "(Epoch 872) \n",
      "loss: 0.839975 val_loss : 1.548597198010178\n",
      "(Epoch 873) \n",
      "loss: 0.885228 val_loss : 1.5436399168746382\n",
      "(Epoch 874) \n",
      "loss: 0.860351 val_loss : 1.5481654611268574\n",
      "(Epoch 875) \n",
      "loss: 0.840437 val_loss : 1.5581617190849415\n",
      "(Epoch 876) \n",
      "loss: 0.879698 val_loss : 1.5454420611180573\n",
      "(Epoch 877) \n",
      "loss: 0.899749 val_loss : 1.5572990174754593\n",
      "(Epoch 878) \n",
      "loss: 0.874800 val_loss : 1.5598424297052647\n",
      "(Epoch 879) \n",
      "loss: 0.869484 val_loss : 1.5465795892800258\n",
      "(Epoch 880) \n",
      "loss: 0.871848 val_loss : 1.5514140059404853\n",
      "(Epoch 881) \n",
      "loss: 0.871887 val_loss : 1.5634000721045094\n",
      "(Epoch 882) \n",
      "loss: 0.886602 val_loss : 1.558977769166305\n",
      "(Epoch 883) \n",
      "loss: 0.889522 val_loss : 1.559287420869765\n",
      "(Epoch 884) \n",
      "loss: 0.915634 val_loss : 1.5583171964733975\n",
      "(Epoch 885) \n",
      "loss: 0.926488 val_loss : 1.5623902162344099\n",
      "(Epoch 886) \n",
      "loss: 0.895833 val_loss : 1.5678104119276413\n",
      "(Epoch 887) \n",
      "loss: 0.891310 val_loss : 1.5622986494425302\n",
      "(Epoch 888) \n",
      "loss: 0.890218 val_loss : 1.5633829213497001\n",
      "(Epoch 889) \n",
      "loss: 0.821572 val_loss : 1.5620993406341352\n",
      "(Epoch 890) \n",
      "loss: 0.809562 val_loss : 1.5707355053695746\n",
      "(Epoch 891) \n",
      "loss: 0.863483 val_loss : 1.5688557814843918\n",
      "(Epoch 892) \n",
      "loss: 0.944078 val_loss : 1.571525773578694\n",
      "(Epoch 893) \n",
      "loss: 0.891434 val_loss : 1.5671704880853947\n",
      "(Epoch 894) \n",
      "loss: 0.832817 val_loss : 1.5743440304642493\n",
      "(Epoch 895) \n",
      "loss: 0.870498 val_loss : 1.5654418592117585\n",
      "(Epoch 896) \n",
      "loss: 0.836576 val_loss : 1.5696735385577807\n",
      "(Epoch 897) \n",
      "loss: 0.868489 val_loss : 1.5747199749764056\n",
      "(Epoch 898) \n",
      "loss: 0.859279 val_loss : 1.5749406683196254\n",
      "(Epoch 899) \n",
      "loss: 0.872706 val_loss : 1.5819198750550032\n",
      "(Epoch 900) \n",
      "loss: 0.923470 val_loss : 1.577910155043728\n",
      "(Epoch 901) \n",
      "loss: 0.849295 val_loss : 1.5814728916152256\n",
      "(Epoch 902) \n",
      "loss: 0.888037 val_loss : 1.591597009939489\n",
      "(Epoch 903) \n",
      "loss: 0.915584 val_loss : 1.5868083407021787\n",
      "(Epoch 904) \n",
      "loss: 0.892598 val_loss : 1.585028171918861\n",
      "(Epoch 905) \n",
      "loss: 0.896963 val_loss : 1.588812986548333\n",
      "(Epoch 906) \n",
      "loss: 0.881262 val_loss : 1.5906945783839592\n",
      "(Epoch 907) \n",
      "loss: 0.826906 val_loss : 1.5787232479459483\n",
      "(Epoch 908) \n",
      "loss: 0.923037 val_loss : 1.5895704632937082\n",
      "(Epoch 909) \n",
      "loss: 0.864932 val_loss : 1.5910785653160107\n",
      "(Epoch 910) \n",
      "loss: 0.913622 val_loss : 1.5946516207451857\n",
      "(Epoch 911) \n",
      "loss: 0.838014 val_loss : 1.5961294426587658\n",
      "(Epoch 912) \n",
      "loss: 0.853579 val_loss : 1.6011248986209379\n",
      "(Epoch 913) \n",
      "loss: 0.895517 val_loss : 1.6006534326004038\n",
      "(Epoch 914) \n",
      "loss: 0.836170 val_loss : 1.597052647775734\n",
      "(Epoch 915) \n",
      "loss: 0.867456 val_loss : 1.5938221713836314\n",
      "(Epoch 916) \n",
      "loss: 0.854199 val_loss : 1.595452045947936\n",
      "(Epoch 917) \n",
      "loss: 0.894909 val_loss : 1.6000518813882452\n",
      "(Epoch 918) \n",
      "loss: 0.919707 val_loss : 1.601219131144379\n",
      "(Epoch 919) \n",
      "loss: 0.913282 val_loss : 1.6034106891402926\n",
      "(Epoch 920) \n",
      "loss: 0.910930 val_loss : 1.605344790636726\n",
      "(Epoch 921) \n",
      "loss: 0.924978 val_loss : 1.6122986327585451\n",
      "(Epoch 922) \n",
      "loss: 0.914915 val_loss : 1.6048556414504334\n",
      "(Epoch 923) \n",
      "loss: 0.899079 val_loss : 1.6079336134866207\n",
      "(Epoch 924) \n",
      "loss: 0.837333 val_loss : 1.6066891806880348\n",
      "(Epoch 925) \n",
      "loss: 0.814004 val_loss : 1.6110242936805368\n",
      "(Epoch 926) \n",
      "loss: 0.874819 val_loss : 1.6112653436264321\n",
      "(Epoch 927) \n",
      "loss: 0.835383 val_loss : 1.608038044762453\n",
      "(Epoch 928) \n",
      "loss: 0.859308 val_loss : 1.6143410629151718\n",
      "(Epoch 929) \n",
      "loss: 0.858531 val_loss : 1.6158714662553657\n",
      "(Epoch 930) \n",
      "loss: 0.869695 val_loss : 1.6066773682533246\n",
      "(Epoch 931) \n",
      "loss: 0.848395 val_loss : 1.6128608542656433\n",
      "(Epoch 932) \n",
      "loss: 0.913970 val_loss : 1.6168584954389567\n",
      "(Epoch 933) \n",
      "loss: 0.864101 val_loss : 1.6180531363378454\n",
      "(Epoch 934) \n",
      "loss: 0.843646 val_loss : 1.6209091518403138\n",
      "(Epoch 935) \n",
      "loss: 0.871153 val_loss : 1.626731200719652\n",
      "(Epoch 936) \n",
      "loss: 0.834282 val_loss : 1.6207931574483716\n",
      "(Epoch 937) \n",
      "loss: 0.845977 val_loss : 1.6283404849107312\n",
      "(Epoch 938) \n",
      "loss: 0.811668 val_loss : 1.622511049536642\n",
      "(Epoch 939) \n",
      "loss: 0.998098 val_loss : 1.634156189154076\n",
      "(Epoch 940) \n",
      "loss: 0.866295 val_loss : 1.6305442195695543\n",
      "(Epoch 941) \n",
      "loss: 0.865035 val_loss : 1.6345654924575241\n",
      "(Epoch 942) \n",
      "loss: 0.906499 val_loss : 1.6282793273588876\n",
      "(Epoch 943) \n",
      "loss: 0.854824 val_loss : 1.634903571960271\n",
      "(Epoch 944) \n",
      "loss: 0.836725 val_loss : 1.6257468720567572\n",
      "(Epoch 945) \n",
      "loss: 0.905582 val_loss : 1.6321699910408196\n",
      "(Epoch 946) \n",
      "loss: 0.957223 val_loss : 1.6398756823916816\n",
      "(Epoch 947) \n",
      "loss: 0.921707 val_loss : 1.6321761556398426\n",
      "(Epoch 948) \n",
      "loss: 0.898590 val_loss : 1.634293203160101\n",
      "(Epoch 949) \n",
      "loss: 0.843701 val_loss : 1.6345691905758895\n",
      "(Epoch 950) \n",
      "loss: 0.840346 val_loss : 1.628496025587137\n",
      "(Epoch 951) \n",
      "loss: 0.879672 val_loss : 1.6305552639659007\n",
      "(Epoch 952) \n",
      "loss: 0.864785 val_loss : 1.6480281118898665\n",
      "(Epoch 953) \n",
      "loss: 0.865484 val_loss : 1.6471002841496711\n",
      "(Epoch 954) \n",
      "loss: 0.841484 val_loss : 1.6429488865542003\n",
      "(Epoch 955) \n",
      "loss: 0.930775 val_loss : 1.645454331996222\n",
      "(Epoch 956) \n",
      "loss: 0.872713 val_loss : 1.648698577495169\n",
      "(Epoch 957) \n",
      "loss: 0.836361 val_loss : 1.6380220421033846\n",
      "(Epoch 958) \n",
      "loss: 0.877924 val_loss : 1.650515283617304\n",
      "(Epoch 959) \n",
      "loss: 0.913378 val_loss : 1.6426999864812817\n",
      "(Epoch 960) \n",
      "loss: 0.935609 val_loss : 1.647739452544267\n",
      "(Epoch 961) \n",
      "loss: 0.940270 val_loss : 1.646403667202883\n",
      "(Epoch 962) \n",
      "loss: 0.876545 val_loss : 1.656537327339667\n",
      "(Epoch 963) \n",
      "loss: 0.904079 val_loss : 1.6506928852186926\n",
      "(Epoch 964) \n",
      "loss: 0.878810 val_loss : 1.6541351853373143\n",
      "(Epoch 965) \n",
      "loss: 0.855173 val_loss : 1.653081169152692\n",
      "(Epoch 966) \n",
      "loss: 0.893664 val_loss : 1.6585595348507325\n",
      "(Epoch 967) \n",
      "loss: 0.860359 val_loss : 1.652914147905965\n",
      "(Epoch 968) \n",
      "loss: 0.820399 val_loss : 1.6497846109715826\n",
      "(Epoch 969) \n",
      "loss: 0.862473 val_loss : 1.6603841086371307\n",
      "(Epoch 970) \n",
      "loss: 0.851991 val_loss : 1.6580295455667708\n",
      "(Epoch 971) \n",
      "loss: 0.874542 val_loss : 1.6536959006591774\n",
      "(Epoch 972) \n",
      "loss: 0.898105 val_loss : 1.6638959171536452\n",
      "(Epoch 973) \n",
      "loss: 0.883829 val_loss : 1.671788380684924\n",
      "(Epoch 974) \n",
      "loss: 0.922913 val_loss : 1.6688857765698129\n",
      "(Epoch 975) \n",
      "loss: 0.917712 val_loss : 1.6598233124708632\n",
      "(Epoch 976) \n",
      "loss: 0.887620 val_loss : 1.6716347832957745\n",
      "(Epoch 977) \n",
      "loss: 0.899105 val_loss : 1.674860918059305\n",
      "(Epoch 978) \n",
      "loss: 0.951957 val_loss : 1.6739643615274646\n",
      "(Epoch 979) \n",
      "loss: 0.821298 val_loss : 1.6660231147725533\n",
      "(Epoch 980) \n",
      "loss: 0.881136 val_loss : 1.6721211187972664\n",
      "(Epoch 981) \n",
      "loss: 0.880520 val_loss : 1.6699264706108763\n",
      "(Epoch 982) \n",
      "loss: 0.790603 val_loss : 1.6695492826611973\n",
      "(Epoch 983) \n",
      "loss: 0.874273 val_loss : 1.6692810494491972\n",
      "(Epoch 984) \n",
      "loss: 0.888244 val_loss : 1.6744064226511246\n",
      "(Epoch 985) \n",
      "loss: 0.827788 val_loss : 1.6748241931619514\n",
      "(Epoch 986) \n",
      "loss: 0.882456 val_loss : 1.674855309221661\n",
      "(Epoch 987) \n",
      "loss: 0.866347 val_loss : 1.671547665873296\n",
      "(Epoch 988) \n",
      "loss: 0.905348 val_loss : 1.6847643958046299\n",
      "(Epoch 989) \n",
      "loss: 0.829365 val_loss : 1.6765086838773113\n",
      "(Epoch 990) \n",
      "loss: 0.870210 val_loss : 1.6857441473575328\n",
      "(Epoch 991) \n",
      "loss: 0.877203 val_loss : 1.6778643259366683\n",
      "(Epoch 992) \n",
      "loss: 0.822641 val_loss : 1.682068598757307\n",
      "(Epoch 993) \n",
      "loss: 0.928700 val_loss : 1.6858478451112444\n",
      "(Epoch 994) \n",
      "loss: 0.840709 val_loss : 1.682644413102975\n",
      "(Epoch 995) \n",
      "loss: 0.911781 val_loss : 1.682404037200943\n",
      "(Epoch 996) \n",
      "loss: 0.837888 val_loss : 1.6755486000137718\n",
      "(Epoch 997) \n",
      "loss: 0.890256 val_loss : 1.690683272664035\n",
      "(Epoch 998) \n",
      "loss: 0.888301 val_loss : 1.6928521085206665\n",
      "(Epoch 999) \n",
      "loss: 0.905970 val_loss : 1.6883320681768066\n",
      "(Epoch 1000) \n",
      "loss: 0.846416 val_loss : 1.684846180264831\n",
      "Done!\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x1a2f6a1f580>]"
      ]
     },
     "metadata": {},
     "execution_count": 33
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 372.103125 248.518125\" width=\"372.103125pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-05-19T21:35:46.143573</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.4.1, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 372.103125 248.518125 \r\nL 372.103125 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 364.903125 224.64 \r\nL 364.903125 7.2 \r\nL 30.103125 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"me8bce5592c\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"45.321307\" xlink:href=\"#me8bce5592c\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(42.140057 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 4250 \r\nQ 1547 4250 1301 3770 \r\nQ 1056 3291 1056 2328 \r\nQ 1056 1369 1301 889 \r\nQ 1547 409 2034 409 \r\nQ 2525 409 2770 889 \r\nQ 3016 1369 3016 2328 \r\nQ 3016 3291 2770 3770 \r\nQ 2525 4250 2034 4250 \r\nz\r\nM 2034 4750 \r\nQ 2819 4750 3233 4129 \r\nQ 3647 3509 3647 2328 \r\nQ 3647 1150 3233 529 \r\nQ 2819 -91 2034 -91 \r\nQ 1250 -91 836 529 \r\nQ 422 1150 422 2328 \r\nQ 422 3509 836 4129 \r\nQ 1250 4750 2034 4750 \r\nz\r\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"106.254968\" xlink:href=\"#me8bce5592c\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 200 -->\r\n      <g transform=\"translate(96.711218 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 1228 531 \r\nL 3431 531 \r\nL 3431 0 \r\nL 469 0 \r\nL 469 531 \r\nQ 828 903 1448 1529 \r\nQ 2069 2156 2228 2338 \r\nQ 2531 2678 2651 2914 \r\nQ 2772 3150 2772 3378 \r\nQ 2772 3750 2511 3984 \r\nQ 2250 4219 1831 4219 \r\nQ 1534 4219 1204 4116 \r\nQ 875 4013 500 3803 \r\nL 500 4441 \r\nQ 881 4594 1212 4672 \r\nQ 1544 4750 1819 4750 \r\nQ 2544 4750 2975 4387 \r\nQ 3406 4025 3406 3419 \r\nQ 3406 3131 3298 2873 \r\nQ 3191 2616 2906 2266 \r\nQ 2828 2175 2409 1742 \r\nQ 1991 1309 1228 531 \r\nz\r\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"167.188629\" xlink:href=\"#me8bce5592c\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 400 -->\r\n      <g transform=\"translate(157.644879 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2419 4116 \r\nL 825 1625 \r\nL 2419 1625 \r\nL 2419 4116 \r\nz\r\nM 2253 4666 \r\nL 3047 4666 \r\nL 3047 1625 \r\nL 3713 1625 \r\nL 3713 1100 \r\nL 3047 1100 \r\nL 3047 0 \r\nL 2419 0 \r\nL 2419 1100 \r\nL 313 1100 \r\nL 313 1709 \r\nL 2253 4666 \r\nz\r\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-34\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"228.12229\" xlink:href=\"#me8bce5592c\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 600 -->\r\n      <g transform=\"translate(218.57854 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2113 2584 \r\nQ 1688 2584 1439 2293 \r\nQ 1191 2003 1191 1497 \r\nQ 1191 994 1439 701 \r\nQ 1688 409 2113 409 \r\nQ 2538 409 2786 701 \r\nQ 3034 994 3034 1497 \r\nQ 3034 2003 2786 2293 \r\nQ 2538 2584 2113 2584 \r\nz\r\nM 3366 4563 \r\nL 3366 3988 \r\nQ 3128 4100 2886 4159 \r\nQ 2644 4219 2406 4219 \r\nQ 1781 4219 1451 3797 \r\nQ 1122 3375 1075 2522 \r\nQ 1259 2794 1537 2939 \r\nQ 1816 3084 2150 3084 \r\nQ 2853 3084 3261 2657 \r\nQ 3669 2231 3669 1497 \r\nQ 3669 778 3244 343 \r\nQ 2819 -91 2113 -91 \r\nQ 1303 -91 875 529 \r\nQ 447 1150 447 2328 \r\nQ 447 3434 972 4092 \r\nQ 1497 4750 2381 4750 \r\nQ 2619 4750 2861 4703 \r\nQ 3103 4656 3366 4563 \r\nz\r\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-36\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"289.055951\" xlink:href=\"#me8bce5592c\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 800 -->\r\n      <g transform=\"translate(279.512201 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 2216 \r\nQ 1584 2216 1326 1975 \r\nQ 1069 1734 1069 1313 \r\nQ 1069 891 1326 650 \r\nQ 1584 409 2034 409 \r\nQ 2484 409 2743 651 \r\nQ 3003 894 3003 1313 \r\nQ 3003 1734 2745 1975 \r\nQ 2488 2216 2034 2216 \r\nz\r\nM 1403 2484 \r\nQ 997 2584 770 2862 \r\nQ 544 3141 544 3541 \r\nQ 544 4100 942 4425 \r\nQ 1341 4750 2034 4750 \r\nQ 2731 4750 3128 4425 \r\nQ 3525 4100 3525 3541 \r\nQ 3525 3141 3298 2862 \r\nQ 3072 2584 2669 2484 \r\nQ 3125 2378 3379 2068 \r\nQ 3634 1759 3634 1313 \r\nQ 3634 634 3220 271 \r\nQ 2806 -91 2034 -91 \r\nQ 1263 -91 848 271 \r\nQ 434 634 434 1313 \r\nQ 434 1759 690 2068 \r\nQ 947 2378 1403 2484 \r\nz\r\nM 1172 3481 \r\nQ 1172 3119 1398 2916 \r\nQ 1625 2713 2034 2713 \r\nQ 2441 2713 2670 2916 \r\nQ 2900 3119 2900 3481 \r\nQ 2900 3844 2670 4047 \r\nQ 2441 4250 2034 4250 \r\nQ 1625 4250 1398 4047 \r\nQ 1172 3844 1172 3481 \r\nz\r\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"349.989611\" xlink:href=\"#me8bce5592c\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 1000 -->\r\n      <g transform=\"translate(337.264611 239.238437)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 794 531 \r\nL 1825 531 \r\nL 1825 4091 \r\nL 703 3866 \r\nL 703 4441 \r\nL 1819 4666 \r\nL 2450 4666 \r\nL 2450 531 \r\nL 3481 531 \r\nL 3481 0 \r\nL 794 0 \r\nL 794 531 \r\nz\r\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"190.869141\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m0691847489\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m0691847489\" y=\"212.69753\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0.8 -->\r\n      <g transform=\"translate(7.2 216.496749)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 684 794 \r\nL 1344 794 \r\nL 1344 0 \r\nL 684 0 \r\nL 684 794 \r\nz\r\n\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m0691847489\" y=\"168.879771\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 1.0 -->\r\n      <g transform=\"translate(7.2 172.67899)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m0691847489\" y=\"125.062012\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 1.2 -->\r\n      <g transform=\"translate(7.2 128.86123)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-32\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m0691847489\" y=\"81.244252\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 1.4 -->\r\n      <g transform=\"translate(7.2 85.043471)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-34\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"30.103125\" xlink:href=\"#m0691847489\" y=\"37.426493\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 1.6 -->\r\n      <g transform=\"translate(7.2 41.225712)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-36\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_12\">\r\n    <path clip-path=\"url(#pb825eedddf)\" d=\"M 45.321307 147.535675 \r\nL 45.625975 148.023235 \r\nL 45.930643 147.701495 \r\nL 46.235312 148.08132 \r\nL 46.53998 150.298219 \r\nL 47.149317 147.834302 \r\nL 47.453985 147.666027 \r\nL 47.758653 148.834364 \r\nL 48.063322 149.310615 \r\nL 48.36799 147.834014 \r\nL 48.672658 150.252305 \r\nL 48.977326 151.595419 \r\nL 49.586663 150.585485 \r\nL 49.891331 150.61834 \r\nL 50.196 150.063528 \r\nL 50.500668 151.339416 \r\nL 50.805336 153.689331 \r\nL 51.110005 151.30118 \r\nL 51.414673 151.454568 \r\nL 51.719341 153.130732 \r\nL 52.02401 152.806249 \r\nL 52.328678 152.300041 \r\nL 52.938014 154.176264 \r\nL 53.242683 153.798867 \r\nL 53.547351 153.6629 \r\nL 53.852019 155.894529 \r\nL 54.156688 153.614922 \r\nL 54.461356 153.991954 \r\nL 54.766024 155.886642 \r\nL 55.070693 152.834638 \r\nL 55.375361 153.28391 \r\nL 55.680029 154.761112 \r\nL 55.984697 154.179921 \r\nL 56.289366 154.490561 \r\nL 56.594034 155.810692 \r\nL 56.898702 155.586474 \r\nL 57.508039 156.091455 \r\nL 57.812707 156.874742 \r\nL 58.422044 155.263297 \r\nL 58.726712 157.384424 \r\nL 59.031381 155.480385 \r\nL 59.336049 156.948472 \r\nL 59.640717 156.870302 \r\nL 59.945385 157.690076 \r\nL 60.250054 156.096547 \r\nL 60.554722 161.918512 \r\nL 60.85939 157.711309 \r\nL 61.164059 158.59541 \r\nL 61.468727 155.341336 \r\nL 61.773395 161.634276 \r\nL 62.078064 159.061005 \r\nL 62.382732 158.607215 \r\nL 62.6874 154.832386 \r\nL 62.992068 160.30456 \r\nL 63.296737 158.152406 \r\nL 63.601405 158.320185 \r\nL 63.906073 157.738732 \r\nL 64.210742 159.448509 \r\nL 64.51541 159.854139 \r\nL 64.820078 160.763417 \r\nL 65.429415 157.468887 \r\nL 65.734083 162.11118 \r\nL 66.038752 158.908767 \r\nL 66.34342 161.150555 \r\nL 66.648088 156.458221 \r\nL 66.952756 159.343152 \r\nL 67.257425 158.311305 \r\nL 67.866761 162.495472 \r\nL 68.17143 161.275345 \r\nL 68.476098 162.318396 \r\nL 68.780766 160.617838 \r\nL 69.085435 160.880998 \r\nL 69.390103 160.326133 \r\nL 69.694771 164.18553 \r\nL 69.999439 163.050833 \r\nL 70.304108 159.287156 \r\nL 70.608776 163.251493 \r\nL 70.913444 160.655683 \r\nL 71.218113 164.324318 \r\nL 71.522781 162.309699 \r\nL 71.827449 161.636209 \r\nL 72.132118 165.201576 \r\nL 72.436786 162.06477 \r\nL 72.741454 162.958717 \r\nL 73.046123 167.215125 \r\nL 73.350791 163.114533 \r\nL 73.655459 163.437945 \r\nL 73.960127 160.917405 \r\nL 74.264796 165.851952 \r\nL 74.569464 164.498809 \r\nL 74.874132 162.124317 \r\nL 75.178801 166.439046 \r\nL 75.483469 167.405025 \r\nL 75.788137 164.516777 \r\nL 76.092806 162.962608 \r\nL 76.397474 161.986522 \r\nL 76.702142 168.841248 \r\nL 77.006811 168.493076 \r\nL 77.311479 167.048835 \r\nL 77.616147 166.598179 \r\nL 77.920815 163.296937 \r\nL 78.225484 166.060918 \r\nL 78.530152 164.760061 \r\nL 78.83482 168.323522 \r\nL 79.139489 169.332229 \r\nL 79.444157 167.973131 \r\nL 79.748825 169.541286 \r\nL 80.053494 167.125203 \r\nL 80.358162 168.349901 \r\nL 80.66283 166.718293 \r\nL 80.967498 170.431146 \r\nL 81.272167 169.506027 \r\nL 81.576835 167.665363 \r\nL 81.881503 167.212957 \r\nL 82.186172 166.215951 \r\nL 82.49084 166.305638 \r\nL 82.795508 169.346998 \r\nL 83.100177 168.407098 \r\nL 83.404845 168.990039 \r\nL 83.709513 164.832119 \r\nL 84.014182 169.743879 \r\nL 84.31885 169.467478 \r\nL 84.623518 168.683028 \r\nL 84.928186 172.274121 \r\nL 85.232855 168.027559 \r\nL 85.537523 174.362104 \r\nL 85.842191 167.42349 \r\nL 86.14686 167.117472 \r\nL 86.451528 174.210166 \r\nL 86.756196 171.142231 \r\nL 87.060865 166.460671 \r\nL 87.670201 166.007743 \r\nL 87.974869 171.603987 \r\nL 88.279538 167.846853 \r\nL 88.584206 169.450972 \r\nL 88.888874 172.427705 \r\nL 89.193543 167.915333 \r\nL 89.498211 168.249375 \r\nL 89.802879 167.345999 \r\nL 90.107548 173.428524 \r\nL 90.412216 168.089327 \r\nL 90.716884 172.101655 \r\nL 91.021553 172.882435 \r\nL 91.326221 174.68707 \r\nL 91.630889 170.306696 \r\nL 91.935557 169.391907 \r\nL 92.240226 173.509514 \r\nL 92.544894 172.415482 \r\nL 92.849562 167.193787 \r\nL 93.154231 176.526037 \r\nL 93.458899 179.413958 \r\nL 93.763567 169.053204 \r\nL 94.068236 170.160438 \r\nL 94.372904 172.780434 \r\nL 94.677572 173.206683 \r\nL 94.98224 171.292746 \r\nL 95.286909 172.109634 \r\nL 95.591577 173.519648 \r\nL 95.896245 168.959102 \r\nL 96.200914 174.691785 \r\nL 96.505582 169.440185 \r\nL 96.81025 179.525009 \r\nL 97.419587 175.291088 \r\nL 97.724255 171.552353 \r\nL 98.028924 170.352532 \r\nL 98.333592 175.726243 \r\nL 98.942928 176.530125 \r\nL 99.247597 172.869925 \r\nL 99.856933 180.883663 \r\nL 100.161602 176.395189 \r\nL 100.46627 168.159374 \r\nL 101.075607 171.203072 \r\nL 101.380275 177.125745 \r\nL 101.684943 170.321113 \r\nL 101.989611 177.657392 \r\nL 102.29428 174.332657 \r\nL 102.598948 175.441968 \r\nL 102.903616 177.632071 \r\nL 103.208285 173.153495 \r\nL 103.512953 176.019633 \r\nL 103.817621 167.372508 \r\nL 104.12229 177.913381 \r\nL 104.426958 180.283002 \r\nL 104.731626 176.470995 \r\nL 105.036295 185.084285 \r\nL 105.340963 176.258125 \r\nL 105.645631 171.917605 \r\nL 105.950299 177.507177 \r\nL 106.254968 178.823756 \r\nL 106.559636 181.631405 \r\nL 106.864304 168.591278 \r\nL 107.168973 185.520877 \r\nL 107.473641 172.601425 \r\nL 107.778309 180.682389 \r\nL 108.082978 176.667319 \r\nL 108.387646 182.848477 \r\nL 108.692314 179.077722 \r\nL 108.996982 182.129046 \r\nL 109.301651 179.697097 \r\nL 109.910987 180.146447 \r\nL 110.215656 181.64479 \r\nL 110.520324 182.070792 \r\nL 110.824992 175.654538 \r\nL 111.129661 174.515009 \r\nL 111.434329 178.904289 \r\nL 111.738997 173.723129 \r\nL 112.043666 181.836231 \r\nL 112.348334 185.548026 \r\nL 112.653002 185.813744 \r\nL 112.95767 187.041472 \r\nL 113.262339 183.379953 \r\nL 113.567007 183.43685 \r\nL 113.871675 178.047364 \r\nL 114.176344 181.884457 \r\nL 114.481012 181.21128 \r\nL 114.78568 185.18118 \r\nL 115.090349 176.173256 \r\nL 115.395017 179.537454 \r\nL 115.699685 180.146094 \r\nL 116.004354 185.203524 \r\nL 116.309022 180.775955 \r\nL 116.61369 186.256291 \r\nL 116.918358 181.185019 \r\nL 117.223027 184.209691 \r\nL 117.527695 185.268256 \r\nL 117.832363 178.412185 \r\nL 118.137032 183.819784 \r\nL 118.4417 185.792654 \r\nL 118.746368 189.780145 \r\nL 119.051037 187.482961 \r\nL 119.355705 186.85374 \r\nL 119.660373 174.750653 \r\nL 119.965041 189.623584 \r\nL 120.26971 182.721847 \r\nL 120.574378 187.499389 \r\nL 120.879046 181.010477 \r\nL 121.183715 182.962754 \r\nL 121.488383 176.515238 \r\nL 121.793051 182.997855 \r\nL 122.09772 182.008462 \r\nL 122.402388 192.169941 \r\nL 122.707056 180.964001 \r\nL 123.011725 188.731034 \r\nL 123.316393 179.048693 \r\nL 123.621061 182.4359 \r\nL 123.925729 181.258553 \r\nL 124.230398 182.46711 \r\nL 124.535066 179.158569 \r\nL 125.144403 184.067325 \r\nL 125.449071 184.144502 \r\nL 126.058408 175.842858 \r\nL 126.363076 178.936166 \r\nL 126.667744 197.432953 \r\nL 126.972412 184.01603 \r\nL 127.277081 188.343242 \r\nL 127.581749 186.796974 \r\nL 127.886417 187.645907 \r\nL 128.191086 180.591788 \r\nL 128.495754 181.534065 \r\nL 128.800422 185.572289 \r\nL 129.105091 191.539858 \r\nL 129.409759 187.023595 \r\nL 129.714427 171.354409 \r\nL 130.019096 173.580567 \r\nL 130.323764 188.561911 \r\nL 130.628432 186.775244 \r\nL 130.9331 188.31514 \r\nL 131.237769 181.065454 \r\nL 131.542437 188.788022 \r\nL 131.847105 193.061472 \r\nL 132.151774 191.635356 \r\nL 132.456442 177.028066 \r\nL 132.76111 184.304797 \r\nL 133.065779 184.645943 \r\nL 133.370447 179.839855 \r\nL 133.675115 181.333105 \r\nL 133.979783 191.350859 \r\nL 134.284452 191.761386 \r\nL 134.58912 186.310485 \r\nL 134.893788 188.693059 \r\nL 135.198457 190.114186 \r\nL 135.503125 185.134143 \r\nL 135.807793 194.29821 \r\nL 136.112462 179.837817 \r\nL 136.41713 189.507479 \r\nL 136.721798 182.684407 \r\nL 137.026467 179.169447 \r\nL 137.635803 190.741174 \r\nL 137.940471 188.010049 \r\nL 138.24514 183.960974 \r\nL 138.549808 185.851131 \r\nL 138.854476 184.434144 \r\nL 139.159145 188.845415 \r\nL 139.463813 196.215869 \r\nL 139.768481 188.128949 \r\nL 140.07315 185.957064 \r\nL 140.377818 175.461126 \r\nL 140.682486 193.373405 \r\nL 140.987154 188.178742 \r\nL 141.291823 192.417403 \r\nL 141.596491 181.77714 \r\nL 141.901159 188.300593 \r\nL 142.205828 191.598178 \r\nL 142.510496 182.855946 \r\nL 142.815164 193.916164 \r\nL 143.119833 173.896405 \r\nL 143.424501 188.232557 \r\nL 143.729169 181.004248 \r\nL 144.033838 191.548973 \r\nL 144.338506 189.853273 \r\nL 144.643174 181.677032 \r\nL 144.947842 194.884598 \r\nL 145.252511 185.97071 \r\nL 145.557179 189.810219 \r\nL 145.861847 189.52358 \r\nL 146.166516 175.640395 \r\nL 146.471184 196.640172 \r\nL 147.080521 183.55541 \r\nL 147.385189 179.857484 \r\nL 147.689857 188.005636 \r\nL 147.994525 190.188739 \r\nL 148.299194 187.81144 \r\nL 148.603862 187.06311 \r\nL 148.90853 189.420403 \r\nL 149.213199 190.874921 \r\nL 149.517867 175.702333 \r\nL 149.822535 182.418702 \r\nL 150.127204 177.885344 \r\nL 150.431872 188.11085 \r\nL 150.73654 179.967229 \r\nL 151.041209 191.880285 \r\nL 151.345877 190.490578 \r\nL 151.650545 189.839248 \r\nL 151.955213 190.400224 \r\nL 152.259882 190.42104 \r\nL 152.56455 192.330876 \r\nL 152.869218 184.409502 \r\nL 153.173887 190.61013 \r\nL 153.783223 189.581065 \r\nL 154.39256 196.347814 \r\nL 154.697228 182.141909 \r\nL 155.001896 189.905234 \r\nL 155.306565 194.811417 \r\nL 155.611233 193.937333 \r\nL 155.915901 194.783132 \r\nL 156.22057 196.502207 \r\nL 156.525238 190.144313 \r\nL 156.829906 194.245818 \r\nL 157.134575 196.401877 \r\nL 157.439243 189.917248 \r\nL 158.04858 191.591139 \r\nL 158.353248 195.292592 \r\nL 158.657916 185.533308 \r\nL 158.962584 193.396493 \r\nL 159.267253 188.860563 \r\nL 159.571921 195.27034 \r\nL 159.876589 192.11717 \r\nL 160.485926 180.838232 \r\nL 160.790594 184.761304 \r\nL 161.095263 183.573131 \r\nL 161.399931 189.885306 \r\nL 161.704599 182.146976 \r\nL 162.009268 194.268658 \r\nL 162.313936 185.891679 \r\nL 162.923272 191.368645 \r\nL 163.227941 190.445995 \r\nL 163.532609 185.899187 \r\nL 163.837277 194.196731 \r\nL 164.141946 194.463102 \r\nL 164.446614 200.290669 \r\nL 164.751282 189.668258 \r\nL 165.055951 198.213277 \r\nL 165.360619 198.635177 \r\nL 165.665287 191.526616 \r\nL 166.274624 200.188576 \r\nL 166.579292 193.564128 \r\nL 166.88396 198.591261 \r\nL 167.188629 185.144668 \r\nL 167.493297 186.134662 \r\nL 167.797965 193.723183 \r\nL 168.102634 198.30427 \r\nL 168.407302 197.978651 \r\nL 168.71197 186.905674 \r\nL 169.016639 200.113136 \r\nL 169.321307 191.891216 \r\nL 169.625975 179.148043 \r\nL 169.930643 189.085356 \r\nL 170.235312 206.593913 \r\nL 170.53998 189.064619 \r\nL 170.844648 191.549769 \r\nL 171.149317 199.999395 \r\nL 171.453985 196.135296 \r\nL 171.758653 198.505348 \r\nL 172.063322 183.399933 \r\nL 172.36799 195.870426 \r\nL 173.281995 187.639587 \r\nL 173.586663 186.976688 \r\nL 173.891331 197.379648 \r\nL 174.196 192.294051 \r\nL 174.500668 191.48808 \r\nL 174.805336 184.681802 \r\nL 175.110005 184.487554 \r\nL 175.414673 206.213982 \r\nL 175.719341 192.031309 \r\nL 176.02401 196.969278 \r\nL 176.328678 185.063848 \r\nL 176.633346 197.811173 \r\nL 176.938014 191.701329 \r\nL 177.242683 198.816537 \r\nL 177.852019 186.869528 \r\nL 178.156688 188.511465 \r\nL 178.461356 175.481863 \r\nL 178.766024 191.508191 \r\nL 179.070693 197.009995 \r\nL 179.375361 182.417135 \r\nL 179.680029 192.061214 \r\nL 179.984697 188.05108 \r\nL 180.289366 200.763055 \r\nL 180.594034 196.276748 \r\nL 180.898702 202.278296 \r\nL 181.203371 201.85065 \r\nL 181.508039 193.849278 \r\nL 181.812707 196.760142 \r\nL 182.117376 185.228766 \r\nL 182.422044 179.33682 \r\nL 182.726712 201.053494 \r\nL 183.031381 186.392572 \r\nL 183.336049 194.248848 \r\nL 183.640717 190.666609 \r\nL 183.945385 205.929707 \r\nL 184.250054 200.724101 \r\nL 184.554722 198.446283 \r\nL 184.85939 199.993845 \r\nL 185.164059 190.981206 \r\nL 185.468727 194.065295 \r\nL 185.773395 199.543358 \r\nL 186.078064 186.456572 \r\nL 186.382732 211.137573 \r\nL 186.6874 196.467223 \r\nL 186.992068 188.038504 \r\nL 187.296737 198.58498 \r\nL 187.601405 191.944143 \r\nL 187.906073 200.368839 \r\nL 188.210742 191.511743 \r\nL 188.51541 190.056049 \r\nL 188.820078 185.703803 \r\nL 189.124747 191.615141 \r\nL 189.429415 187.49265 \r\nL 190.038752 196.416672 \r\nL 190.34342 198.901653 \r\nL 190.648088 183.538995 \r\nL 190.952756 201.955054 \r\nL 191.257425 175.688647 \r\nL 191.562093 196.576524 \r\nL 191.866761 196.615334 \r\nL 192.17143 194.981677 \r\nL 192.476098 194.114265 \r\nL 192.780766 205.162626 \r\nL 193.085435 208.111034 \r\nL 193.390103 190.903207 \r\nL 193.694771 199.127674 \r\nL 194.304108 195.456335 \r\nL 194.608776 189.817741 \r\nL 194.913444 195.928629 \r\nL 195.218113 188.084824 \r\nL 195.522781 210.228073 \r\nL 195.827449 188.615843 \r\nL 196.132118 182.669716 \r\nL 196.436786 207.348784 \r\nL 196.741454 191.940303 \r\nL 197.046123 198.039282 \r\nL 197.350791 199.720826 \r\nL 197.655459 187.165739 \r\nL 197.960127 201.172589 \r\nL 198.264796 192.458094 \r\nL 198.569464 191.896165 \r\nL 198.874132 201.626262 \r\nL 199.178801 183.881682 \r\nL 199.483469 191.248126 \r\nL 199.788137 195.890641 \r\nL 200.092806 195.988059 \r\nL 200.397474 195.756672 \r\nL 200.702142 194.943872 \r\nL 201.006811 195.203949 \r\nL 201.311479 197.869493 \r\nL 201.616147 192.844736 \r\nL 201.920815 200.648321 \r\nL 202.225484 203.3211 \r\nL 202.83482 183.75372 \r\nL 203.139489 197.257692 \r\nL 203.444157 199.419862 \r\nL 203.748825 191.039801 \r\nL 204.053494 199.188932 \r\nL 204.358162 188.963035 \r\nL 204.66283 198.884795 \r\nL 205.272167 206.851326 \r\nL 205.576835 196.929906 \r\nL 205.881503 203.14623 \r\nL 206.186172 191.870635 \r\nL 206.49084 199.476211 \r\nL 206.795508 179.125386 \r\nL 207.100177 190.57051 \r\nL 207.404845 191.672103 \r\nL 207.709513 204.762833 \r\nL 208.014182 200.915777 \r\nL 208.31885 180.402803 \r\nL 208.623518 180.206243 \r\nL 209.232855 191.992564 \r\nL 209.537523 196.974958 \r\nL 209.842191 195.444608 \r\nL 210.14686 182.254671 \r\nL 210.451528 190.084086 \r\nL 210.756196 188.981578 \r\nL 211.060865 198.667236 \r\nL 211.365533 188.195914 \r\nL 211.670201 202.399207 \r\nL 211.974869 193.719056 \r\nL 212.279538 199.347177 \r\nL 212.584206 195.836683 \r\nL 212.888874 202.020726 \r\nL 213.193543 205.18732 \r\nL 213.498211 192.367989 \r\nL 213.802879 191.344382 \r\nL 214.107548 189.30102 \r\nL 214.412216 191.727211 \r\nL 214.716884 196.854257 \r\nL 215.021553 198.73889 \r\nL 215.326221 197.440554 \r\nL 215.630889 206.07099 \r\nL 215.935557 183.746564 \r\nL 216.240226 199.324703 \r\nL 216.544894 195.799661 \r\nL 216.849562 189.552205 \r\nL 217.154231 185.84677 \r\nL 217.458899 193.946226 \r\nL 217.763567 195.537704 \r\nL 218.068236 198.769329 \r\nL 218.372904 186.111261 \r\nL 218.677572 204.073921 \r\nL 218.98224 194.147369 \r\nL 219.286909 199.759976 \r\nL 219.591577 191.130768 \r\nL 219.896245 201.300473 \r\nL 220.200914 197.457608 \r\nL 220.505582 203.952292 \r\nL 221.114919 201.78785 \r\nL 221.419587 200.214184 \r\nL 221.724255 189.198353 \r\nL 222.028924 190.89417 \r\nL 222.333592 202.273034 \r\nL 222.942928 193.844942 \r\nL 223.247597 202.309546 \r\nL 223.552265 199.713487 \r\nL 224.161602 186.105802 \r\nL 224.46627 188.865408 \r\nL 224.770938 197.029178 \r\nL 225.075607 189.184811 \r\nL 225.380275 190.937851 \r\nL 225.684943 184.377051 \r\nL 225.989611 196.564405 \r\nL 226.29428 185.266088 \r\nL 226.598948 199.473338 \r\nL 226.903616 197.64726 \r\nL 227.208285 183.845235 \r\nL 227.817621 195.229479 \r\nL 228.12229 181.420703 \r\nL 228.426958 190.261841 \r\nL 228.731626 192.25635 \r\nL 229.036295 197.870995 \r\nL 229.340963 195.467631 \r\nL 229.645631 197.402487 \r\nL 229.950299 193.837721 \r\nL 230.254968 205.411499 \r\nL 230.559636 207.473025 \r\nL 230.864304 191.135221 \r\nL 231.168973 198.300039 \r\nL 231.473641 186.129399 \r\nL 231.778309 196.244428 \r\nL 232.082978 182.26329 \r\nL 232.387646 192.959705 \r\nL 232.692314 198.572966 \r\nL 232.996982 201.792591 \r\nL 233.301651 200.535846 \r\nL 233.606319 201.530737 \r\nL 233.910987 183.796605 \r\nL 234.215656 197.657485 \r\nL 234.520324 195.123194 \r\nL 234.824992 178.225015 \r\nL 235.129661 208.617386 \r\nL 235.434329 192.393232 \r\nL 236.043666 198.657821 \r\nL 236.348334 196.804921 \r\nL 236.653002 192.082878 \r\nL 236.95767 191.766087 \r\nL 237.262339 196.814323 \r\nL 237.567007 190.098477 \r\nL 237.871675 199.271267 \r\nL 238.176344 194.074684 \r\nL 238.481012 200.029704 \r\nL 238.78568 193.73995 \r\nL 239.090349 202.240478 \r\nL 239.395017 204.477161 \r\nL 239.699685 198.328559 \r\nL 240.004354 199.992983 \r\nL 240.309022 182.901247 \r\nL 240.918358 205.586081 \r\nL 241.223027 185.629159 \r\nL 241.527695 197.934551 \r\nL 241.832363 197.349351 \r\nL 242.137032 197.036386 \r\nL 242.4417 189.423994 \r\nL 243.051037 206.90155 \r\nL 243.355705 185.667565 \r\nL 243.660373 200.539281 \r\nL 244.26971 188.471949 \r\nL 244.879046 197.357356 \r\nL 245.183715 182.893686 \r\nL 245.793051 207.273396 \r\nL 246.402388 183.405796 \r\nL 246.707056 190.345756 \r\nL 247.011725 201.547648 \r\nL 247.621061 187.782423 \r\nL 247.925729 202.367187 \r\nL 248.230398 204.235483 \r\nL 248.535066 185.680611 \r\nL 248.839734 197.588248 \r\nL 249.144403 183.236099 \r\nL 249.449071 196.109727 \r\nL 249.753739 190.78735 \r\nL 250.058408 196.497976 \r\nL 250.363076 190.703722 \r\nL 250.667744 206.138294 \r\nL 250.972412 194.726366 \r\nL 251.277081 193.40028 \r\nL 251.581749 206.914295 \r\nL 251.886417 206.020087 \r\nL 252.191086 195.050065 \r\nL 252.495754 188.212982 \r\nL 252.800422 187.297175 \r\nL 253.105091 198.040732 \r\nL 253.409759 198.420597 \r\nL 254.019096 186.32592 \r\nL 254.323764 186.352886 \r\nL 254.628432 198.890919 \r\nL 254.9331 197.469191 \r\nL 255.237769 202.179324 \r\nL 255.542437 200.574161 \r\nL 255.847105 198.328232 \r\nL 256.151774 190.904813 \r\nL 256.456442 194.307207 \r\nL 256.76111 205.658713 \r\nL 257.065779 198.217795 \r\nL 257.370447 203.760499 \r\nL 257.675115 190.3646 \r\nL 257.979783 197.821359 \r\nL 258.284452 197.318559 \r\nL 258.58912 198.326678 \r\nL 258.893788 192.280313 \r\nL 259.503125 198.496624 \r\nL 260.112462 193.578623 \r\nL 260.721798 205.561804 \r\nL 261.026467 197.873659 \r\nL 261.331135 185.906996 \r\nL 261.635803 189.626052 \r\nL 261.940471 197.357827 \r\nL 262.24514 188.678002 \r\nL 262.549808 195.484307 \r\nL 262.854476 192.925727 \r\nL 263.159145 182.041488 \r\nL 263.768481 192.486079 \r\nL 264.07315 193.777729 \r\nL 264.377818 203.848149 \r\nL 264.682486 186.861732 \r\nL 264.987154 189.642179 \r\nL 265.291823 189.874259 \r\nL 265.596491 193.35925 \r\nL 265.901159 200.846683 \r\nL 266.205828 199.703053 \r\nL 266.510496 188.323733 \r\nL 266.815164 182.534323 \r\nL 267.119833 206.182263 \r\nL 267.424501 206.05797 \r\nL 267.729169 197.358519 \r\nL 268.033838 176.33999 \r\nL 268.338506 198.307952 \r\nL 268.643174 195.944038 \r\nL 268.947842 188.733358 \r\nL 269.252511 204.164091 \r\nL 269.557179 197.508198 \r\nL 269.861847 202.688392 \r\nL 270.166516 191.913102 \r\nL 270.471184 196.417743 \r\nL 270.775852 198.806168 \r\nL 271.080521 190.060959 \r\nL 271.385189 189.007852 \r\nL 271.689857 201.884707 \r\nL 271.994525 189.059447 \r\nL 272.299194 193.535516 \r\nL 272.603862 193.073486 \r\nL 272.90853 190.388223 \r\nL 273.213199 204.078139 \r\nL 273.517867 190.079045 \r\nL 273.822535 204.720327 \r\nL 274.127204 190.025949 \r\nL 274.431872 196.706223 \r\nL 274.73654 193.89369 \r\nL 275.041209 201.260148 \r\nL 275.345877 192.380721 \r\nL 275.650545 189.274955 \r\nL 275.955213 189.103024 \r\nL 276.259882 208.618666 \r\nL 276.56455 209.481468 \r\nL 276.869218 193.664823 \r\nL 277.173887 187.415996 \r\nL 277.478555 188.118894 \r\nL 277.783223 198.675881 \r\nL 278.087892 200.104648 \r\nL 278.39256 182.632603 \r\nL 278.697228 199.342764 \r\nL 279.001896 193.166581 \r\nL 279.306565 196.787265 \r\nL 279.611233 208.55648 \r\nL 279.915901 193.600823 \r\nL 280.22057 197.400542 \r\nL 280.525238 196.498929 \r\nL 280.829906 185.385001 \r\nL 281.134575 192.019622 \r\nL 281.439243 195.737215 \r\nL 281.743911 201.386439 \r\nL 282.04858 197.880501 \r\nL 282.353248 185.133294 \r\nL 282.657916 188.657944 \r\nL 282.962584 199.730124 \r\nL 283.267253 193.323756 \r\nL 283.571921 192.09834 \r\nL 283.876589 193.353386 \r\nL 284.181258 189.945181 \r\nL 284.485926 203.174646 \r\nL 284.790594 187.968862 \r\nL 285.095263 198.029632 \r\nL 285.399931 197.881376 \r\nL 285.704599 203.634012 \r\nL 286.009268 187.705795 \r\nL 286.313936 193.836258 \r\nL 286.618604 207.392505 \r\nL 286.923272 196.59081 \r\nL 287.227941 195.744149 \r\nL 287.532609 186.890644 \r\nL 287.837277 181.499656 \r\nL 288.141946 209.02692 \r\nL 288.446614 197.581261 \r\nL 288.751282 198.489429 \r\nL 289.055951 196.761213 \r\nL 289.360619 187.509326 \r\nL 289.665287 188.405963 \r\nL 290.274624 200.398508 \r\nL 290.579292 192.873792 \r\nL 290.88396 188.920111 \r\nL 291.188629 195.42579 \r\nL 291.493297 205.802725 \r\nL 291.797965 192.098875 \r\nL 292.102634 204.042606 \r\nL 292.407302 181.439547 \r\nL 292.71197 212.238448 \r\nL 293.016639 193.532826 \r\nL 293.321307 197.243942 \r\nL 293.625975 191.36286 \r\nL 293.930643 195.991076 \r\nL 294.235312 209.173021 \r\nL 294.53998 198.600415 \r\nL 294.844648 198.709155 \r\nL 295.149317 195.803736 \r\nL 295.453985 194.198089 \r\nL 295.758653 200.185912 \r\nL 296.063322 191.210648 \r\nL 296.672658 202.382492 \r\nL 296.977326 192.247157 \r\nL 297.281995 200.9952 \r\nL 297.586663 184.533677 \r\nL 297.891331 195.478796 \r\nL 298.196 194.026106 \r\nL 298.500668 186.159069 \r\nL 298.805336 194.678963 \r\nL 299.110005 192.286633 \r\nL 299.414673 185.136363 \r\nL 299.719341 201.224915 \r\nL 300.02401 202.419644 \r\nL 300.328678 192.156725 \r\nL 300.633346 185.852072 \r\nL 300.938014 190.733848 \r\nL 301.242683 198.702169 \r\nL 301.547351 188.566259 \r\nL 301.852019 197.442173 \r\nL 302.156688 186.727201 \r\nL 302.766024 196.418527 \r\nL 303.070693 195.841919 \r\nL 303.375361 192.430697 \r\nL 303.680029 210.473002 \r\nL 303.984697 193.717633 \r\nL 304.289366 184.281683 \r\nL 304.594034 201.0218 \r\nL 304.898702 198.690063 \r\nL 305.203371 193.534785 \r\nL 305.508039 203.80687 \r\nL 305.812707 195.780713 \r\nL 306.117376 200.597131 \r\nL 306.422044 190.805723 \r\nL 306.726712 196.654563 \r\nL 307.031381 204.85286 \r\nL 307.336049 194.51804 \r\nL 307.640717 194.053764 \r\nL 307.945385 198.397666 \r\nL 308.250054 204.630836 \r\nL 308.554722 203.39163 \r\nL 308.85939 189.230268 \r\nL 309.164059 200.171091 \r\nL 309.468727 195.10324 \r\nL 309.773395 201.058064 \r\nL 310.078064 199.564318 \r\nL 310.382732 194.980084 \r\nL 310.6874 203.939351 \r\nL 310.992068 194.02497 \r\nL 311.601405 203.838172 \r\nL 311.906073 195.236609 \r\nL 312.210742 190.843711 \r\nL 312.51541 196.309695 \r\nL 312.820078 197.474363 \r\nL 313.124747 196.956402 \r\nL 313.429415 196.947848 \r\nL 313.734083 193.723901 \r\nL 314.038752 193.084272 \r\nL 314.34342 187.363395 \r\nL 314.648088 184.985365 \r\nL 314.952756 191.701668 \r\nL 315.257425 192.692616 \r\nL 315.562093 192.931707 \r\nL 315.866761 207.971254 \r\nL 316.17143 210.602662 \r\nL 316.476098 198.789218 \r\nL 316.780766 181.131622 \r\nL 317.390103 205.507663 \r\nL 317.694771 197.252182 \r\nL 317.999439 204.684076 \r\nL 318.304108 197.692365 \r\nL 318.608776 199.710196 \r\nL 318.913444 196.768526 \r\nL 319.218113 185.646671 \r\nL 319.522781 201.897622 \r\nL 320.132118 187.374417 \r\nL 320.436786 192.410378 \r\nL 320.741454 191.453945 \r\nL 321.046123 194.894014 \r\nL 321.350791 206.802813 \r\nL 321.655459 185.741517 \r\nL 321.960127 198.471552 \r\nL 322.264796 187.804166 \r\nL 322.569464 204.369074 \r\nL 322.874132 200.95887 \r\nL 323.178801 191.770932 \r\nL 323.483469 204.773032 \r\nL 323.788137 197.918633 \r\nL 324.092806 200.823177 \r\nL 324.397474 191.903974 \r\nL 324.702142 186.471159 \r\nL 325.006811 187.878718 \r\nL 325.311479 188.394015 \r\nL 325.616147 185.316142 \r\nL 325.920815 187.520909 \r\nL 326.225484 190.990374 \r\nL 326.530152 204.518283 \r\nL 326.83482 209.629331 \r\nL 327.139489 196.305477 \r\nL 327.444157 204.945551 \r\nL 327.748825 199.703876 \r\nL 328.053494 199.874096 \r\nL 328.358162 197.428187 \r\nL 328.66283 202.094691 \r\nL 328.967498 187.728034 \r\nL 329.272167 198.653747 \r\nL 329.576835 203.13513 \r\nL 329.881503 197.108758 \r\nL 330.186172 205.186837 \r\nL 330.49084 202.62443 \r\nL 330.795508 210.141115 \r\nL 331.100177 169.2965 \r\nL 331.404845 198.173069 \r\nL 331.709513 198.44913 \r\nL 332.014182 189.364734 \r\nL 332.31885 200.686204 \r\nL 332.623518 204.651508 \r\nL 333.232855 178.251667 \r\nL 334.14686 203.12309 \r\nL 334.451528 203.858152 \r\nL 334.756196 195.242342 \r\nL 335.060865 198.503768 \r\nL 335.365533 198.350667 \r\nL 335.670201 203.608953 \r\nL 335.974869 184.046235 \r\nL 336.279538 196.766972 \r\nL 336.584206 204.731244 \r\nL 337.193543 187.857707 \r\nL 337.498211 182.98716 \r\nL 337.802879 181.965995 \r\nL 338.107548 195.927428 \r\nL 338.412216 189.89497 \r\nL 339.021553 200.609746 \r\nL 339.326221 192.176901 \r\nL 339.935557 208.228288 \r\nL 340.240226 199.010393 \r\nL 340.544894 201.306859 \r\nL 341.154231 191.203805 \r\nL 341.458899 194.331588 \r\nL 341.763567 185.768744 \r\nL 342.068236 186.908221 \r\nL 342.372904 193.500976 \r\nL 342.677572 190.984628 \r\nL 342.98224 179.405417 \r\nL 343.286909 208.031363 \r\nL 343.591577 194.921568 \r\nL 343.896245 195.056595 \r\nL 344.200914 214.756364 \r\nL 344.505582 196.425056 \r\nL 344.81025 193.364225 \r\nL 345.114919 206.609465 \r\nL 345.419587 194.63233 \r\nL 345.724255 198.161577 \r\nL 346.028924 189.616871 \r\nL 346.333592 206.263958 \r\nL 346.63826 197.315216 \r\nL 346.942928 195.783168 \r\nL 347.247597 207.737059 \r\nL 347.552265 184.500795 \r\nL 347.856933 203.77869 \r\nL 348.161602 188.20751 \r\nL 348.46627 204.396785 \r\nL 348.770938 192.92348 \r\nL 349.075607 193.351676 \r\nL 349.380275 189.480721 \r\nL 349.684943 202.528371 \r\nL 349.684943 202.528371 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_13\">\r\n    <path clip-path=\"url(#pb825eedddf)\" d=\"M 45.321307 147.044929 \r\nL 45.625975 146.614148 \r\nL 46.235312 147.518628 \r\nL 46.53998 148.534814 \r\nL 46.844648 148.553685 \r\nL 47.149317 149.449495 \r\nL 47.453985 149.599991 \r\nL 48.063322 150.616411 \r\nL 48.672658 151.519028 \r\nL 49.281995 152.705586 \r\nL 49.586663 152.856673 \r\nL 50.196 153.864268 \r\nL 50.500668 154.314432 \r\nL 50.805336 154.510896 \r\nL 52.328678 156.470622 \r\nL 52.938014 157.052465 \r\nL 53.547351 157.741268 \r\nL 54.461356 158.72232 \r\nL 54.766024 158.857984 \r\nL 55.375361 159.449246 \r\nL 55.680029 159.483023 \r\nL 55.984697 160.061041 \r\nL 56.289366 160.167534 \r\nL 56.594034 160.579397 \r\nL 56.898702 160.747118 \r\nL 58.422044 162.244069 \r\nL 59.945385 163.542065 \r\nL 60.250054 163.631394 \r\nL 60.554722 164.115169 \r\nL 61.164059 164.45937 \r\nL 61.468727 164.286933 \r\nL 61.773395 165.112059 \r\nL 63.296737 166.319067 \r\nL 63.601405 166.483388 \r\nL 64.820078 167.579974 \r\nL 65.124747 167.724975 \r\nL 65.429415 168.273706 \r\nL 65.734083 168.267378 \r\nL 66.038752 168.712444 \r\nL 66.648088 169.017693 \r\nL 66.952756 169.177653 \r\nL 67.257425 169.513647 \r\nL 67.562093 169.645379 \r\nL 68.17143 170.066504 \r\nL 68.476098 170.687974 \r\nL 68.780766 170.96368 \r\nL 69.085435 170.826874 \r\nL 69.390103 171.276108 \r\nL 69.999439 171.678719 \r\nL 70.913444 172.544248 \r\nL 71.218113 172.269609 \r\nL 71.522781 172.816583 \r\nL 71.827449 172.996673 \r\nL 72.741454 173.951455 \r\nL 73.046123 173.847258 \r\nL 73.350791 174.291126 \r\nL 73.960127 174.600521 \r\nL 74.264796 175.289244 \r\nL 74.569464 175.135199 \r\nL 75.178801 175.715044 \r\nL 75.483469 176.185626 \r\nL 75.788137 176.172644 \r\nL 76.092806 176.513484 \r\nL 76.397474 176.62336 \r\nL 77.006811 177.099147 \r\nL 77.311479 177.014308 \r\nL 77.616147 177.49802 \r\nL 77.920815 177.504627 \r\nL 78.530152 178.178975 \r\nL 79.139489 178.178044 \r\nL 79.444157 178.684666 \r\nL 80.053494 178.745209 \r\nL 80.358162 179.127159 \r\nL 80.66283 179.246338 \r\nL 80.967498 179.83228 \r\nL 81.272167 179.820833 \r\nL 81.881503 180.265062 \r\nL 82.186172 180.657983 \r\nL 83.100177 180.759983 \r\nL 83.404845 180.935115 \r\nL 84.014182 181.781378 \r\nL 84.623518 182.119899 \r\nL 84.928186 182.109497 \r\nL 86.14686 183.21592 \r\nL 86.451528 183.214082 \r\nL 86.756196 183.644692 \r\nL 87.060865 183.874553 \r\nL 87.365533 183.840927 \r\nL 87.670201 184.149482 \r\nL 87.974869 184.137869 \r\nL 88.279538 184.540044 \r\nL 88.584206 184.322832 \r\nL 88.888874 185.049869 \r\nL 89.193543 184.663891 \r\nL 89.498211 185.065415 \r\nL 89.802879 185.176139 \r\nL 90.107548 185.463022 \r\nL 90.412216 185.339783 \r\nL 90.716884 186.160358 \r\nL 91.021553 185.815483 \r\nL 91.326221 185.82408 \r\nL 91.630889 186.365829 \r\nL 92.849562 187.159419 \r\nL 93.154231 187.456514 \r\nL 93.458899 187.548913 \r\nL 93.763567 187.841346 \r\nL 94.068236 187.748577 \r\nL 94.372904 187.951202 \r\nL 94.677572 187.988943 \r\nL 95.591577 188.616411 \r\nL 96.200914 188.711837 \r\nL 96.505582 189.314056 \r\nL 96.81025 188.997017 \r\nL 97.114919 189.310962 \r\nL 97.419587 189.938402 \r\nL 97.724255 189.615149 \r\nL 98.028924 189.978454 \r\nL 98.63826 190.187576 \r\nL 98.942928 190.605145 \r\nL 99.247597 190.489055 \r\nL 99.856933 190.578324 \r\nL 100.161602 190.983025 \r\nL 100.770938 191.040218 \r\nL 101.380275 191.854444 \r\nL 101.684943 191.636765 \r\nL 102.29428 192.050623 \r\nL 102.598948 192.256964 \r\nL 102.903616 191.950517 \r\nL 104.12229 193.116707 \r\nL 104.426958 193.057866 \r\nL 105.340963 193.474101 \r\nL 105.645631 193.398551 \r\nL 105.950299 194.035263 \r\nL 106.254968 194.163952 \r\nL 106.559636 193.789664 \r\nL 106.864304 194.282755 \r\nL 107.168973 194.482624 \r\nL 107.473641 194.176412 \r\nL 108.082978 194.927707 \r\nL 108.387646 194.654576 \r\nL 108.692314 194.81934 \r\nL 108.996982 194.735115 \r\nL 109.606319 195.162098 \r\nL 109.910987 195.082461 \r\nL 110.520324 195.582835 \r\nL 110.824992 196.07848 \r\nL 111.129661 195.865809 \r\nL 111.434329 196.131346 \r\nL 111.738997 196.086756 \r\nL 112.043666 196.526599 \r\nL 112.348334 196.269347 \r\nL 112.653002 196.677021 \r\nL 112.95767 196.548838 \r\nL 113.567007 196.899544 \r\nL 113.871675 196.908207 \r\nL 114.176344 197.209337 \r\nL 114.481012 197.016886 \r\nL 115.090349 197.167236 \r\nL 115.699685 197.558208 \r\nL 116.004354 197.808264 \r\nL 116.309022 197.859335 \r\nL 116.61369 197.738513 \r\nL 116.918358 198.036875 \r\nL 117.223027 198.013951 \r\nL 117.832363 198.296272 \r\nL 118.137032 198.16746 \r\nL 118.4417 198.518855 \r\nL 118.746368 198.348183 \r\nL 119.660373 198.685967 \r\nL 120.26971 199.0461 \r\nL 120.574378 198.952285 \r\nL 120.879046 199.290292 \r\nL 121.183715 199.361812 \r\nL 121.488383 199.144996 \r\nL 122.09772 199.086168 \r\nL 122.402388 199.578468 \r\nL 122.707056 199.154365 \r\nL 123.011725 199.497098 \r\nL 123.316393 199.535315 \r\nL 123.925729 199.802231 \r\nL 124.230398 199.777605 \r\nL 125.144403 199.98 \r\nL 125.449071 199.988682 \r\nL 125.753739 200.313271 \r\nL 126.058408 200.042435 \r\nL 126.667744 200.445119 \r\nL 126.972412 200.379458 \r\nL 127.277081 200.11379 \r\nL 127.581749 200.421934 \r\nL 127.886417 200.396182 \r\nL 128.191086 200.602376 \r\nL 129.105091 200.567982 \r\nL 129.409759 200.358368 \r\nL 129.714427 200.774938 \r\nL 130.323764 200.667208 \r\nL 130.9331 200.838037 \r\nL 131.237769 200.715425 \r\nL 131.542437 201.205807 \r\nL 131.847105 201.106017 \r\nL 132.151774 200.87837 \r\nL 132.456442 200.906291 \r\nL 132.76111 201.158841 \r\nL 133.065779 201.2278 \r\nL 133.370447 201.168142 \r\nL 133.675115 201.230596 \r\nL 133.979783 200.813326 \r\nL 134.284452 201.041448 \r\nL 134.58912 200.928567 \r\nL 134.893788 201.201906 \r\nL 135.198457 201.309784 \r\nL 136.112462 201.038243 \r\nL 136.721798 201.38778 \r\nL 137.026467 201.17146 \r\nL 137.635803 201.251624 \r\nL 137.940471 200.965685 \r\nL 138.24514 201.042269 \r\nL 138.549808 201.234432 \r\nL 138.854476 200.966299 \r\nL 139.463813 201.355162 \r\nL 139.768481 200.84491 \r\nL 140.07315 201.16356 \r\nL 140.377818 201.022519 \r\nL 140.682486 201.127941 \r\nL 140.987154 200.888878 \r\nL 141.596491 201.002837 \r\nL 141.901159 201.121386 \r\nL 142.205828 200.955996 \r\nL 142.510496 201.088229 \r\nL 142.815164 200.759023 \r\nL 143.424501 200.937392 \r\nL 143.729169 200.978404 \r\nL 144.338506 200.599886 \r\nL 144.643174 200.5739 \r\nL 144.947842 200.804065 \r\nL 145.557179 200.517642 \r\nL 145.861847 200.709669 \r\nL 146.775852 200.345195 \r\nL 147.080521 200.366896 \r\nL 147.689857 200.113625 \r\nL 147.994525 200.229065 \r\nL 148.603862 200.299895 \r\nL 148.90853 200.347915 \r\nL 149.213199 200.010967 \r\nL 149.517867 200.035289 \r\nL 149.822535 199.794338 \r\nL 150.127204 199.735463 \r\nL 150.431872 200.028163 \r\nL 150.73654 199.673 \r\nL 151.041209 199.584006 \r\nL 151.345877 199.692194 \r\nL 151.650545 199.612115 \r\nL 151.955213 199.249326 \r\nL 152.259882 199.606316 \r\nL 152.56455 199.547439 \r\nL 152.869218 199.145432 \r\nL 153.173887 199.004575 \r\nL 153.478555 199.223203 \r\nL 154.087892 198.806099 \r\nL 154.39256 198.915933 \r\nL 154.697228 198.555904 \r\nL 155.001896 198.695207 \r\nL 155.306565 198.395999 \r\nL 155.611233 198.560239 \r\nL 155.915901 198.247971 \r\nL 156.22057 198.384892 \r\nL 156.525238 198.063064 \r\nL 156.829906 198.074697 \r\nL 157.134575 197.891204 \r\nL 157.439243 197.524876 \r\nL 157.743911 197.912644 \r\nL 158.353248 197.460373 \r\nL 158.962584 197.44759 \r\nL 159.876589 196.751633 \r\nL 160.181258 196.937224 \r\nL 160.485926 196.676036 \r\nL 161.095263 196.716582 \r\nL 161.399931 196.416359 \r\nL 161.704599 196.352465 \r\nL 162.009268 196.020496 \r\nL 162.313936 195.859761 \r\nL 162.618604 195.984789 \r\nL 162.923272 195.91112 \r\nL 163.532609 195.633378 \r\nL 164.141946 195.397355 \r\nL 164.446614 195.059401 \r\nL 164.751282 194.92696 \r\nL 165.055951 194.934803 \r\nL 166.274624 194.543967 \r\nL 166.88396 193.954725 \r\nL 167.188629 193.696241 \r\nL 167.797965 193.573477 \r\nL 168.407302 193.108371 \r\nL 168.71197 193.185309 \r\nL 169.016639 192.823082 \r\nL 169.321307 192.952261 \r\nL 169.625975 192.310609 \r\nL 170.235312 192.614897 \r\nL 170.53998 192.394782 \r\nL 171.149317 191.5902 \r\nL 171.453985 191.725721 \r\nL 172.063322 191.746864 \r\nL 172.36799 191.008471 \r\nL 172.672658 191.235302 \r\nL 172.977326 190.979174 \r\nL 173.281995 190.416049 \r\nL 173.586663 190.69265 \r\nL 173.891331 190.621686 \r\nL 174.500668 189.760537 \r\nL 174.805336 189.88997 \r\nL 175.110005 189.416878 \r\nL 175.414673 189.463484 \r\nL 175.719341 189.073988 \r\nL 176.02401 189.275796 \r\nL 176.328678 188.638398 \r\nL 176.938014 188.865201 \r\nL 177.242683 188.308037 \r\nL 177.547351 188.184445 \r\nL 177.852019 187.639927 \r\nL 178.156688 187.843928 \r\nL 178.461356 187.612346 \r\nL 178.766024 187.211808 \r\nL 179.070693 187.137753 \r\nL 179.375361 186.846507 \r\nL 179.680029 186.728737 \r\nL 179.984697 186.387354 \r\nL 180.289366 186.401136 \r\nL 180.898702 185.765683 \r\nL 181.203371 185.824013 \r\nL 181.508039 185.160221 \r\nL 181.812707 184.902739 \r\nL 182.117376 185.263472 \r\nL 182.726712 184.49844 \r\nL 183.031381 184.331849 \r\nL 183.336049 184.446868 \r\nL 183.640717 183.539451 \r\nL 183.945385 184.114044 \r\nL 184.250054 183.296916 \r\nL 184.85939 182.996535 \r\nL 185.164059 182.380413 \r\nL 185.468727 182.512868 \r\nL 186.078064 182.05606 \r\nL 186.382732 182.086923 \r\nL 186.992068 181.343139 \r\nL 187.296737 181.488887 \r\nL 187.601405 181.086483 \r\nL 187.906073 180.303175 \r\nL 188.210742 180.350283 \r\nL 188.51541 180.187144 \r\nL 188.820078 180.660484 \r\nL 189.429415 179.430816 \r\nL 189.734083 179.319496 \r\nL 190.648088 178.339025 \r\nL 190.952756 179.087537 \r\nL 191.257425 178.027244 \r\nL 191.562093 178.037471 \r\nL 191.866761 177.168568 \r\nL 192.17143 177.097262 \r\nL 192.476098 176.52149 \r\nL 192.780766 176.816166 \r\nL 193.085435 176.635656 \r\nL 193.694771 175.543443 \r\nL 193.999439 175.52363 \r\nL 194.304108 175.714757 \r\nL 194.913444 174.594294 \r\nL 195.218113 174.303592 \r\nL 195.522781 174.363363 \r\nL 195.827449 173.939212 \r\nL 196.132118 173.803813 \r\nL 196.436786 172.924708 \r\nL 196.741454 173.257662 \r\nL 197.046123 172.970978 \r\nL 197.350791 173.023151 \r\nL 197.655459 172.33024 \r\nL 198.264796 172.192152 \r\nL 198.569464 171.156259 \r\nL 199.483469 171.13503 \r\nL 200.092806 169.827353 \r\nL 200.397474 170.496656 \r\nL 200.702142 169.448103 \r\nL 201.006811 169.341699 \r\nL 201.616147 168.671714 \r\nL 201.920815 168.700479 \r\nL 202.530152 167.387696 \r\nL 202.83482 167.603321 \r\nL 203.444157 166.437962 \r\nL 204.053494 166.758551 \r\nL 204.66283 165.445584 \r\nL 204.967498 165.483619 \r\nL 205.272167 164.781394 \r\nL 205.576835 164.555151 \r\nL 205.881503 164.586745 \r\nL 206.186172 163.820535 \r\nL 206.49084 164.326845 \r\nL 206.795508 163.049913 \r\nL 207.100177 163.710399 \r\nL 207.404845 162.774093 \r\nL 207.709513 162.257299 \r\nL 208.014182 162.53623 \r\nL 208.31885 161.76428 \r\nL 208.928186 161.727668 \r\nL 209.232855 161.218534 \r\nL 210.451528 160.069595 \r\nL 210.756196 159.532715 \r\nL 211.060865 159.362122 \r\nL 211.365533 159.511408 \r\nL 211.670201 159.029792 \r\nL 211.974869 157.971322 \r\nL 212.279538 157.355879 \r\nL 212.888874 156.771588 \r\nL 213.193543 156.688831 \r\nL 213.498211 156.883532 \r\nL 213.802879 156.61771 \r\nL 214.107548 155.737938 \r\nL 214.412216 155.819229 \r\nL 214.716884 155.487697 \r\nL 215.021553 154.778644 \r\nL 215.326221 154.94467 \r\nL 215.935557 153.996747 \r\nL 216.240226 154.132734 \r\nL 216.849562 152.618306 \r\nL 217.154231 152.797999 \r\nL 217.458899 152.091774 \r\nL 217.763567 152.314499 \r\nL 218.068236 151.184152 \r\nL 218.372904 150.855298 \r\nL 218.677572 151.612528 \r\nL 219.286909 149.93785 \r\nL 219.896245 150.246677 \r\nL 220.81025 148.989405 \r\nL 221.114919 148.940169 \r\nL 221.419587 148.028805 \r\nL 221.724255 148.169496 \r\nL 222.028924 147.447317 \r\nL 222.333592 147.621325 \r\nL 222.63826 146.316519 \r\nL 223.247597 146.838356 \r\nL 223.552265 146.555312 \r\nL 223.856933 145.658691 \r\nL 224.161602 145.877689 \r\nL 224.46627 144.269289 \r\nL 224.770938 144.697942 \r\nL 225.075607 143.718533 \r\nL 225.380275 144.777042 \r\nL 225.989611 141.853535 \r\nL 226.29428 142.592688 \r\nL 226.598948 142.461042 \r\nL 226.903616 142.151726 \r\nL 227.817621 140.727225 \r\nL 228.731626 140.587619 \r\nL 229.036295 139.335733 \r\nL 229.340963 139.498962 \r\nL 229.950299 138.608382 \r\nL 230.254968 137.164555 \r\nL 230.559636 137.735313 \r\nL 230.864304 137.228855 \r\nL 231.168973 137.37103 \r\nL 231.473641 136.575625 \r\nL 231.778309 137.101387 \r\nL 232.082978 135.560149 \r\nL 232.387646 136.011395 \r\nL 232.692314 135.532883 \r\nL 232.996982 135.865676 \r\nL 233.301651 134.9592 \r\nL 233.606319 133.469455 \r\nL 233.910987 133.950328 \r\nL 234.215656 133.767041 \r\nL 234.520324 133.202863 \r\nL 234.824992 132.999272 \r\nL 235.129661 133.019447 \r\nL 235.434329 132.61821 \r\nL 235.738997 132.705242 \r\nL 236.043666 131.021183 \r\nL 236.348334 131.565009 \r\nL 236.95767 130.252592 \r\nL 237.262339 129.866225 \r\nL 237.567007 129.118637 \r\nL 238.176344 128.650856 \r\nL 238.481012 128.288989 \r\nL 238.78568 127.740093 \r\nL 239.090349 127.759922 \r\nL 239.395017 127.367718 \r\nL 239.699685 126.430428 \r\nL 240.004354 126.993657 \r\nL 240.309022 127.061426 \r\nL 240.918358 125.662928 \r\nL 241.223027 125.14724 \r\nL 241.527695 124.868836 \r\nL 241.832363 125.320634 \r\nL 242.137032 124.843572 \r\nL 242.4417 124.14049 \r\nL 242.746368 124.279206 \r\nL 243.355705 122.936813 \r\nL 243.660373 122.48513 \r\nL 243.965041 122.989967 \r\nL 244.26971 121.9033 \r\nL 244.574378 122.148781 \r\nL 244.879046 121.043266 \r\nL 245.183715 121.162337 \r\nL 245.488383 121.551173 \r\nL 245.793051 119.869679 \r\nL 246.09772 119.656049 \r\nL 246.402388 118.570155 \r\nL 246.707056 119.432402 \r\nL 247.011725 118.012732 \r\nL 247.316393 118.867009 \r\nL 247.621061 118.448277 \r\nL 247.925729 117.73931 \r\nL 248.230398 117.732017 \r\nL 248.535066 115.863427 \r\nL 248.839734 115.587796 \r\nL 249.144403 116.812941 \r\nL 249.449071 116.772553 \r\nL 250.058408 115.023287 \r\nL 250.363076 113.97659 \r\nL 250.667744 114.733973 \r\nL 250.972412 115.175614 \r\nL 251.277081 112.653198 \r\nL 251.581749 112.792138 \r\nL 251.886417 112.516891 \r\nL 252.495754 112.566584 \r\nL 252.800422 111.884097 \r\nL 253.105091 110.612518 \r\nL 253.409759 111.048511 \r\nL 253.714427 110.518567 \r\nL 254.019096 110.436083 \r\nL 254.323764 109.338289 \r\nL 254.628432 109.601846 \r\nL 255.237769 108.112271 \r\nL 255.542437 108.45488 \r\nL 255.847105 108.38439 \r\nL 256.151774 107.582372 \r\nL 256.456442 107.351046 \r\nL 256.76111 106.381779 \r\nL 257.065779 106.689633 \r\nL 257.370447 106.079157 \r\nL 257.675115 106.222222 \r\nL 257.979783 105.740647 \r\nL 258.284452 106.652488 \r\nL 258.58912 104.249676 \r\nL 258.893788 105.449293 \r\nL 259.198457 104.24172 \r\nL 259.503125 104.018562 \r\nL 260.112462 103.005763 \r\nL 260.41713 104.477782 \r\nL 260.721798 103.881018 \r\nL 261.331135 101.577055 \r\nL 261.635803 101.868151 \r\nL 261.940471 101.590739 \r\nL 262.24514 101.995117 \r\nL 262.549808 99.490182 \r\nL 262.854476 99.330167 \r\nL 263.159145 99.586342 \r\nL 263.463813 100.086836 \r\nL 263.768481 98.797047 \r\nL 264.07315 99.925529 \r\nL 264.377818 98.759107 \r\nL 264.987154 98.129711 \r\nL 265.596491 96.528797 \r\nL 265.901159 96.883569 \r\nL 266.205828 96.864108 \r\nL 266.510496 96.439309 \r\nL 266.815164 96.285154 \r\nL 267.119833 95.510897 \r\nL 267.424501 95.837703 \r\nL 267.729169 95.68318 \r\nL 268.033838 94.099156 \r\nL 268.338506 94.046763 \r\nL 268.947842 92.352305 \r\nL 269.252511 93.904485 \r\nL 269.557179 91.387854 \r\nL 269.861847 93.085053 \r\nL 270.166516 92.650672 \r\nL 270.471184 91.230458 \r\nL 270.775852 90.724762 \r\nL 271.080521 89.888896 \r\nL 271.385189 90.159762 \r\nL 271.689857 89.357351 \r\nL 271.994525 89.583724 \r\nL 272.299194 89.523257 \r\nL 272.603862 87.640179 \r\nL 272.90853 88.593143 \r\nL 273.213199 88.043516 \r\nL 273.517867 88.828564 \r\nL 273.822535 86.210192 \r\nL 274.127204 87.771959 \r\nL 274.431872 86.9563 \r\nL 274.73654 86.488208 \r\nL 275.041209 85.521765 \r\nL 275.345877 85.362189 \r\nL 275.650545 85.61951 \r\nL 275.955213 87.076294 \r\nL 276.259882 85.762119 \r\nL 276.56455 84.961238 \r\nL 276.869218 84.574058 \r\nL 277.173887 83.704564 \r\nL 277.478555 84.424892 \r\nL 277.783223 82.208232 \r\nL 278.087892 82.030202 \r\nL 278.39256 82.714623 \r\nL 278.697228 82.216619 \r\nL 279.306565 79.432544 \r\nL 279.611233 82.415343 \r\nL 279.915901 80.487784 \r\nL 280.22057 80.397815 \r\nL 280.525238 82.36534 \r\nL 280.829906 80.14141 \r\nL 281.134575 80.390607 \r\nL 281.439243 78.879162 \r\nL 281.743911 79.017427 \r\nL 282.04858 78.51212 \r\nL 282.353248 78.269088 \r\nL 282.657916 77.452016 \r\nL 282.962584 78.147179 \r\nL 283.267253 76.432794 \r\nL 283.876589 77.427851 \r\nL 284.181258 76.560014 \r\nL 284.485926 77.089256 \r\nL 284.790594 74.853473 \r\nL 285.399931 75.107857 \r\nL 285.704599 74.727928 \r\nL 286.009268 74.859677 \r\nL 286.313936 76.412808 \r\nL 286.618604 74.809021 \r\nL 286.923272 72.549775 \r\nL 287.227941 71.703626 \r\nL 287.532609 72.696559 \r\nL 287.837277 74.881358 \r\nL 288.141946 72.470408 \r\nL 288.446614 71.81924 \r\nL 288.751282 71.799543 \r\nL 289.055951 70.944158 \r\nL 289.360619 70.714913 \r\nL 289.969955 70.052045 \r\nL 290.274624 71.769387 \r\nL 290.579292 69.635488 \r\nL 290.88396 68.517811 \r\nL 291.188629 69.346989 \r\nL 291.493297 69.223159 \r\nL 291.797965 70.173845 \r\nL 292.102634 67.64846 \r\nL 292.407302 66.198519 \r\nL 292.71197 67.092685 \r\nL 293.016639 68.588911 \r\nL 293.321307 65.860857 \r\nL 293.625975 66.906946 \r\nL 293.930643 66.090549 \r\nL 294.235312 66.440603 \r\nL 294.53998 64.603652 \r\nL 294.844648 66.407636 \r\nL 295.149317 65.403709 \r\nL 295.453985 65.52527 \r\nL 295.758653 62.989917 \r\nL 296.063322 64.295257 \r\nL 296.36799 63.143566 \r\nL 296.977326 62.739757 \r\nL 297.281995 64.265644 \r\nL 297.891331 61.621982 \r\nL 298.196 61.935967 \r\nL 298.500668 61.886465 \r\nL 298.805336 60.684873 \r\nL 299.110005 60.681835 \r\nL 299.414673 59.727612 \r\nL 299.719341 60.172701 \r\nL 300.02401 60.308695 \r\nL 300.328678 58.464158 \r\nL 300.633346 59.392818 \r\nL 300.938014 61.251883 \r\nL 301.852019 57.110636 \r\nL 302.156688 58.188543 \r\nL 302.461356 57.075873 \r\nL 302.766024 56.642244 \r\nL 303.070693 56.810345 \r\nL 303.375361 57.742343 \r\nL 303.680029 55.916169 \r\nL 303.984697 56.354801 \r\nL 304.289366 56.524794 \r\nL 304.594034 54.970421 \r\nL 304.898702 55.371383 \r\nL 305.203371 53.508515 \r\nL 306.422044 54.582587 \r\nL 306.726712 53.867645 \r\nL 307.336049 51.477561 \r\nL 307.945385 52.830325 \r\nL 308.250054 53.376565 \r\nL 308.554722 52.231257 \r\nL 308.85939 51.625844 \r\nL 309.164059 50.663082 \r\nL 309.468727 51.7085 \r\nL 309.773395 50.409969 \r\nL 310.078064 51.309157 \r\nL 310.6874 48.688271 \r\nL 310.992068 49.774356 \r\nL 311.296737 48.78286 \r\nL 311.601405 46.592792 \r\nL 311.906073 49.379526 \r\nL 312.210742 46.7818 \r\nL 312.51541 46.224567 \r\nL 312.820078 49.130307 \r\nL 313.124747 48.07114 \r\nL 313.429415 45.445127 \r\nL 313.734083 46.414004 \r\nL 314.038752 46.346163 \r\nL 314.34342 46.558728 \r\nL 314.952756 44.478871 \r\nL 315.257425 45.686437 \r\nL 315.562093 45.448885 \r\nL 315.866761 45.730103 \r\nL 316.17143 43.838016 \r\nL 316.476098 44.249842 \r\nL 316.780766 43.664877 \r\nL 317.085435 44.619071 \r\nL 317.390103 43.047429 \r\nL 317.694771 44.997795 \r\nL 318.304108 42.965063 \r\nL 318.608776 42.916712 \r\nL 318.913444 41.387646 \r\nL 319.218113 42.266131 \r\nL 319.522781 41.485575 \r\nL 319.827449 39.267494 \r\nL 320.132118 40.316638 \r\nL 320.436786 40.706653 \r\nL 320.741454 39.877442 \r\nL 321.046123 39.465207 \r\nL 321.350791 42.087991 \r\nL 321.655459 39.711488 \r\nL 321.960127 39.381079 \r\nL 322.264796 38.598263 \r\nL 322.569464 38.274489 \r\nL 322.874132 37.18004 \r\nL 323.178801 37.283333 \r\nL 323.788137 38.779986 \r\nL 324.092806 38.422899 \r\nL 324.397474 37.415126 \r\nL 324.702142 37.159395 \r\nL 325.311479 36.255509 \r\nL 325.616147 34.732 \r\nL 325.920815 36.362676 \r\nL 326.225484 35.688327 \r\nL 326.530152 35.960969 \r\nL 326.83482 35.011194 \r\nL 327.139489 34.958382 \r\nL 327.444157 35.665447 \r\nL 327.748825 34.284527 \r\nL 328.053494 33.949233 \r\nL 328.358162 35.963556 \r\nL 328.967498 33.732986 \r\nL 329.272167 33.471253 \r\nL 329.576835 32.845532 \r\nL 329.881503 31.569986 \r\nL 330.186172 32.870945 \r\nL 330.49084 31.21741 \r\nL 330.795508 32.494574 \r\nL 331.100177 29.943255 \r\nL 331.404845 30.734597 \r\nL 331.709513 29.853581 \r\nL 332.014182 31.230809 \r\nL 332.31885 29.779511 \r\nL 332.623518 31.785642 \r\nL 333.232855 28.690178 \r\nL 333.537523 30.377058 \r\nL 333.842191 29.913236 \r\nL 334.14686 29.852771 \r\nL 334.451528 31.183333 \r\nL 334.756196 30.732177 \r\nL 335.060865 26.904072 \r\nL 335.365533 27.107348 \r\nL 335.670201 28.016873 \r\nL 336.279538 26.75718 \r\nL 336.584206 29.09629 \r\nL 336.888874 26.35916 \r\nL 337.193543 28.071404 \r\nL 337.498211 26.967314 \r\nL 337.802879 27.259969 \r\nL 338.107548 25.039798 \r\nL 338.412216 26.32025 \r\nL 338.716884 25.56608 \r\nL 339.021553 25.797004 \r\nL 339.326221 24.596755 \r\nL 339.630889 25.833596 \r\nL 339.935557 26.519243 \r\nL 340.240226 24.197011 \r\nL 340.544894 24.71287 \r\nL 340.849562 25.662323 \r\nL 341.458899 21.698463 \r\nL 341.763567 22.334391 \r\nL 342.068236 24.319876 \r\nL 342.372904 21.732115 \r\nL 342.677572 21.025305 \r\nL 342.98224 21.22173 \r\nL 343.286909 22.961568 \r\nL 343.591577 21.625564 \r\nL 343.896245 22.106387 \r\nL 344.505582 22.247791 \r\nL 344.81025 21.124879 \r\nL 345.419587 21.026533 \r\nL 345.724255 21.751201 \r\nL 346.028924 18.855564 \r\nL 346.333592 20.664298 \r\nL 346.63826 18.640911 \r\nL 346.942928 20.367292 \r\nL 347.552265 18.618192 \r\nL 347.856933 19.320028 \r\nL 348.161602 19.372692 \r\nL 348.46627 20.874641 \r\nL 348.770938 17.558804 \r\nL 349.075607 17.083636 \r\nL 349.684943 18.837646 \r\nL 349.684943 18.837646 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 30.103125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 364.903125 224.64 \r\nL 364.903125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 30.103125 224.64 \r\nL 364.903125 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 30.103125 7.2 \r\nL 364.903125 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"pb825eedddf\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"30.103125\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(17, 15),\n",
    "            nn.BatchNorm1d(15),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(15, 15),\n",
    "            nn.BatchNorm1d(15),\n",
    "            nn.SiLU(),                  # Swish\n",
    "            nn.Linear(15, 15),\n",
    "            nn.BatchNorm1d(15),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(15, 3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "plt_loss = []\n",
    "plt_val_loss = []\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # 예측(prediction)과 손실(loss) 계산\n",
    "        X=X.float()\n",
    "        pred = model(X)\n",
    "        y = y.reshape(y.shape[0],).long()\n",
    "        #print(f\"{pred.shape}, {y.shape}\")\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # 역전파\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    loss, current = loss.item(), batch * len(X)\n",
    "    plt_loss.append(loss)\n",
    "    #print(pred)\n",
    "    print(f\"loss: {loss:>7f}\", end=\" \")\n",
    "            \n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.float()\n",
    "            pred = model(X)\n",
    "            y = y.reshape(y.shape[0],).long()\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "model = NeuralNetwork()\n",
    "learning_rate = 2e-5\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 1000\n",
    "for t in range(epochs):\n",
    "    print(f\"(Epoch {t+1}) \")\n",
    "    model.train()\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    with torch.no_grad():\n",
    "        model.eval()\n",
    "        pred = model(X_val_ts.float())\n",
    "        y_val_onehot = pd.get_dummies(Y_val)\n",
    "        val_loss = log_loss(y_val_onehot, pred.detach().numpy())\n",
    "    plt_val_loss.append(val_loss)\n",
    "    print(f\"val_loss : {val_loss}\")\n",
    "    #test_loop(val_dataloader, model, loss_fn)\n",
    "print(\"Done!\")\n",
    "plt.plot(plt_loss)\n",
    "plt.plot(plt_val_loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "2.5604251496805572"
      ]
     },
     "metadata": {},
     "execution_count": 35
    }
   ],
   "source": [
    "pred = model(X_val_ts.float())\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "log_loss(y_val_onehot, pred.detach().numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8516217629536095"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "min(plt_val_loss)"
   ]
  },
  {
   "source": [
    "1. 0.8643049465735934\n",
    "2. 0.8608484612998781\n",
    "3. 0.8568888006654828"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}