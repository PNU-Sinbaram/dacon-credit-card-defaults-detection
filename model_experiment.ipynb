{
 "metadata": {
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  },
  "orig_nbformat": 2,
  "kernelspec": {
   "name": "python383jvsc74a57bd0bed0ea09e0a217f0cd8c3af8b97b5ce48feb5846fec0e29c23d707f4dd7f9787",
   "display_name": "Python 3.8.3 64-bit ('mlenv': venv)"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2,
 "cells": [
  {
   "source": [
    "## 0) 데이터 전처리\n",
    "* Baseline코드에서 사용했던 전처리 방식을 그대로 사용\n",
    "* index column은 학습에 관련이 없으니 제거"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn import preprocessing\n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "\n",
    "train = pd.read_csv(\"../data/train.csv\")                            #r*c = 20000*20, test와 달리 credit이라는 column을 갖고 있고, 이 값을 예측\n",
    "test = pd.read_csv(\"../data/test.csv\")                              #10000*19. train으로 학습시키고 test데이터를 입력으로 넣어서 credit을 예측\n",
    "sample_submission = pd.read_csv(\"../data/sample_submission.csv\")    #예측값은 sample_submission과 형태가 같아야 함."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.concat([train, test], axis=0)    # train데이터 밑에 test데이터가 붙음. test에는 credit이 없으므로, 결측치(NaN)형태로 저장됨\n",
    "# 실제로는 결측치를 완전히 날리는건 좋지 않지만, 1회 출제용으로 사용하기때문에 완전히 날림\n",
    "data = data.drop(\"occyp_type\", axis=1) # occyp_type column을 지움. axis : occyp_type이 row에 있는지 column에 있는지 알려줌. 1이면 column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "unique_len = data.apply(lambda x : len(x.unique()))  # 각 column별로 unique()를 수행하여, 그 길이를 반환. 즉, 모든 column의 요소의 개수를 출력\n",
    "group_1 = unique_len[unique_len <= 2].index   # 요소의 값이 2개 이하인 column들의 이름을 추출\n",
    "group_2 = unique_len[(unique_len > 2) & (unique_len <= 10)].index\n",
    "group_3 = unique_len[(unique_len > 10)].index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "data['gender'] = data['gender'].replace(['F','M'], [0, 1])   # F를 0으로, M을 1로 교체\n",
    "data['car'] = data['car'].replace(['N', 'Y'], [0, 1])\n",
    "data['reality'] = data['reality'].replace(['N', 'Y'], [0,1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[data['child_num']>2, 'child_num'] = 2  # child_num>2인 child_num column을 가져와서 2로 바꿈\n",
    "data[group_2].apply(lambda x : len(x.unique()))\n",
    "label_encoder = preprocessing.LabelEncoder() # categorical 변수(문자로 되어있는 변수)들을 숫자로 인코딩해주는 함수\n",
    "set(label_encoder.fit_transform(data['income_type'])) # income_type column에서 각 요소들을 숫자로 바꿔줌. fit_transform이 배열을 반환해서 unique()대신 set을 사용\n",
    "data['income_type'] = label_encoder.fit_transform(data['income_type'])\n",
    "data['edu_type'] = label_encoder.fit_transform(data['edu_type'])\n",
    "data['family_type'] = label_encoder.fit_transform(data['family_type'])\n",
    "data['house_type'] = label_encoder.fit_transform(data['house_type'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts, bin_dividers = np.histogram(data['income_total'], bins=7) # 연속형 변수를 입력으로 받아 몇 개의 구간으로 나눌지 설정해줌. 각 구간들의 분절점(나누는 기준) 및 구간별 요소의 개수를 출력해줌\n",
    "data['income_total'] = pd.factorize(pd.cut(data['income_total'], bins = bin_dividers, include_lowest=True, labels=[0,1,2,3,4,5,6]))[0] # pd.cut의 반환 데이터 타입은 category이기 때문에, series타입(int형 배열)으로 바꿔주는 작업을 거쳐야 함\n",
    "#위의 과정을 함수로 만듬\n",
    "def make_bin(array, N):\n",
    "    array = -array      #DAYS_BIRTH등의 column은 음수이기 때문에 양수로 바꿔줌\n",
    "    _, bin_dividers = np.histogram(array, bins = N)       # 여기선 counts 변수를 사용하지 않을 것이기 때문에 사용하지 않는다는 의미로 _로 설정.\n",
    "    cut_categories = pd.cut(array, bin_dividers, labels = [i for i in range(N)], include_lowest=True)\n",
    "    bined_array = pd.factorize(cut_categories)[0]\n",
    "    return bined_array\n",
    "data['DAYS_BIRTH'] = make_bin(data['DAYS_BIRTH'], 10)\n",
    "data['DAYS_EMPLOYED'] = make_bin(data['DAYS_EMPLOYED'], 10)\n",
    "data['begin_month'] = make_bin(data['begin_month'], 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data.drop('index', axis=1)\n",
    "train = data[:-10000]   # train에 해당하는 값\n",
    "test = data[-10000:]   #test에 해당하는 값\n",
    "train_x = train.drop(\"credit\", axis = 1) # credit은 출력값이고, credit을 제외한 값들이 모델의 입력값이므로 column들 중(axis =1) credit을 찾아 없앰.\n",
    "train_y = train['credit']               # 모델의 출력이 credit\n",
    "test_x = test.drop(\"credit\", axis=1)        # data라는 dataframe을 만들면서 test set에 없던 credit이라는 column이 생겼으므로, 이를 다시 제거"
   ]
  },
  {
   "source": [
    "## 1) RandomForestClassifier"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, stratify=train_y, test_size=0.2, random_state=10086)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "1.466047343279552\n"
     ]
    }
   ],
   "source": [
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, Y_train)\n",
    "predict = clf.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "logloss = log_loss(y_val_onehot, predict)\n",
    "print(logloss)"
   ]
  },
  {
   "source": [
    "### 1_1) 하이퍼파라미터 조정"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "rf_param_grid = {\n",
    "    'n_estimators' : [100, 200],\n",
    "    'max_depth' : [8, 10, 12],\n",
    "    'min_samples_leaf' : [3, 5, 7],\n",
    "    'min_samples_split' : [3, 4, 5]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "0.8262986236456679\n"
     ]
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, stratify=train_y, test_size=0.25, random_state=10086)\n",
    "\n",
    "clf = RandomForestClassifier()\n",
    "clf_grid = GridSearchCV(clf, param_grid=rf_param_grid, scoring=\"accuracy\", n_jobs=-1, verbose=2)\n",
    "clf_grid.fit(X_train, Y_train)\n",
    "\n",
    "predict = clf_grid.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "logloss=log_loss(y_val_onehot, predict)\n",
    "print(logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
      "[0.8282641291283019, 0.8225467954824329, 0.8253606275571261, 0.8224319510375313, 0.8201274143462735]\n"
     ]
    }
   ],
   "source": [
    "folds = StratifiedKFold(n_splits = 5, shuffle=True, random_state = 55)  # 5개의 fold로 나눔\n",
    "\n",
    "outcomes = []\n",
    "for n_fold, (train_index, val_index) in enumerate(folds.split(train_x, train_y)):\n",
    "    X_train, X_val = train_x.iloc[train_index], train_x.iloc[val_index]\n",
    "    Y_train, Y_val = train_y.iloc[train_index], train_y.iloc[val_index]\n",
    "    clf = RandomForestClassifier()\n",
    "    clf_grid = GridSearchCV(clf, param_grid=rf_param_grid, scoring=\"accuracy\", n_jobs=-1, verbose=2)\n",
    "    clf_grid.fit(X_train, Y_train)\n",
    "    predictions = clf_grid.predict_proba(X_val)\n",
    "    y_val_onehot = pd.get_dummies(Y_val)\n",
    "\n",
    "    logloss = log_loss(y_val_onehot, predictions)\n",
    "    outcomes.append(logloss)\n",
    "\n",
    "print(outcomes)"
   ]
  },
  {
   "source": [
    "## 2) Gradient Boosting"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "0.8335746908166416\n"
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, stratify=train_y, test_size=0.25, random_state=10086)\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "gb.fit(X_train, Y_train)\n",
    "predict = gb.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "logloss = log_loss(y_val_onehot, predict)\n",
    "print(logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.8417415560977669, 0.8341085098795408, 0.8386208102239824, 0.8392900501671904, 0.8335760935299279]\n"
     ]
    }
   ],
   "source": [
    "folds = StratifiedKFold(n_splits = 5, shuffle=True, random_state = 55)  # 5개의 fold로 나눔\n",
    "\n",
    "outcomes = []\n",
    "for n_fold, (train_index, val_index) in enumerate(folds.split(train_x, train_y)):\n",
    "    X_train, X_val = train_x.iloc[train_index], train_x.iloc[val_index]\n",
    "    Y_train, Y_val = train_y.iloc[train_index], train_y.iloc[val_index]\n",
    "    gb = GradientBoostingClassifier()\n",
    "    gb.fit(X_train, Y_train)\n",
    "    predictions = gb.predict_proba(X_val)\n",
    "    y_val_onehot = pd.get_dummies(Y_val)\n",
    "\n",
    "    logloss = log_loss(y_val_onehot, predictions)\n",
    "    outcomes.append(logloss)\n",
    "print(outcomes)"
   ]
  },
  {
   "source": [
    "### 2_1) Gradient Boosting parameter 조정\n",
    "* **주의. 한번 실행에 시간 소요가 너무 김**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "gb_param_grid = {\n",
    "    'n_estimators' : [100, 200],\n",
    "    'max_depth' : [8, 10, 12],\n",
    "    'min_samples_leaf' : [5, 7, 10],\n",
    "    'min_samples_split' : [2, 3, 5],\n",
    "    'learning_rate' : [0.05, 0.1, 0.2]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Fitting 5 folds for each of 162 candidates, totalling 810 fits\n"
     ]
    },
    {
     "output_type": "error",
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-46-4a78ca480250>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[0mgb\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGradientBoostingClassifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mgb_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgb\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mparam_grid\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgb_param_grid\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"accuracy\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m-\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 6\u001b[1;33m \u001b[0mgb_grid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mY_train\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      7\u001b[0m \u001b[0mpredict\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mgb_grid\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[0my_val_onehot\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_dummies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mY_val\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pythonenv\\mlenv\\lib\\site-packages\\sklearn\\utils\\validation.py\u001b[0m in \u001b[0;36minner_f\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m     61\u001b[0m             \u001b[0mextra_args\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mall_args\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     62\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mextra_args\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 63\u001b[1;33m                 \u001b[1;32mreturn\u001b[0m \u001b[0mf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m*\u001b[0m\u001b[0margs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     64\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     65\u001b[0m             \u001b[1;31m# extra_args > 0\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pythonenv\\mlenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[0;32m    839\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mresults\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    840\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 841\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    842\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    843\u001b[0m             \u001b[1;31m# multimetric is determined here because in the case of a callable\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pythonenv\\mlenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36m_run_search\u001b[1;34m(self, evaluate_candidates)\u001b[0m\n\u001b[0;32m   1294\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_run_search\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1295\u001b[0m         \u001b[1;34m\"\"\"Search all candidates in param_grid\"\"\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1296\u001b[1;33m         \u001b[0mevaluate_candidates\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mParameterGrid\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1297\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1298\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pythonenv\\mlenv\\lib\\site-packages\\sklearn\\model_selection\\_search.py\u001b[0m in \u001b[0;36mevaluate_candidates\u001b[1;34m(candidate_params, cv, more_results)\u001b[0m\n\u001b[0;32m    793\u001b[0m                               n_splits, n_candidates, n_candidates * n_splits))\n\u001b[0;32m    794\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 795\u001b[1;33m                 out = parallel(delayed(_fit_and_score)(clone(base_estimator),\n\u001b[0m\u001b[0;32m    796\u001b[0m                                                        \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    797\u001b[0m                                                        \u001b[0mtrain\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtrain\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtest\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtest\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pythonenv\\mlenv\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[1;34m(self, iterable)\u001b[0m\n\u001b[0;32m   1052\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1053\u001b[0m             \u001b[1;32mwith\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieval_context\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1054\u001b[1;33m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1055\u001b[0m             \u001b[1;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1056\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m-\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pythonenv\\mlenv\\lib\\site-packages\\joblib\\parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    931\u001b[0m             \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    932\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'supports_timeout'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 933\u001b[1;33m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    934\u001b[0m                 \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    935\u001b[0m                     \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\pythonenv\\mlenv\\lib\\site-packages\\joblib\\_parallel_backends.py\u001b[0m in \u001b[0;36mwrap_future_result\u001b[1;34m(future, timeout)\u001b[0m\n\u001b[0;32m    540\u001b[0m         AsyncResults.get from multiprocessing.\"\"\"\n\u001b[0;32m    541\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 542\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mfuture\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresult\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    543\u001b[0m         \u001b[1;32mexcept\u001b[0m \u001b[0mCfTimeoutError\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    544\u001b[0m             \u001b[1;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m \u001b[1;32mfrom\u001b[0m \u001b[0me\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\concurrent\\futures\\_base.py\u001b[0m in \u001b[0;36mresult\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    432\u001b[0m                 \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m__get_result\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    433\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 434\u001b[1;33m             \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_condition\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    435\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    436\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_state\u001b[0m \u001b[1;32min\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mCANCELLED\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mCANCELLED_AND_NOTIFIED\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python38\\lib\\threading.py\u001b[0m in \u001b[0;36mwait\u001b[1;34m(self, timeout)\u001b[0m\n\u001b[0;32m    300\u001b[0m         \u001b[1;32mtry\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    301\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 302\u001b[1;33m                 \u001b[0mwaiter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    303\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mTrue\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    304\u001b[0m             \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "gb = GradientBoostingClassifier()\n",
    "gb_grid = GridSearchCV(gb, param_grid = gb_param_grid, scoring=\"accuracy\", n_jobs=-1, verbose=3)\n",
    "gb_grid.fit(X_train, Y_train)\n",
    "predict = gb_grid.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "logloss = log_loss(y_val_onehot, predict)\n",
    "print(logloss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "{'learning_rate': 0.05,\n",
       " 'max_depth': 8,\n",
       " 'min_samples_leaf': 7,\n",
       " 'min_samples_split': 2,\n",
       " 'n_estimators': 100}"
      ]
     },
     "metadata": {},
     "execution_count": 34
    }
   ],
   "source": [
    "gb_grid.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[0.8299092537458699, 0.8195784379737493, 0.822702511343288, 0.8219319425895892, 0.8183218626777928]\n"
     ]
    }
   ],
   "source": [
    "folds = StratifiedKFold(n_splits = 5, shuffle=True, random_state = 55)  # 5개의 fold로 나눔\n",
    "\n",
    "outcomes = []\n",
    "for n_fold, (train_index, val_index) in enumerate(folds.split(train_x, train_y)):\n",
    "    X_train, X_val = train_x.iloc[train_index], train_x.iloc[val_index]\n",
    "    Y_train, Y_val = train_y.iloc[train_index], train_y.iloc[val_index]\n",
    "    gb = GradientBoostingClassifier(n_estimators=100, learning_rate=0.05, max_depth=8, min_samples_leaf=7, min_samples_split=2)\n",
    "    gb.fit(X_train, Y_train)\n",
    "    predict = gb.predict_proba(X_val)\n",
    "    y_val_onehot = pd.get_dummies(Y_val)\n",
    "    logloss = log_loss(y_val_onehot, predict)\n",
    "    outcomes.append(logloss)\n",
    "print(outcomes)"
   ]
  },
  {
   "source": [
    "## 3) AdaBoost"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "1.0877788002340585"
      ]
     },
     "metadata": {},
     "execution_count": 51
    }
   ],
   "source": [
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, stratify=train_y, test_size=0.25, random_state=10086)\n",
    "\n",
    "ag = AdaBoostClassifier()\n",
    "ag.fit(X_train, Y_train)\n",
    "\n",
    "predict = ag.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "log_loss(y_val_onehot, predict)"
   ]
  },
  {
   "source": [
    "## 4) XgBoost"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[20:56:01] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "[20:56:07] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "[20:56:13] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "[20:56:20] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n",
      "[20:56:26] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "[0.8324195141525684, 0.8266500276527476, 0.8265990901853171, 0.8235172349839919, 0.8234311175080091]\n",
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "outcomes = []\n",
    "for n_fold, (train_index, val_index) in enumerate(folds.split(train_x, train_y)):\n",
    "    X_train, X_val = train_x.iloc[train_index], train_x.iloc[val_index]\n",
    "    Y_train, Y_val = train_y.iloc[train_index], train_y.iloc[val_index]\n",
    "    xgb = XGBClassifier(n_estimators=500, learning_rate=0.1, max_depth = 4, use_label_encoder=False)\n",
    "    xgb.fit(X_train, Y_train)\n",
    "    predictions = xgb.predict_proba(X_val)\n",
    "    y_val_onehot = pd.get_dummies(Y_val)\n",
    "\n",
    "    logloss = log_loss(y_val_onehot, predictions)\n",
    "    outcomes.append(logloss)\n",
    "print(outcomes)"
   ]
  },
  {
   "source": [
    "### 4_1) XgBoost hyperparameter tuning\n",
    "* **주의. 한번 실행에 시간 소요가 너무 김**"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "\n",
    "xgb = XGBClassifier()\n",
    "\n",
    "xgb_param_grid = {\n",
    "    'n_estimators' : [300, 500, 600],\n",
    "    'learning_rate' : [0.01, 0.05, 0.1, 0.15],\n",
    "    'max_depth' : [3, 4, 6, 8]\n",
    "}\n",
    "\n",
    "xgb_grid = GridSearchCV(xgb, param_grid = xgb_param_grid, scoring=\"accuracy\", n_jobs= -1, verbose = 3)\n",
    "xgb_grid.fit(X_train, Y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "실행 시간이 너무 오래 걸려 전에 돌려놨던 결과로 대체함\n",
    "Fitting 5 folds for each of 1 candidates, totalling 5 fits\n",
    "c:\\Users\\lijm1\\Desktop\\pythonenv\\mlenv\\lib\\site-packages\\xgboost\\sklearn.py:1146: UserWarning: The use of label encoder in XGBClassifier is deprecated and will be removed in a future release. To remove this warning, do the following: 1) Pass option use_label_encoder=False when constructing XGBClassifier object; and 2) Encode your labels (y) as integers starting with 0, i.e. 0, 1, 2, ..., [num_class - 1].\n",
    "  warnings.warn(label_encoder_deprecation_msg, UserWarning)\n",
    "[01:20:08] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
    "GridSearchCV(estimator=XGBClassifier(base_score=None, booster=None,\n",
    "                                     colsample_bylevel=None,\n",
    "                                     colsample_bynode=None,\n",
    "                                     colsample_bytree=None, gamma=None,\n",
    "                                     gpu_id=None, importance_type='gain',\n",
    "                                     interaction_constraints=None,\n",
    "                                     learning_rate=None, max_delta_step=None,\n",
    "                                     max_depth=None, min_child_weight=None,\n",
    "                                     missing=nan, monotone_constraints=None,\n",
    "                                     n_estimators=100, n_jobs=None,\n",
    "                                     num_parallel_tree=None, random_state=None,\n",
    "                                     reg_alpha=None, reg_lambda=None,\n",
    "                                     scale_pos_weight=None, subsample=None,\n",
    "                                     tree_method=None, validate_parameters=None,\n",
    "                                     verbosity=None),\n",
    "             n_jobs=-1,\n",
    "             param_grid={'learning_rate': [0.01], 'max_depth': [4],\n",
    "                         'n_estimators': [500]},\n",
    "             scoring='accuracy', verbose=3)\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[20:57:22] WARNING: C:/Users/Administrator/workspace/xgboost-win64_release_1.4.0/src/learner.cc:1095: Starting in XGBoost 1.3.0, the default evaluation metric used with the objective 'multi:softprob' was changed from 'merror' to 'mlogloss'. Explicitly set eval_metric if you'd like to restore the old behavior.\n",
      "d:\\pythonenv\\mlenv\\lib\\site-packages\\xgboost\\data.py:112: UserWarning: Use subset (sliced data) of np.ndarray is not recommended because it will generate extra copies and increase memory consumption\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8359875737021389"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "from xgboost import XGBClassifier\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, stratify=train_y, test_size=0.25, random_state=10086)\n",
    "\n",
    "xgb = XGBClassifier(n_estimators=500, learning_rate=0.01, max_depth = 4, use_label_encoder=False)\n",
    "xgb.fit(X_train, Y_train)\n",
    "predictions = xgb.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "\n",
    "log_loss(y_val_onehot, predictions)"
   ]
  },
  {
   "source": [
    "## 5) LightGBM"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8248750259707588"
      ]
     },
     "metadata": {},
     "execution_count": 26
    }
   ],
   "source": [
    "from lightgbm import LGBMClassifier, plot_importance\n",
    "\n",
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, stratify=train_y, test_size=0.25, random_state=10086)\n",
    "\n",
    "lgb = LGBMClassifier(n_estimators=400)\n",
    "lgb.fit(X_train, Y_train)\n",
    "predict = lgb.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "log_loss(y_val_onehot, predict)"
   ]
  },
  {
   "source": [
    "### 5_1) LGBM Early stopping 적용"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "[1]\tvalid_0's multi_logloss: 0.871384\n",
      "Training until validation scores don't improve for 100 rounds\n",
      "[2]\tvalid_0's multi_logloss: 0.863215\n",
      "[3]\tvalid_0's multi_logloss: 0.857059\n",
      "[4]\tvalid_0's multi_logloss: 0.852325\n",
      "[5]\tvalid_0's multi_logloss: 0.848258\n",
      "[6]\tvalid_0's multi_logloss: 0.84532\n",
      "[7]\tvalid_0's multi_logloss: 0.842952\n",
      "[8]\tvalid_0's multi_logloss: 0.840694\n",
      "[9]\tvalid_0's multi_logloss: 0.83901\n",
      "[10]\tvalid_0's multi_logloss: 0.837603\n",
      "[11]\tvalid_0's multi_logloss: 0.836505\n",
      "[12]\tvalid_0's multi_logloss: 0.835527\n",
      "[13]\tvalid_0's multi_logloss: 0.834671\n",
      "[14]\tvalid_0's multi_logloss: 0.833793\n",
      "[15]\tvalid_0's multi_logloss: 0.833183\n",
      "[16]\tvalid_0's multi_logloss: 0.83234\n",
      "[17]\tvalid_0's multi_logloss: 0.831761\n",
      "[18]\tvalid_0's multi_logloss: 0.831425\n",
      "[19]\tvalid_0's multi_logloss: 0.830885\n",
      "[20]\tvalid_0's multi_logloss: 0.830542\n",
      "[21]\tvalid_0's multi_logloss: 0.830351\n",
      "[22]\tvalid_0's multi_logloss: 0.830166\n",
      "[23]\tvalid_0's multi_logloss: 0.830036\n",
      "[24]\tvalid_0's multi_logloss: 0.829671\n",
      "[25]\tvalid_0's multi_logloss: 0.829519\n",
      "[26]\tvalid_0's multi_logloss: 0.829229\n",
      "[27]\tvalid_0's multi_logloss: 0.829262\n",
      "[28]\tvalid_0's multi_logloss: 0.829037\n",
      "[29]\tvalid_0's multi_logloss: 0.828849\n",
      "[30]\tvalid_0's multi_logloss: 0.828643\n",
      "[31]\tvalid_0's multi_logloss: 0.828665\n",
      "[32]\tvalid_0's multi_logloss: 0.828359\n",
      "[33]\tvalid_0's multi_logloss: 0.828262\n",
      "[34]\tvalid_0's multi_logloss: 0.827804\n",
      "[35]\tvalid_0's multi_logloss: 0.827619\n",
      "[36]\tvalid_0's multi_logloss: 0.827389\n",
      "[37]\tvalid_0's multi_logloss: 0.827356\n",
      "[38]\tvalid_0's multi_logloss: 0.827293\n",
      "[39]\tvalid_0's multi_logloss: 0.826958\n",
      "[40]\tvalid_0's multi_logloss: 0.826816\n",
      "[41]\tvalid_0's multi_logloss: 0.826786\n",
      "[42]\tvalid_0's multi_logloss: 0.826573\n",
      "[43]\tvalid_0's multi_logloss: 0.82634\n",
      "[44]\tvalid_0's multi_logloss: 0.826291\n",
      "[45]\tvalid_0's multi_logloss: 0.826135\n",
      "[46]\tvalid_0's multi_logloss: 0.826101\n",
      "[47]\tvalid_0's multi_logloss: 0.825966\n",
      "[48]\tvalid_0's multi_logloss: 0.825712\n",
      "[49]\tvalid_0's multi_logloss: 0.825559\n",
      "[50]\tvalid_0's multi_logloss: 0.825187\n",
      "[51]\tvalid_0's multi_logloss: 0.825019\n",
      "[52]\tvalid_0's multi_logloss: 0.825123\n",
      "[53]\tvalid_0's multi_logloss: 0.825115\n",
      "[54]\tvalid_0's multi_logloss: 0.824682\n",
      "[55]\tvalid_0's multi_logloss: 0.824667\n",
      "[56]\tvalid_0's multi_logloss: 0.824501\n",
      "[57]\tvalid_0's multi_logloss: 0.824404\n",
      "[58]\tvalid_0's multi_logloss: 0.824241\n",
      "[59]\tvalid_0's multi_logloss: 0.824274\n",
      "[60]\tvalid_0's multi_logloss: 0.824144\n",
      "[61]\tvalid_0's multi_logloss: 0.823991\n",
      "[62]\tvalid_0's multi_logloss: 0.823945\n",
      "[63]\tvalid_0's multi_logloss: 0.823793\n",
      "[64]\tvalid_0's multi_logloss: 0.82363\n",
      "[65]\tvalid_0's multi_logloss: 0.823577\n",
      "[66]\tvalid_0's multi_logloss: 0.823364\n",
      "[67]\tvalid_0's multi_logloss: 0.823236\n",
      "[68]\tvalid_0's multi_logloss: 0.823105\n",
      "[69]\tvalid_0's multi_logloss: 0.823008\n",
      "[70]\tvalid_0's multi_logloss: 0.823012\n",
      "[71]\tvalid_0's multi_logloss: 0.823018\n",
      "[72]\tvalid_0's multi_logloss: 0.822906\n",
      "[73]\tvalid_0's multi_logloss: 0.822797\n",
      "[74]\tvalid_0's multi_logloss: 0.822914\n",
      "[75]\tvalid_0's multi_logloss: 0.822859\n",
      "[76]\tvalid_0's multi_logloss: 0.822457\n",
      "[77]\tvalid_0's multi_logloss: 0.822447\n",
      "[78]\tvalid_0's multi_logloss: 0.822259\n",
      "[79]\tvalid_0's multi_logloss: 0.822138\n",
      "[80]\tvalid_0's multi_logloss: 0.822083\n",
      "[81]\tvalid_0's multi_logloss: 0.822013\n",
      "[82]\tvalid_0's multi_logloss: 0.822043\n",
      "[83]\tvalid_0's multi_logloss: 0.822145\n",
      "[84]\tvalid_0's multi_logloss: 0.822083\n",
      "[85]\tvalid_0's multi_logloss: 0.821986\n",
      "[86]\tvalid_0's multi_logloss: 0.821953\n",
      "[87]\tvalid_0's multi_logloss: 0.821722\n",
      "[88]\tvalid_0's multi_logloss: 0.821543\n",
      "[89]\tvalid_0's multi_logloss: 0.821408\n",
      "[90]\tvalid_0's multi_logloss: 0.821313\n",
      "[91]\tvalid_0's multi_logloss: 0.821416\n",
      "[92]\tvalid_0's multi_logloss: 0.820971\n",
      "[93]\tvalid_0's multi_logloss: 0.820791\n",
      "[94]\tvalid_0's multi_logloss: 0.820557\n",
      "[95]\tvalid_0's multi_logloss: 0.820226\n",
      "[96]\tvalid_0's multi_logloss: 0.820044\n",
      "[97]\tvalid_0's multi_logloss: 0.819827\n",
      "[98]\tvalid_0's multi_logloss: 0.819404\n",
      "[99]\tvalid_0's multi_logloss: 0.819393\n",
      "[100]\tvalid_0's multi_logloss: 0.819325\n",
      "[101]\tvalid_0's multi_logloss: 0.81921\n",
      "[102]\tvalid_0's multi_logloss: 0.819381\n",
      "[103]\tvalid_0's multi_logloss: 0.819383\n",
      "[104]\tvalid_0's multi_logloss: 0.819118\n",
      "[105]\tvalid_0's multi_logloss: 0.818937\n",
      "[106]\tvalid_0's multi_logloss: 0.818774\n",
      "[107]\tvalid_0's multi_logloss: 0.818807\n",
      "[108]\tvalid_0's multi_logloss: 0.818723\n",
      "[109]\tvalid_0's multi_logloss: 0.818886\n",
      "[110]\tvalid_0's multi_logloss: 0.818767\n",
      "[111]\tvalid_0's multi_logloss: 0.818832\n",
      "[112]\tvalid_0's multi_logloss: 0.818768\n",
      "[113]\tvalid_0's multi_logloss: 0.818561\n",
      "[114]\tvalid_0's multi_logloss: 0.818637\n",
      "[115]\tvalid_0's multi_logloss: 0.8185\n",
      "[116]\tvalid_0's multi_logloss: 0.818314\n",
      "[117]\tvalid_0's multi_logloss: 0.818235\n",
      "[118]\tvalid_0's multi_logloss: 0.818215\n",
      "[119]\tvalid_0's multi_logloss: 0.818015\n",
      "[120]\tvalid_0's multi_logloss: 0.818105\n",
      "[121]\tvalid_0's multi_logloss: 0.817966\n",
      "[122]\tvalid_0's multi_logloss: 0.817894\n",
      "[123]\tvalid_0's multi_logloss: 0.817587\n",
      "[124]\tvalid_0's multi_logloss: 0.817499\n",
      "[125]\tvalid_0's multi_logloss: 0.817386\n",
      "[126]\tvalid_0's multi_logloss: 0.817303\n",
      "[127]\tvalid_0's multi_logloss: 0.81739\n",
      "[128]\tvalid_0's multi_logloss: 0.817411\n",
      "[129]\tvalid_0's multi_logloss: 0.817387\n",
      "[130]\tvalid_0's multi_logloss: 0.817349\n",
      "[131]\tvalid_0's multi_logloss: 0.817435\n",
      "[132]\tvalid_0's multi_logloss: 0.817424\n",
      "[133]\tvalid_0's multi_logloss: 0.817413\n",
      "[134]\tvalid_0's multi_logloss: 0.817493\n",
      "[135]\tvalid_0's multi_logloss: 0.817377\n",
      "[136]\tvalid_0's multi_logloss: 0.81738\n",
      "[137]\tvalid_0's multi_logloss: 0.817454\n",
      "[138]\tvalid_0's multi_logloss: 0.817259\n",
      "[139]\tvalid_0's multi_logloss: 0.81714\n",
      "[140]\tvalid_0's multi_logloss: 0.816845\n",
      "[141]\tvalid_0's multi_logloss: 0.816691\n",
      "[142]\tvalid_0's multi_logloss: 0.816643\n",
      "[143]\tvalid_0's multi_logloss: 0.816532\n",
      "[144]\tvalid_0's multi_logloss: 0.81637\n",
      "[145]\tvalid_0's multi_logloss: 0.81613\n",
      "[146]\tvalid_0's multi_logloss: 0.815985\n",
      "[147]\tvalid_0's multi_logloss: 0.816012\n",
      "[148]\tvalid_0's multi_logloss: 0.816032\n",
      "[149]\tvalid_0's multi_logloss: 0.81607\n",
      "[150]\tvalid_0's multi_logloss: 0.815985\n",
      "[151]\tvalid_0's multi_logloss: 0.816081\n",
      "[152]\tvalid_0's multi_logloss: 0.815991\n",
      "[153]\tvalid_0's multi_logloss: 0.815889\n",
      "[154]\tvalid_0's multi_logloss: 0.815904\n",
      "[155]\tvalid_0's multi_logloss: 0.815886\n",
      "[156]\tvalid_0's multi_logloss: 0.815752\n",
      "[157]\tvalid_0's multi_logloss: 0.815806\n",
      "[158]\tvalid_0's multi_logloss: 0.815742\n",
      "[159]\tvalid_0's multi_logloss: 0.815868\n",
      "[160]\tvalid_0's multi_logloss: 0.815914\n",
      "[161]\tvalid_0's multi_logloss: 0.815892\n",
      "[162]\tvalid_0's multi_logloss: 0.815838\n",
      "[163]\tvalid_0's multi_logloss: 0.815685\n",
      "[164]\tvalid_0's multi_logloss: 0.815754\n",
      "[165]\tvalid_0's multi_logloss: 0.815742\n",
      "[166]\tvalid_0's multi_logloss: 0.815818\n",
      "[167]\tvalid_0's multi_logloss: 0.815877\n",
      "[168]\tvalid_0's multi_logloss: 0.815856\n",
      "[169]\tvalid_0's multi_logloss: 0.816004\n",
      "[170]\tvalid_0's multi_logloss: 0.816022\n",
      "[171]\tvalid_0's multi_logloss: 0.816048\n",
      "[172]\tvalid_0's multi_logloss: 0.8161\n",
      "[173]\tvalid_0's multi_logloss: 0.816062\n",
      "[174]\tvalid_0's multi_logloss: 0.816022\n",
      "[175]\tvalid_0's multi_logloss: 0.815977\n",
      "[176]\tvalid_0's multi_logloss: 0.81602\n",
      "[177]\tvalid_0's multi_logloss: 0.816042\n",
      "[178]\tvalid_0's multi_logloss: 0.816101\n",
      "[179]\tvalid_0's multi_logloss: 0.816191\n",
      "[180]\tvalid_0's multi_logloss: 0.816228\n",
      "[181]\tvalid_0's multi_logloss: 0.8162\n",
      "[182]\tvalid_0's multi_logloss: 0.816205\n",
      "[183]\tvalid_0's multi_logloss: 0.816312\n",
      "[184]\tvalid_0's multi_logloss: 0.816287\n",
      "[185]\tvalid_0's multi_logloss: 0.816403\n",
      "[186]\tvalid_0's multi_logloss: 0.816519\n",
      "[187]\tvalid_0's multi_logloss: 0.816444\n",
      "[188]\tvalid_0's multi_logloss: 0.816421\n",
      "[189]\tvalid_0's multi_logloss: 0.816298\n",
      "[190]\tvalid_0's multi_logloss: 0.816265\n",
      "[191]\tvalid_0's multi_logloss: 0.816343\n",
      "[192]\tvalid_0's multi_logloss: 0.816423\n",
      "[193]\tvalid_0's multi_logloss: 0.816623\n",
      "[194]\tvalid_0's multi_logloss: 0.81658\n",
      "[195]\tvalid_0's multi_logloss: 0.816574\n",
      "[196]\tvalid_0's multi_logloss: 0.816605\n",
      "[197]\tvalid_0's multi_logloss: 0.816762\n",
      "[198]\tvalid_0's multi_logloss: 0.816645\n",
      "[199]\tvalid_0's multi_logloss: 0.816797\n",
      "[200]\tvalid_0's multi_logloss: 0.816738\n",
      "[201]\tvalid_0's multi_logloss: 0.816695\n",
      "[202]\tvalid_0's multi_logloss: 0.816749\n",
      "[203]\tvalid_0's multi_logloss: 0.816764\n",
      "[204]\tvalid_0's multi_logloss: 0.816815\n",
      "[205]\tvalid_0's multi_logloss: 0.816856\n",
      "[206]\tvalid_0's multi_logloss: 0.816949\n",
      "[207]\tvalid_0's multi_logloss: 0.816955\n",
      "[208]\tvalid_0's multi_logloss: 0.816852\n",
      "[209]\tvalid_0's multi_logloss: 0.816964\n",
      "[210]\tvalid_0's multi_logloss: 0.817072\n",
      "[211]\tvalid_0's multi_logloss: 0.817013\n",
      "[212]\tvalid_0's multi_logloss: 0.817144\n",
      "[213]\tvalid_0's multi_logloss: 0.817175\n",
      "[214]\tvalid_0's multi_logloss: 0.817195\n",
      "[215]\tvalid_0's multi_logloss: 0.817245\n",
      "[216]\tvalid_0's multi_logloss: 0.817418\n",
      "[217]\tvalid_0's multi_logloss: 0.817516\n",
      "[218]\tvalid_0's multi_logloss: 0.817628\n",
      "[219]\tvalid_0's multi_logloss: 0.817768\n",
      "[220]\tvalid_0's multi_logloss: 0.817828\n",
      "[221]\tvalid_0's multi_logloss: 0.81796\n",
      "[222]\tvalid_0's multi_logloss: 0.818176\n",
      "[223]\tvalid_0's multi_logloss: 0.8182\n",
      "[224]\tvalid_0's multi_logloss: 0.818271\n",
      "[225]\tvalid_0's multi_logloss: 0.818258\n",
      "[226]\tvalid_0's multi_logloss: 0.818392\n",
      "[227]\tvalid_0's multi_logloss: 0.818549\n",
      "[228]\tvalid_0's multi_logloss: 0.818574\n",
      "[229]\tvalid_0's multi_logloss: 0.818664\n",
      "[230]\tvalid_0's multi_logloss: 0.818693\n",
      "[231]\tvalid_0's multi_logloss: 0.818632\n",
      "[232]\tvalid_0's multi_logloss: 0.818626\n",
      "[233]\tvalid_0's multi_logloss: 0.818638\n",
      "[234]\tvalid_0's multi_logloss: 0.818587\n",
      "[235]\tvalid_0's multi_logloss: 0.818552\n",
      "[236]\tvalid_0's multi_logloss: 0.818431\n",
      "[237]\tvalid_0's multi_logloss: 0.818532\n",
      "[238]\tvalid_0's multi_logloss: 0.818426\n",
      "[239]\tvalid_0's multi_logloss: 0.818505\n",
      "[240]\tvalid_0's multi_logloss: 0.818419\n",
      "[241]\tvalid_0's multi_logloss: 0.81839\n",
      "[242]\tvalid_0's multi_logloss: 0.818433\n",
      "[243]\tvalid_0's multi_logloss: 0.818293\n",
      "[244]\tvalid_0's multi_logloss: 0.818186\n",
      "[245]\tvalid_0's multi_logloss: 0.818149\n",
      "[246]\tvalid_0's multi_logloss: 0.818069\n",
      "[247]\tvalid_0's multi_logloss: 0.81819\n",
      "[248]\tvalid_0's multi_logloss: 0.818154\n",
      "[249]\tvalid_0's multi_logloss: 0.818171\n",
      "[250]\tvalid_0's multi_logloss: 0.818223\n",
      "[251]\tvalid_0's multi_logloss: 0.818296\n",
      "[252]\tvalid_0's multi_logloss: 0.818262\n",
      "[253]\tvalid_0's multi_logloss: 0.818232\n",
      "[254]\tvalid_0's multi_logloss: 0.818197\n",
      "[255]\tvalid_0's multi_logloss: 0.818272\n",
      "[256]\tvalid_0's multi_logloss: 0.818405\n",
      "[257]\tvalid_0's multi_logloss: 0.818496\n",
      "[258]\tvalid_0's multi_logloss: 0.818531\n",
      "[259]\tvalid_0's multi_logloss: 0.818558\n",
      "[260]\tvalid_0's multi_logloss: 0.818767\n",
      "[261]\tvalid_0's multi_logloss: 0.81879\n",
      "[262]\tvalid_0's multi_logloss: 0.818777\n",
      "[263]\tvalid_0's multi_logloss: 0.818814\n",
      "Early stopping, best iteration is:\n",
      "[163]\tvalid_0's multi_logloss: 0.815685\n"
     ]
    }
   ],
   "source": [
    "lgb = LGBMClassifier(n_estimators=1000)\n",
    "evals = [(X_val, Y_val)]\n",
    "lgb.fit(X_train, Y_train, early_stopping_rounds=100, eval_metric=\"logloss\", eval_set=evals, verbose=True)\n",
    "predict = lgb.predict_proba(X_val)\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "logloss = log_loss(y_val_onehot, predict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8156847808255132"
      ]
     },
     "metadata": {},
     "execution_count": 56
    }
   ],
   "source": [
    "logloss"
   ]
  },
  {
   "source": [
    "## 6) TabNet"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "metadata": {},
     "execution_count": 47
    }
   ],
   "source": [
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, Y_train, Y_val = train_test_split(train_x, train_y, stratify=train_y, test_size=0.18, random_state=10086)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "  0%|          | 0/17 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "340eef1d072d4db19b4dd8035eb6b259"
      }
     },
     "metadata": {}
    }
   ],
   "source": [
    "'''\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "features = [col for col in train_x.columns]\n",
    "\n",
    "categorical_columns = []\n",
    "categorical_dims = {}\n",
    "\n",
    "for col in tqdm(train_x.columns):\n",
    "    l_enc = preprocessing.LabelEncoder()\n",
    "    l_enc.fit_transform(train_x[col].values)\n",
    "    categorical_columns.append(col)\n",
    "    categorical_dims[col] = len(l_enc.classes_)\n",
    "\n",
    "cat_idxs = [i for i, f in enumerate(features) if f in categorical_columns]\n",
    "cat_dims = [categorical_dims[f] for i, f in enumerate(features) if f in categorical_columns]\n",
    "'''"
   ]
  },
  {
   "source": [
    "### Hyperparameter 정리\n",
    "#### * https://github.com/dreamquark-ai/tabnet 및 https://arxiv.org/pdf/1908.07442.pdf 를 참고함. 더 자세한 내용이나 추가 Hyperparameter는 여기로\n",
    "- 모델 파라미터\n",
    "* <code>n_d</code> = decision prediction layer의 층. 값이 클수록 featrue특징을 더 잘 잡아내지만, 오버피팅될 확률이 높음. 8~64값\n",
    "* <code>n_a</code> = Width of the attention embedding for each mask. 논문에서는 n_d == n_a가 되도록 추천함.\n",
    "* <code>n_steps</code> = decision step. 정보를 담은 feature들이 많다면 높은 값을 주는 것이 좋지만, 너무 높으면 성능을 떨어뜨리고 오버피팅될 확률이 높음. 3~10\n",
    "* <code>gamma</code> = This is the coefficient for feature reusage in the masks(?). 한번 선택된 feature가 다시 사용될지를 결정하는 hyperparameter. 논문에서는 성능에 큰 역할을 하고, n_steps가 높을수록 gamma값도 높이는 것을 추천. 1.0~2.0\n",
    "* <code>momentum</code> = 배치 정규화 모멘텀 값. 0.01~0.4\n",
    "* <code>lambda_sparse</code> = sparsity loss coefficient. 이것도 잘... 적당히 높이면 학습에 도움을 준다고 나와있음\n",
    "* <code>optimizer_fn</code> = 신경망 최적화 함수. Adam을 주로 사용\n",
    "* <code>optimizer_params</code> = optimizer_fn에 넣어줄 parameter. optimizer_fn이 Adam이라면 초기 학습률을 parameter로 받음. 기본값이 dict(lr=2e-2)\n",
    "* <code>scheduler_fn</code> = 학습률 scheduler. 여기서는 단계별로 학습률을 감쇠시키는 StepLR사용\n",
    "* <code>scheduler_params</code> = scheduler_fn에 넣어줄 parameter. 여기서는 감쇠율 gamma와 몇 단계 후 감쇠시킬지 정하는 step_size를 설정\n",
    "* <code>device_name</code> = 'cpu'로 설정하면 cpu, 'cuda'로 설정하면 nvidia gpu사용. gpu 사용하려면 cuda 11.1버전 필요(그 이상 버전은 아직 지원 안하는듯)\n",
    "* <code>mask_type</code> = featrue선택에 사용하는 masking function(?). sparsemax 또는 entmax 둘 중 하나 사용.\n",
    "- Fit 파라미터\n",
    "* <code>max_epochs</code> = 최대 epoch\n",
    "* <code>patience</code> = 지정해준 patience만큼의 epoch동안, 학습의 향상이 이루어지지 않으면, 학습을 중단하고 제일 결과가 잘 나온 것의 가중치로 설정. 0으로 하면 조기 종료 없이 계속 진행\n",
    "* <code>loss_fn</code> = loss function 지정\n",
    "* <code>batch_size</code> = 메모리 크기가 되는 한 크게 잡아주는게 좋다고 하는데 batch 크기도 logloss에 영향을 주니 잘 조절하는것이 좋을듯\n",
    "* <code>virtual_batch_size</code> = ghost batch normalization(?)에 사용된다고 함. 작은 값이 좋고, batch_size값을 나눌 수 있어야 함.\n",
    "* <code>num_workers</code> = GPU연산 시 사용할 cpu코어 개수를 설정한다는데... 오히려 설정하니까 더 느림. CPU보다 GPU가 훨씬 좋으면 어느정도 설정해주는 게 좋아보임\n",
    "* <code>drop_last</code> = 맨 마지막, 남는 배치를 쓸지 안쓸지 결정"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "source": [
    "* **주의. 한번 실행에 시간 소요가 너무 김(약 1시간)**\n"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device used : cuda\n",
      "epoch 0  | loss: 3.81459 | val_0_logloss: 5.64735 |  0:00:01s\n",
      "epoch 1  | loss: 1.28557 | val_0_logloss: 2.12134 |  0:00:03s\n",
      "epoch 2  | loss: 1.03773 | val_0_logloss: 1.38402 |  0:00:05s\n",
      "epoch 3  | loss: 0.98534 | val_0_logloss: 1.11979 |  0:00:07s\n",
      "epoch 4  | loss: 0.93837 | val_0_logloss: 1.02961 |  0:00:09s\n",
      "epoch 5  | loss: 0.9175  | val_0_logloss: 0.98219 |  0:00:11s\n",
      "epoch 6  | loss: 0.90652 | val_0_logloss: 0.99071 |  0:00:13s\n",
      "epoch 7  | loss: 0.89724 | val_0_logloss: 0.99378 |  0:00:15s\n",
      "epoch 8  | loss: 0.89369 | val_0_logloss: 0.98137 |  0:00:17s\n",
      "epoch 9  | loss: 0.89046 | val_0_logloss: 0.96116 |  0:00:19s\n",
      "epoch 10 | loss: 0.88661 | val_0_logloss: 0.94896 |  0:00:21s\n",
      "epoch 11 | loss: 0.8849  | val_0_logloss: 0.93632 |  0:00:23s\n",
      "epoch 12 | loss: 0.88414 | val_0_logloss: 0.92281 |  0:00:25s\n",
      "epoch 13 | loss: 0.8811  | val_0_logloss: 0.91622 |  0:00:27s\n",
      "epoch 14 | loss: 0.87958 | val_0_logloss: 0.91203 |  0:00:29s\n",
      "epoch 15 | loss: 0.88089 | val_0_logloss: 0.9036  |  0:00:30s\n",
      "epoch 16 | loss: 0.87841 | val_0_logloss: 0.89869 |  0:00:32s\n",
      "epoch 17 | loss: 0.8763  | val_0_logloss: 0.89261 |  0:00:34s\n",
      "epoch 18 | loss: 0.87468 | val_0_logloss: 0.88652 |  0:00:35s\n",
      "epoch 19 | loss: 0.87431 | val_0_logloss: 0.8823  |  0:00:37s\n",
      "epoch 20 | loss: 0.87203 | val_0_logloss: 0.88086 |  0:00:38s\n",
      "epoch 21 | loss: 0.87203 | val_0_logloss: 0.87962 |  0:00:40s\n",
      "epoch 22 | loss: 0.86825 | val_0_logloss: 0.87701 |  0:00:41s\n",
      "epoch 23 | loss: 0.86756 | val_0_logloss: 0.8721  |  0:00:43s\n",
      "epoch 24 | loss: 0.86673 | val_0_logloss: 0.87102 |  0:00:44s\n",
      "epoch 25 | loss: 0.86476 | val_0_logloss: 0.87053 |  0:00:46s\n",
      "epoch 26 | loss: 0.86531 | val_0_logloss: 0.86902 |  0:00:47s\n",
      "epoch 27 | loss: 0.86207 | val_0_logloss: 0.8693  |  0:00:49s\n",
      "epoch 28 | loss: 0.86175 | val_0_logloss: 0.86791 |  0:00:51s\n",
      "epoch 29 | loss: 0.86089 | val_0_logloss: 0.86632 |  0:00:52s\n",
      "epoch 30 | loss: 0.86053 | val_0_logloss: 0.8715  |  0:00:54s\n",
      "epoch 31 | loss: 0.85931 | val_0_logloss: 0.86281 |  0:00:56s\n",
      "epoch 32 | loss: 0.85617 | val_0_logloss: 0.86289 |  0:00:57s\n",
      "epoch 33 | loss: 0.85773 | val_0_logloss: 0.86257 |  0:00:59s\n",
      "epoch 34 | loss: 0.85621 | val_0_logloss: 0.86101 |  0:01:01s\n",
      "epoch 35 | loss: 0.85491 | val_0_logloss: 0.86181 |  0:01:02s\n",
      "epoch 36 | loss: 0.85406 | val_0_logloss: 0.85867 |  0:01:05s\n",
      "epoch 37 | loss: 0.85071 | val_0_logloss: 0.86214 |  0:01:07s\n",
      "epoch 38 | loss: 0.85183 | val_0_logloss: 0.8626  |  0:01:09s\n",
      "epoch 39 | loss: 0.85033 | val_0_logloss: 0.86298 |  0:01:12s\n",
      "epoch 40 | loss: 0.85013 | val_0_logloss: 0.86835 |  0:01:14s\n",
      "epoch 41 | loss: 0.85094 | val_0_logloss: 0.86579 |  0:01:16s\n",
      "epoch 42 | loss: 0.85037 | val_0_logloss: 0.8602  |  0:01:18s\n",
      "epoch 43 | loss: 0.84864 | val_0_logloss: 0.8635  |  0:01:21s\n",
      "epoch 44 | loss: 0.84882 | val_0_logloss: 0.86394 |  0:01:23s\n",
      "epoch 45 | loss: 0.84797 | val_0_logloss: 0.85897 |  0:01:25s\n",
      "epoch 46 | loss: 0.8482  | val_0_logloss: 0.86516 |  0:01:27s\n",
      "epoch 47 | loss: 0.84893 | val_0_logloss: 0.86593 |  0:01:28s\n",
      "epoch 48 | loss: 0.84823 | val_0_logloss: 0.8641  |  0:01:30s\n",
      "epoch 49 | loss: 0.84807 | val_0_logloss: 0.8597  |  0:01:31s\n",
      "epoch 50 | loss: 0.84752 | val_0_logloss: 0.86177 |  0:01:33s\n",
      "epoch 51 | loss: 0.84704 | val_0_logloss: 0.86472 |  0:01:35s\n",
      "epoch 52 | loss: 0.84658 | val_0_logloss: 0.85726 |  0:01:36s\n",
      "epoch 53 | loss: 0.84573 | val_0_logloss: 0.85575 |  0:01:38s\n",
      "epoch 54 | loss: 0.84557 | val_0_logloss: 0.85605 |  0:01:40s\n",
      "epoch 55 | loss: 0.845   | val_0_logloss: 0.85234 |  0:01:42s\n",
      "epoch 56 | loss: 0.84355 | val_0_logloss: 0.84988 |  0:01:43s\n",
      "epoch 57 | loss: 0.84549 | val_0_logloss: 0.85491 |  0:01:45s\n",
      "epoch 58 | loss: 0.84499 | val_0_logloss: 0.8545  |  0:01:47s\n",
      "epoch 59 | loss: 0.84451 | val_0_logloss: 0.84793 |  0:01:49s\n",
      "epoch 60 | loss: 0.84364 | val_0_logloss: 0.85127 |  0:01:50s\n",
      "epoch 61 | loss: 0.8438  | val_0_logloss: 0.85133 |  0:01:52s\n",
      "epoch 62 | loss: 0.84289 | val_0_logloss: 0.85059 |  0:01:54s\n",
      "epoch 63 | loss: 0.84394 | val_0_logloss: 0.84632 |  0:01:56s\n",
      "epoch 64 | loss: 0.84319 | val_0_logloss: 0.84528 |  0:01:58s\n",
      "epoch 65 | loss: 0.84291 | val_0_logloss: 0.84585 |  0:02:00s\n",
      "epoch 66 | loss: 0.84164 | val_0_logloss: 0.84646 |  0:02:01s\n",
      "epoch 67 | loss: 0.8427  | val_0_logloss: 0.84645 |  0:02:03s\n",
      "epoch 68 | loss: 0.8407  | val_0_logloss: 0.84804 |  0:02:04s\n",
      "epoch 69 | loss: 0.84338 | val_0_logloss: 0.84693 |  0:02:06s\n",
      "epoch 70 | loss: 0.84207 | val_0_logloss: 0.8436  |  0:02:07s\n",
      "epoch 71 | loss: 0.84353 | val_0_logloss: 0.84663 |  0:02:09s\n",
      "epoch 72 | loss: 0.84179 | val_0_logloss: 0.8465  |  0:02:10s\n",
      "epoch 73 | loss: 0.84136 | val_0_logloss: 0.84516 |  0:02:12s\n",
      "epoch 74 | loss: 0.84098 | val_0_logloss: 0.84442 |  0:02:14s\n",
      "epoch 75 | loss: 0.84187 | val_0_logloss: 0.84676 |  0:02:15s\n",
      "epoch 76 | loss: 0.84009 | val_0_logloss: 0.84347 |  0:02:17s\n",
      "epoch 77 | loss: 0.84123 | val_0_logloss: 0.84499 |  0:02:18s\n",
      "epoch 78 | loss: 0.84034 | val_0_logloss: 0.84424 |  0:02:20s\n",
      "epoch 79 | loss: 0.83956 | val_0_logloss: 0.84537 |  0:02:21s\n",
      "epoch 80 | loss: 0.84032 | val_0_logloss: 0.84811 |  0:02:23s\n",
      "epoch 81 | loss: 0.83898 | val_0_logloss: 0.84316 |  0:02:24s\n",
      "epoch 82 | loss: 0.83793 | val_0_logloss: 0.84097 |  0:02:26s\n",
      "epoch 83 | loss: 0.83827 | val_0_logloss: 0.84467 |  0:02:27s\n",
      "epoch 84 | loss: 0.83739 | val_0_logloss: 0.84347 |  0:02:29s\n",
      "epoch 85 | loss: 0.83823 | val_0_logloss: 0.84195 |  0:02:30s\n",
      "epoch 86 | loss: 0.83872 | val_0_logloss: 0.84272 |  0:02:32s\n",
      "epoch 87 | loss: 0.83793 | val_0_logloss: 0.84495 |  0:02:34s\n",
      "epoch 88 | loss: 0.83777 | val_0_logloss: 0.84223 |  0:02:35s\n",
      "epoch 89 | loss: 0.83759 | val_0_logloss: 0.8415  |  0:02:37s\n",
      "epoch 90 | loss: 0.83682 | val_0_logloss: 0.84203 |  0:02:38s\n",
      "epoch 91 | loss: 0.83555 | val_0_logloss: 0.84181 |  0:02:40s\n",
      "epoch 92 | loss: 0.83576 | val_0_logloss: 0.84281 |  0:02:41s\n",
      "epoch 93 | loss: 0.83738 | val_0_logloss: 0.84066 |  0:02:43s\n",
      "epoch 94 | loss: 0.83581 | val_0_logloss: 0.84035 |  0:02:44s\n",
      "epoch 95 | loss: 0.83539 | val_0_logloss: 0.84392 |  0:02:46s\n",
      "epoch 96 | loss: 0.83555 | val_0_logloss: 0.84247 |  0:02:48s\n",
      "epoch 97 | loss: 0.83583 | val_0_logloss: 0.84265 |  0:02:50s\n",
      "epoch 98 | loss: 0.83475 | val_0_logloss: 0.84055 |  0:02:52s\n",
      "epoch 99 | loss: 0.83394 | val_0_logloss: 0.84037 |  0:02:53s\n",
      "epoch 100| loss: 0.83386 | val_0_logloss: 0.84207 |  0:02:55s\n",
      "epoch 101| loss: 0.83314 | val_0_logloss: 0.84021 |  0:02:57s\n",
      "epoch 102| loss: 0.83435 | val_0_logloss: 0.83913 |  0:02:59s\n",
      "epoch 103| loss: 0.8323  | val_0_logloss: 0.841   |  0:03:01s\n",
      "epoch 104| loss: 0.83328 | val_0_logloss: 0.83857 |  0:03:03s\n",
      "epoch 105| loss: 0.83322 | val_0_logloss: 0.83579 |  0:03:04s\n",
      "epoch 106| loss: 0.83328 | val_0_logloss: 0.83673 |  0:03:06s\n",
      "epoch 107| loss: 0.83359 | val_0_logloss: 0.83949 |  0:03:08s\n",
      "epoch 108| loss: 0.83277 | val_0_logloss: 0.83802 |  0:03:09s\n",
      "epoch 109| loss: 0.83138 | val_0_logloss: 0.83872 |  0:03:11s\n",
      "epoch 110| loss: 0.83121 | val_0_logloss: 0.83847 |  0:03:13s\n",
      "epoch 111| loss: 0.83087 | val_0_logloss: 0.83699 |  0:03:15s\n",
      "epoch 112| loss: 0.83116 | val_0_logloss: 0.83675 |  0:03:16s\n",
      "epoch 113| loss: 0.83045 | val_0_logloss: 0.83684 |  0:03:18s\n",
      "epoch 114| loss: 0.82946 | val_0_logloss: 0.8404  |  0:03:20s\n",
      "epoch 115| loss: 0.8301  | val_0_logloss: 0.84221 |  0:03:21s\n",
      "epoch 116| loss: 0.82973 | val_0_logloss: 0.83781 |  0:03:23s\n",
      "epoch 117| loss: 0.82954 | val_0_logloss: 0.83797 |  0:03:25s\n",
      "epoch 118| loss: 0.83004 | val_0_logloss: 0.83733 |  0:03:26s\n",
      "epoch 119| loss: 0.82959 | val_0_logloss: 0.83732 |  0:03:28s\n",
      "epoch 120| loss: 0.82935 | val_0_logloss: 0.83771 |  0:03:29s\n",
      "epoch 121| loss: 0.82914 | val_0_logloss: 0.83835 |  0:03:31s\n",
      "epoch 122| loss: 0.82888 | val_0_logloss: 0.83918 |  0:03:33s\n",
      "epoch 123| loss: 0.83026 | val_0_logloss: 0.84004 |  0:03:34s\n",
      "epoch 124| loss: 0.82899 | val_0_logloss: 0.84024 |  0:03:36s\n",
      "epoch 125| loss: 0.83004 | val_0_logloss: 0.84063 |  0:03:37s\n",
      "epoch 126| loss: 0.82963 | val_0_logloss: 0.83943 |  0:03:39s\n",
      "epoch 127| loss: 0.82874 | val_0_logloss: 0.83865 |  0:03:41s\n",
      "epoch 128| loss: 0.82802 | val_0_logloss: 0.83726 |  0:03:42s\n",
      "epoch 129| loss: 0.82846 | val_0_logloss: 0.8374  |  0:03:44s\n",
      "epoch 130| loss: 0.82703 | val_0_logloss: 0.83719 |  0:03:46s\n",
      "epoch 131| loss: 0.8265  | val_0_logloss: 0.83718 |  0:03:48s\n",
      "epoch 132| loss: 0.82613 | val_0_logloss: 0.83556 |  0:03:50s\n",
      "epoch 133| loss: 0.82455 | val_0_logloss: 0.83671 |  0:03:52s\n",
      "epoch 134| loss: 0.82545 | val_0_logloss: 0.83788 |  0:03:54s\n",
      "epoch 135| loss: 0.82561 | val_0_logloss: 0.83925 |  0:03:55s\n",
      "epoch 136| loss: 0.8251  | val_0_logloss: 0.8387  |  0:03:57s\n",
      "epoch 137| loss: 0.82443 | val_0_logloss: 0.83843 |  0:03:58s\n",
      "epoch 138| loss: 0.82503 | val_0_logloss: 0.83856 |  0:04:00s\n",
      "epoch 139| loss: 0.82498 | val_0_logloss: 0.83862 |  0:04:02s\n",
      "epoch 140| loss: 0.82553 | val_0_logloss: 0.83766 |  0:04:04s\n",
      "epoch 141| loss: 0.82473 | val_0_logloss: 0.83565 |  0:04:06s\n",
      "epoch 142| loss: 0.82519 | val_0_logloss: 0.83662 |  0:04:08s\n",
      "epoch 143| loss: 0.82545 | val_0_logloss: 0.83692 |  0:04:10s\n",
      "epoch 144| loss: 0.82594 | val_0_logloss: 0.83762 |  0:04:12s\n",
      "epoch 145| loss: 0.82623 | val_0_logloss: 0.83776 |  0:04:14s\n",
      "epoch 146| loss: 0.82554 | val_0_logloss: 0.83765 |  0:04:16s\n",
      "epoch 147| loss: 0.82569 | val_0_logloss: 0.83844 |  0:04:18s\n",
      "epoch 148| loss: 0.82363 | val_0_logloss: 0.83664 |  0:04:20s\n",
      "epoch 149| loss: 0.82345 | val_0_logloss: 0.83501 |  0:04:21s\n",
      "epoch 150| loss: 0.82401 | val_0_logloss: 0.83528 |  0:04:23s\n",
      "epoch 151| loss: 0.82354 | val_0_logloss: 0.83562 |  0:04:25s\n",
      "epoch 152| loss: 0.82248 | val_0_logloss: 0.83565 |  0:04:27s\n",
      "epoch 153| loss: 0.82235 | val_0_logloss: 0.8357  |  0:04:29s\n",
      "epoch 154| loss: 0.82355 | val_0_logloss: 0.83485 |  0:04:31s\n",
      "epoch 155| loss: 0.82443 | val_0_logloss: 0.83545 |  0:04:33s\n",
      "epoch 156| loss: 0.82277 | val_0_logloss: 0.83604 |  0:04:35s\n",
      "epoch 157| loss: 0.82336 | val_0_logloss: 0.83287 |  0:04:37s\n",
      "epoch 158| loss: 0.82111 | val_0_logloss: 0.83263 |  0:04:38s\n",
      "epoch 159| loss: 0.82243 | val_0_logloss: 0.83612 |  0:04:40s\n",
      "epoch 160| loss: 0.82069 | val_0_logloss: 0.83444 |  0:04:42s\n",
      "epoch 161| loss: 0.82119 | val_0_logloss: 0.83339 |  0:04:44s\n",
      "epoch 162| loss: 0.82072 | val_0_logloss: 0.83608 |  0:04:46s\n",
      "epoch 163| loss: 0.8215  | val_0_logloss: 0.83723 |  0:04:48s\n",
      "epoch 164| loss: 0.82073 | val_0_logloss: 0.83649 |  0:04:50s\n",
      "epoch 165| loss: 0.82058 | val_0_logloss: 0.83563 |  0:04:52s\n",
      "epoch 166| loss: 0.82051 | val_0_logloss: 0.8347  |  0:04:54s\n",
      "epoch 167| loss: 0.82075 | val_0_logloss: 0.83393 |  0:04:55s\n",
      "epoch 168| loss: 0.82262 | val_0_logloss: 0.8345  |  0:04:57s\n",
      "epoch 169| loss: 0.82255 | val_0_logloss: 0.83493 |  0:04:59s\n",
      "epoch 170| loss: 0.8232  | val_0_logloss: 0.83406 |  0:05:01s\n",
      "epoch 171| loss: 0.82332 | val_0_logloss: 0.83263 |  0:05:03s\n",
      "epoch 172| loss: 0.82077 | val_0_logloss: 0.83187 |  0:05:04s\n",
      "epoch 173| loss: 0.82172 | val_0_logloss: 0.83062 |  0:05:07s\n",
      "epoch 174| loss: 0.81949 | val_0_logloss: 0.83103 |  0:05:09s\n",
      "epoch 175| loss: 0.81979 | val_0_logloss: 0.83042 |  0:05:11s\n",
      "epoch 176| loss: 0.81846 | val_0_logloss: 0.83069 |  0:05:13s\n",
      "epoch 177| loss: 0.81941 | val_0_logloss: 0.83015 |  0:05:15s\n",
      "epoch 178| loss: 0.81726 | val_0_logloss: 0.83161 |  0:05:16s\n",
      "epoch 179| loss: 0.81798 | val_0_logloss: 0.83156 |  0:05:19s\n",
      "epoch 180| loss: 0.81509 | val_0_logloss: 0.83269 |  0:05:22s\n",
      "epoch 181| loss: 0.81653 | val_0_logloss: 0.83316 |  0:05:24s\n",
      "epoch 182| loss: 0.81459 | val_0_logloss: 0.83333 |  0:05:25s\n",
      "epoch 183| loss: 0.81327 | val_0_logloss: 0.83419 |  0:05:27s\n",
      "epoch 184| loss: 0.81411 | val_0_logloss: 0.83364 |  0:05:29s\n",
      "epoch 185| loss: 0.81468 | val_0_logloss: 0.83559 |  0:05:31s\n",
      "epoch 186| loss: 0.81479 | val_0_logloss: 0.83611 |  0:05:32s\n",
      "epoch 187| loss: 0.81461 | val_0_logloss: 0.83537 |  0:05:34s\n",
      "epoch 188| loss: 0.81425 | val_0_logloss: 0.83308 |  0:05:35s\n",
      "epoch 189| loss: 0.81355 | val_0_logloss: 0.83211 |  0:05:37s\n",
      "epoch 190| loss: 0.8142  | val_0_logloss: 0.83127 |  0:05:39s\n",
      "epoch 191| loss: 0.8142  | val_0_logloss: 0.83209 |  0:05:41s\n",
      "epoch 192| loss: 0.81446 | val_0_logloss: 0.83233 |  0:05:42s\n",
      "epoch 193| loss: 0.81492 | val_0_logloss: 0.8339  |  0:05:44s\n",
      "epoch 194| loss: 0.81285 | val_0_logloss: 0.83596 |  0:05:47s\n",
      "epoch 195| loss: 0.81361 | val_0_logloss: 0.83667 |  0:05:48s\n",
      "epoch 196| loss: 0.81581 | val_0_logloss: 0.83613 |  0:05:50s\n",
      "epoch 197| loss: 0.81748 | val_0_logloss: 0.83373 |  0:05:52s\n",
      "epoch 198| loss: 0.81409 | val_0_logloss: 0.83172 |  0:05:54s\n",
      "epoch 199| loss: 0.81485 | val_0_logloss: 0.83159 |  0:05:56s\n",
      "epoch 200| loss: 0.81399 | val_0_logloss: 0.83113 |  0:05:58s\n",
      "epoch 201| loss: 0.81406 | val_0_logloss: 0.83202 |  0:05:59s\n",
      "epoch 202| loss: 0.81707 | val_0_logloss: 0.8318  |  0:06:01s\n",
      "epoch 203| loss: 0.8129  | val_0_logloss: 0.82945 |  0:06:03s\n",
      "epoch 204| loss: 0.81235 | val_0_logloss: 0.83074 |  0:06:05s\n",
      "epoch 205| loss: 0.81121 | val_0_logloss: 0.83162 |  0:06:06s\n",
      "epoch 206| loss: 0.81173 | val_0_logloss: 0.83267 |  0:06:08s\n",
      "epoch 207| loss: 0.81054 | val_0_logloss: 0.83348 |  0:06:10s\n",
      "epoch 208| loss: 0.81001 | val_0_logloss: 0.83439 |  0:06:11s\n",
      "epoch 209| loss: 0.80958 | val_0_logloss: 0.83479 |  0:06:13s\n",
      "epoch 210| loss: 0.80963 | val_0_logloss: 0.83406 |  0:06:15s\n",
      "epoch 211| loss: 0.80909 | val_0_logloss: 0.8337  |  0:06:17s\n",
      "epoch 212| loss: 0.80825 | val_0_logloss: 0.83286 |  0:06:19s\n",
      "epoch 213| loss: 0.80788 | val_0_logloss: 0.83183 |  0:06:21s\n",
      "epoch 214| loss: 0.80786 | val_0_logloss: 0.8316  |  0:06:23s\n",
      "epoch 215| loss: 0.80874 | val_0_logloss: 0.83376 |  0:06:25s\n",
      "epoch 216| loss: 0.8099  | val_0_logloss: 0.83573 |  0:06:27s\n",
      "epoch 217| loss: 0.81005 | val_0_logloss: 0.83578 |  0:06:29s\n",
      "epoch 218| loss: 0.80854 | val_0_logloss: 0.83421 |  0:06:31s\n",
      "epoch 219| loss: 0.80972 | val_0_logloss: 0.83469 |  0:06:33s\n",
      "epoch 220| loss: 0.80941 | val_0_logloss: 0.83307 |  0:06:35s\n",
      "epoch 221| loss: 0.80843 | val_0_logloss: 0.83118 |  0:06:36s\n",
      "epoch 222| loss: 0.80686 | val_0_logloss: 0.82963 |  0:06:39s\n",
      "epoch 223| loss: 0.80715 | val_0_logloss: 0.8293  |  0:06:41s\n",
      "epoch 224| loss: 0.80599 | val_0_logloss: 0.82955 |  0:06:43s\n",
      "epoch 225| loss: 0.80614 | val_0_logloss: 0.83039 |  0:06:45s\n",
      "epoch 226| loss: 0.80351 | val_0_logloss: 0.83003 |  0:06:47s\n",
      "epoch 227| loss: 0.80587 | val_0_logloss: 0.83031 |  0:06:48s\n",
      "epoch 228| loss: 0.80555 | val_0_logloss: 0.83062 |  0:06:50s\n",
      "epoch 229| loss: 0.80328 | val_0_logloss: 0.83121 |  0:06:51s\n",
      "epoch 230| loss: 0.80322 | val_0_logloss: 0.8316  |  0:06:54s\n",
      "epoch 231| loss: 0.80262 | val_0_logloss: 0.83273 |  0:06:56s\n",
      "epoch 232| loss: 0.80273 | val_0_logloss: 0.83086 |  0:06:58s\n",
      "epoch 233| loss: 0.8025  | val_0_logloss: 0.83133 |  0:07:00s\n",
      "epoch 234| loss: 0.80358 | val_0_logloss: 0.83168 |  0:07:02s\n",
      "epoch 235| loss: 0.80011 | val_0_logloss: 0.83263 |  0:07:03s\n",
      "epoch 236| loss: 0.80229 | val_0_logloss: 0.83411 |  0:07:05s\n",
      "epoch 237| loss: 0.79933 | val_0_logloss: 0.83495 |  0:07:07s\n",
      "epoch 238| loss: 0.79899 | val_0_logloss: 0.83564 |  0:07:09s\n",
      "epoch 239| loss: 0.79793 | val_0_logloss: 0.83461 |  0:07:10s\n",
      "epoch 240| loss: 0.79778 | val_0_logloss: 0.8342  |  0:07:12s\n",
      "epoch 241| loss: 0.79797 | val_0_logloss: 0.83406 |  0:07:14s\n",
      "epoch 242| loss: 0.7968  | val_0_logloss: 0.83445 |  0:07:15s\n",
      "epoch 243| loss: 0.79579 | val_0_logloss: 0.83423 |  0:07:17s\n",
      "epoch 244| loss: 0.79426 | val_0_logloss: 0.83294 |  0:07:18s\n",
      "epoch 245| loss: 0.79441 | val_0_logloss: 0.8302  |  0:07:20s\n",
      "epoch 246| loss: 0.7938  | val_0_logloss: 0.83049 |  0:07:22s\n",
      "epoch 247| loss: 0.79489 | val_0_logloss: 0.83022 |  0:07:23s\n",
      "epoch 248| loss: 0.79418 | val_0_logloss: 0.83135 |  0:07:25s\n",
      "epoch 249| loss: 0.7933  | val_0_logloss: 0.83059 |  0:07:27s\n",
      "epoch 250| loss: 0.79331 | val_0_logloss: 0.83206 |  0:07:28s\n",
      "epoch 251| loss: 0.79151 | val_0_logloss: 0.83325 |  0:07:30s\n",
      "epoch 252| loss: 0.79164 | val_0_logloss: 0.83431 |  0:07:33s\n",
      "epoch 253| loss: 0.79137 | val_0_logloss: 0.83422 |  0:07:35s\n",
      "epoch 254| loss: 0.79156 | val_0_logloss: 0.8329  |  0:07:37s\n",
      "epoch 255| loss: 0.79198 | val_0_logloss: 0.83231 |  0:07:39s\n",
      "epoch 256| loss: 0.78928 | val_0_logloss: 0.83175 |  0:07:41s\n",
      "epoch 257| loss: 0.79067 | val_0_logloss: 0.83366 |  0:07:43s\n",
      "epoch 258| loss: 0.79048 | val_0_logloss: 0.83617 |  0:07:45s\n",
      "epoch 259| loss: 0.79087 | val_0_logloss: 0.83634 |  0:07:47s\n",
      "epoch 260| loss: 0.79208 | val_0_logloss: 0.83487 |  0:07:48s\n",
      "epoch 261| loss: 0.7894  | val_0_logloss: 0.83249 |  0:07:50s\n",
      "epoch 262| loss: 0.78994 | val_0_logloss: 0.83371 |  0:07:52s\n",
      "epoch 263| loss: 0.79015 | val_0_logloss: 0.83044 |  0:07:54s\n",
      "epoch 264| loss: 0.78844 | val_0_logloss: 0.82991 |  0:07:56s\n",
      "epoch 265| loss: 0.78778 | val_0_logloss: 0.83058 |  0:07:58s\n",
      "epoch 266| loss: 0.78808 | val_0_logloss: 0.83272 |  0:08:00s\n",
      "epoch 267| loss: 0.78797 | val_0_logloss: 0.83351 |  0:08:02s\n",
      "epoch 268| loss: 0.78568 | val_0_logloss: 0.83464 |  0:08:04s\n",
      "epoch 269| loss: 0.78489 | val_0_logloss: 0.83314 |  0:08:05s\n",
      "epoch 270| loss: 0.78497 | val_0_logloss: 0.83079 |  0:08:08s\n",
      "epoch 271| loss: 0.78441 | val_0_logloss: 0.83068 |  0:08:11s\n",
      "epoch 272| loss: 0.78233 | val_0_logloss: 0.83185 |  0:08:14s\n",
      "epoch 273| loss: 0.7867  | val_0_logloss: 0.83034 |  0:08:17s\n",
      "epoch 274| loss: 0.78569 | val_0_logloss: 0.83229 |  0:08:20s\n",
      "epoch 275| loss: 0.78385 | val_0_logloss: 0.8319  |  0:08:22s\n",
      "epoch 276| loss: 0.78353 | val_0_logloss: 0.8313  |  0:08:25s\n",
      "epoch 277| loss: 0.78464 | val_0_logloss: 0.83314 |  0:08:28s\n",
      "epoch 278| loss: 0.78309 | val_0_logloss: 0.83316 |  0:08:30s\n",
      "epoch 279| loss: 0.78236 | val_0_logloss: 0.83085 |  0:08:32s\n",
      "epoch 280| loss: 0.78442 | val_0_logloss: 0.83119 |  0:08:33s\n",
      "epoch 281| loss: 0.78083 | val_0_logloss: 0.83079 |  0:08:35s\n",
      "epoch 282| loss: 0.7822  | val_0_logloss: 0.8308  |  0:08:37s\n",
      "epoch 283| loss: 0.78339 | val_0_logloss: 0.83052 |  0:08:39s\n",
      "\n",
      "Early stopping occurred at epoch 283 with best_epoch = 223 and best_val_0_logloss = 0.8293\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.multitask import TabNetMultiTaskClassifier\n",
    "\n",
    "clf = TabNetMultiTaskClassifier(\n",
    "    n_d=16, n_a=16, n_steps=5,\n",
    "    lambda_sparse=1e-2,\n",
    "    gamma = 1.3,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=2e-2),\n",
    "    scheduler_params = {\"gamma\": 0.95, \"step_size\": 20},\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    mask_type='entmax',\n",
    "    device_name='cuda'\n",
    ")\n",
    "clf.fit(\n",
    "    X_train = X_train.values, y_train = np.array(Y_train).reshape(Y_train.shape[0],1),\n",
    "    eval_set = [(X_val.values, np.array(Y_val).reshape(Y_val.shape[0],1))],\n",
    "    max_epochs=300,\n",
    "    patience=60,\n",
    "    batch_size=8192,\n",
    "    virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x275398d7460>]"
      ]
     },
     "metadata": {},
     "execution_count": 51
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 378.465625 248.518125\" width=\"378.465625pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-05-11T01:13:41.756956</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.4.1, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 378.465625 248.518125 \r\nL 378.465625 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 371.265625 224.64 \r\nL 371.265625 7.2 \r\nL 36.465625 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m6e8954b22a\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.683807\" xlink:href=\"#m6e8954b22a\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(48.502557 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 4250 \r\nQ 1547 4250 1301 3770 \r\nQ 1056 3291 1056 2328 \r\nQ 1056 1369 1301 889 \r\nQ 1547 409 2034 409 \r\nQ 2525 409 2770 889 \r\nQ 3016 1369 3016 2328 \r\nQ 3016 3291 2770 3770 \r\nQ 2525 4250 2034 4250 \r\nz\r\nM 2034 4750 \r\nQ 2819 4750 3233 4129 \r\nQ 3647 3509 3647 2328 \r\nQ 3647 1150 3233 529 \r\nQ 2819 -91 2034 -91 \r\nQ 1250 -91 836 529 \r\nQ 422 1150 422 2328 \r\nQ 422 3509 836 4129 \r\nQ 1250 4750 2034 4750 \r\nz\r\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"106.425468\" xlink:href=\"#m6e8954b22a\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 50 -->\r\n      <g transform=\"translate(100.062968 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 691 4666 \r\nL 3169 4666 \r\nL 3169 4134 \r\nL 1269 4134 \r\nL 1269 2991 \r\nQ 1406 3038 1543 3061 \r\nQ 1681 3084 1819 3084 \r\nQ 2600 3084 3056 2656 \r\nQ 3513 2228 3513 1497 \r\nQ 3513 744 3044 326 \r\nQ 2575 -91 1722 -91 \r\nQ 1428 -91 1123 -41 \r\nQ 819 9 494 109 \r\nL 494 744 \r\nQ 775 591 1075 516 \r\nQ 1375 441 1709 441 \r\nQ 2250 441 2565 725 \r\nQ 2881 1009 2881 1497 \r\nQ 2881 1984 2565 2268 \r\nQ 2250 2553 1709 2553 \r\nQ 1456 2553 1204 2497 \r\nQ 953 2441 691 2322 \r\nL 691 4666 \r\nz\r\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"161.167129\" xlink:href=\"#m6e8954b22a\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 100 -->\r\n      <g transform=\"translate(151.623379 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 794 531 \r\nL 1825 531 \r\nL 1825 4091 \r\nL 703 3866 \r\nL 703 4441 \r\nL 1819 4666 \r\nL 2450 4666 \r\nL 2450 531 \r\nL 3481 531 \r\nL 3481 0 \r\nL 794 0 \r\nL 794 531 \r\nz\r\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"215.90879\" xlink:href=\"#m6e8954b22a\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 150 -->\r\n      <g transform=\"translate(206.36504 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"270.650452\" xlink:href=\"#m6e8954b22a\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 200 -->\r\n      <g transform=\"translate(261.106702 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 1228 531 \r\nL 3431 531 \r\nL 3431 0 \r\nL 469 0 \r\nL 469 531 \r\nQ 828 903 1448 1529 \r\nQ 2069 2156 2228 2338 \r\nQ 2531 2678 2651 2914 \r\nQ 2772 3150 2772 3378 \r\nQ 2772 3750 2511 3984 \r\nQ 2250 4219 1831 4219 \r\nQ 1534 4219 1204 4116 \r\nQ 875 4013 500 3803 \r\nL 500 4441 \r\nQ 881 4594 1212 4672 \r\nQ 1544 4750 1819 4750 \r\nQ 2544 4750 2975 4387 \r\nQ 3406 4025 3406 3419 \r\nQ 3406 3131 3298 2873 \r\nQ 3191 2616 2906 2266 \r\nQ 2828 2175 2409 1742 \r\nQ 1991 1309 1228 531 \r\nz\r\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"325.392113\" xlink:href=\"#m6e8954b22a\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 250 -->\r\n      <g transform=\"translate(315.848363 239.238438)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-35\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m22428ae0b9\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m22428ae0b9\" y=\"196.958514\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0.80 -->\r\n      <g transform=\"translate(7.2 200.757733)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 684 794 \r\nL 1344 794 \r\nL 1344 0 \r\nL 684 0 \r\nL 684 794 \r\nz\r\n\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\r\n        <path d=\"M 2034 2216 \r\nQ 1584 2216 1326 1975 \r\nQ 1069 1734 1069 1313 \r\nQ 1069 891 1326 650 \r\nQ 1584 409 2034 409 \r\nQ 2484 409 2743 651 \r\nQ 3003 894 3003 1313 \r\nQ 3003 1734 2745 1975 \r\nQ 2488 2216 2034 2216 \r\nz\r\nM 1403 2484 \r\nQ 997 2584 770 2862 \r\nQ 544 3141 544 3541 \r\nQ 544 4100 942 4425 \r\nQ 1341 4750 2034 4750 \r\nQ 2731 4750 3128 4425 \r\nQ 3525 4100 3525 3541 \r\nQ 3525 3141 3298 2862 \r\nQ 3072 2584 2669 2484 \r\nQ 3125 2378 3379 2068 \r\nQ 3634 1759 3634 1313 \r\nQ 3634 634 3220 271 \r\nQ 2806 -91 2034 -91 \r\nQ 1263 -91 848 271 \r\nQ 434 634 434 1313 \r\nQ 434 1759 690 2068 \r\nQ 947 2378 1403 2484 \r\nz\r\nM 1172 3481 \r\nQ 1172 3119 1398 2916 \r\nQ 1625 2713 2034 2713 \r\nQ 2441 2713 2670 2916 \r\nQ 2900 3119 2900 3481 \r\nQ 2900 3844 2670 4047 \r\nQ 2441 4250 2034 4250 \r\nQ 1625 4250 1398 4047 \r\nQ 1172 3844 1172 3481 \r\nz\r\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m22428ae0b9\" y=\"150.545599\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.85 -->\r\n      <g transform=\"translate(7.2 154.344818)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m22428ae0b9\" y=\"104.132684\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.90 -->\r\n      <g transform=\"translate(7.2 107.931903)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 703 97 \r\nL 703 672 \r\nQ 941 559 1184 500 \r\nQ 1428 441 1663 441 \r\nQ 2288 441 2617 861 \r\nQ 2947 1281 2994 2138 \r\nQ 2813 1869 2534 1725 \r\nQ 2256 1581 1919 1581 \r\nQ 1219 1581 811 2004 \r\nQ 403 2428 403 3163 \r\nQ 403 3881 828 4315 \r\nQ 1253 4750 1959 4750 \r\nQ 2769 4750 3195 4129 \r\nQ 3622 3509 3622 2328 \r\nQ 3622 1225 3098 567 \r\nQ 2575 -91 1691 -91 \r\nQ 1453 -91 1209 -44 \r\nQ 966 3 703 97 \r\nz\r\nM 1959 2075 \r\nQ 2384 2075 2632 2365 \r\nQ 2881 2656 2881 3163 \r\nQ 2881 3666 2632 3958 \r\nQ 2384 4250 1959 4250 \r\nQ 1534 4250 1286 3958 \r\nQ 1038 3666 1038 3163 \r\nQ 1038 2656 1286 2365 \r\nQ 1534 2075 1959 2075 \r\nz\r\n\" id=\"DejaVuSans-39\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-39\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m22428ae0b9\" y=\"57.71977\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.95 -->\r\n      <g transform=\"translate(7.2 61.518988)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-39\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m22428ae0b9\" y=\"11.306855\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 1.00 -->\r\n      <g transform=\"translate(7.2 15.106074)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_12\">\r\n    <path clip-path=\"url(#paca74df3fa)\" d=\"M 51.683807 87.888312 \r\nL 52.77864 98.08216 \r\nL 53.873473 106.691575 \r\nL 57.157973 116.559969 \r\nL 58.252806 118.151891 \r\nL 59.347639 118.851791 \r\nL 60.442473 121.676411 \r\nL 61.537306 123.085144 \r\nL 62.632139 121.867924 \r\nL 63.726972 124.171822 \r\nL 64.821806 126.128344 \r\nL 65.916639 127.634106 \r\nL 67.011472 127.981881 \r\nL 68.106305 130.096808 \r\nL 69.201138 130.098405 \r\nL 70.295972 133.606555 \r\nL 72.485638 135.019338 \r\nL 73.580471 136.845268 \r\nL 74.675305 136.330347 \r\nL 75.770138 139.342518 \r\nL 76.864971 139.634268 \r\nL 77.959804 140.440129 \r\nL 79.054637 140.773241 \r\nL 80.149471 141.901286 \r\nL 81.244304 144.81769 \r\nL 82.339137 143.365592 \r\nL 83.43397 144.784432 \r\nL 84.528804 145.986261 \r\nL 85.623637 146.774718 \r\nL 86.71847 149.886625 \r\nL 87.813303 148.843271 \r\nL 88.908136 150.242682 \r\nL 90.00297 150.421028 \r\nL 91.097803 149.673851 \r\nL 92.192636 150.20659 \r\nL 93.287469 151.809869 \r\nL 94.382303 151.645357 \r\nL 95.477136 152.42926 \r\nL 96.571969 152.21668 \r\nL 97.666802 151.537409 \r\nL 98.761635 152.18913 \r\nL 99.856469 152.33632 \r\nL 102.046135 153.291437 \r\nL 103.140968 153.718057 \r\nL 104.235802 154.509541 \r\nL 105.330635 154.657943 \r\nL 106.425468 155.190399 \r\nL 107.520301 156.532439 \r\nL 108.615134 154.728755 \r\nL 110.804801 155.642619 \r\nL 111.899634 156.449474 \r\nL 112.994467 156.303642 \r\nL 114.089301 157.147751 \r\nL 115.184134 156.167533 \r\nL 116.278967 156.863478 \r\nL 117.3738 157.130905 \r\nL 118.468634 158.304263 \r\nL 119.563467 157.324176 \r\nL 120.6583 159.174253 \r\nL 121.753133 156.688976 \r\nL 122.847966 157.906212 \r\nL 123.9428 156.554299 \r\nL 125.037633 158.165719 \r\nL 127.227299 158.915862 \r\nL 128.322133 158.092216 \r\nL 129.416966 159.742124 \r\nL 130.511799 158.686731 \r\nL 132.701465 160.236369 \r\nL 133.796299 159.529839 \r\nL 134.891132 160.774514 \r\nL 135.985965 161.749499 \r\nL 137.080798 161.429994 \r\nL 138.175632 162.249767 \r\nL 139.270465 161.469046 \r\nL 140.365298 161.016003 \r\nL 141.460131 161.752817 \r\nL 143.649798 162.060912 \r\nL 144.744631 162.783192 \r\nL 145.839464 163.963253 \r\nL 146.934297 163.767316 \r\nL 148.029131 162.255908 \r\nL 149.123964 163.713591 \r\nL 150.218797 164.107884 \r\nL 151.31363 163.958372 \r\nL 152.408463 163.696451 \r\nL 153.503297 164.701836 \r\nL 154.59813 165.457141 \r\nL 155.692963 165.524966 \r\nL 156.787796 166.198795 \r\nL 157.88263 165.074711 \r\nL 158.977463 166.980229 \r\nL 160.072296 166.066455 \r\nL 161.167129 166.125007 \r\nL 162.261962 166.065931 \r\nL 163.356796 165.775405 \r\nL 164.451629 166.541704 \r\nL 165.546462 167.826194 \r\nL 166.641295 167.987587 \r\nL 167.736129 168.304095 \r\nL 168.830962 168.03124 \r\nL 169.925795 168.697047 \r\nL 171.020628 169.60977 \r\nL 172.115461 169.018652 \r\nL 173.210295 169.361881 \r\nL 174.305128 169.536021 \r\nL 175.399961 169.076605 \r\nL 176.494794 169.491261 \r\nL 179.779294 170.151303 \r\nL 180.874127 168.869122 \r\nL 181.968961 170.048387 \r\nL 183.063794 169.070573 \r\nL 184.158627 169.456533 \r\nL 185.25346 170.276036 \r\nL 186.348293 170.952428 \r\nL 187.443127 170.543345 \r\nL 188.53796 171.871558 \r\nL 189.632793 172.361501 \r\nL 190.727626 172.700626 \r\nL 191.82246 174.168211 \r\nL 192.917293 173.332302 \r\nL 194.012126 173.183943 \r\nL 195.106959 173.657258 \r\nL 196.201792 174.281816 \r\nL 197.296626 173.727275 \r\nL 198.391459 173.774027 \r\nL 199.486292 173.256302 \r\nL 200.581125 174.007046 \r\nL 201.675959 173.573347 \r\nL 202.770792 173.330083 \r\nL 203.865625 172.8774 \r\nL 204.960458 172.609516 \r\nL 206.055291 173.248216 \r\nL 207.150125 173.109417 \r\nL 208.244958 175.023662 \r\nL 209.339791 175.193613 \r\nL 210.434624 174.670001 \r\nL 211.529458 175.109341 \r\nL 212.624291 176.094711 \r\nL 213.719124 176.20763 \r\nL 214.813957 175.099296 \r\nL 215.90879 174.282493 \r\nL 217.003624 175.820368 \r\nL 218.098457 175.273241 \r\nL 219.19329 177.367521 \r\nL 220.288123 176.141948 \r\nL 221.382957 177.753365 \r\nL 222.47779 177.291247 \r\nL 223.572623 177.723324 \r\nL 224.667456 177.002499 \r\nL 225.762289 177.713822 \r\nL 227.951956 177.920558 \r\nL 229.046789 177.698711 \r\nL 230.141622 175.964137 \r\nL 231.236456 176.023786 \r\nL 232.331289 175.425189 \r\nL 233.426122 175.308945 \r\nL 234.520955 177.677791 \r\nL 235.615789 176.800053 \r\nL 236.710622 178.867945 \r\nL 237.805455 178.591695 \r\nL 238.900288 179.820303 \r\nL 239.995121 178.942132 \r\nL 241.089955 180.94064 \r\nL 242.184788 180.270117 \r\nL 243.279621 182.952095 \r\nL 244.374454 181.613652 \r\nL 245.469288 183.412286 \r\nL 246.564121 184.644599 \r\nL 247.658954 183.863122 \r\nL 248.753787 183.335848 \r\nL 249.84862 183.231657 \r\nL 250.943454 183.39877 \r\nL 252.038287 183.731172 \r\nL 253.13312 184.385035 \r\nL 254.227953 183.779859 \r\nL 255.322787 183.78028 \r\nL 256.41762 183.535486 \r\nL 257.512453 183.108064 \r\nL 258.607286 185.026716 \r\nL 259.702119 184.325155 \r\nL 260.796953 182.285648 \r\nL 261.891786 180.733537 \r\nL 262.986619 183.879666 \r\nL 264.081452 183.178006 \r\nL 265.176286 183.974556 \r\nL 266.271119 183.90878 \r\nL 267.365952 181.115021 \r\nL 268.460785 184.988162 \r\nL 269.555618 185.496421 \r\nL 270.650452 186.550892 \r\nL 271.745285 186.071252 \r\nL 272.840118 187.178174 \r\nL 275.029785 188.065355 \r\nL 276.124618 188.021097 \r\nL 277.219451 188.522361 \r\nL 278.314284 189.29641 \r\nL 279.409117 189.640414 \r\nL 280.503951 189.662665 \r\nL 281.598784 188.850082 \r\nL 282.693617 187.766467 \r\nL 283.78845 187.628204 \r\nL 284.883284 189.034958 \r\nL 285.978117 187.936509 \r\nL 287.07295 188.224966 \r\nL 288.167783 189.131331 \r\nL 289.262616 190.591292 \r\nL 290.35745 190.325326 \r\nL 291.452283 191.402347 \r\nL 292.547116 191.261458 \r\nL 293.641949 193.698908 \r\nL 294.736783 191.512039 \r\nL 295.831616 191.802061 \r\nL 296.926449 193.912826 \r\nL 298.021282 193.973577 \r\nL 299.116116 194.522948 \r\nL 300.210949 194.421447 \r\nL 301.305782 194.640981 \r\nL 302.400615 193.633724 \r\nL 303.495448 196.852434 \r\nL 304.590282 194.829278 \r\nL 305.685115 197.583472 \r\nL 306.779948 197.898642 \r\nL 307.874781 198.880873 \r\nL 308.969615 199.015201 \r\nL 310.064448 198.84388 \r\nL 312.254114 200.864485 \r\nL 313.348947 202.28439 \r\nL 314.443781 202.147009 \r\nL 315.538614 202.709372 \r\nL 316.633447 201.703829 \r\nL 317.72828 202.364678 \r\nL 318.823114 203.17533 \r\nL 319.917947 203.166223 \r\nL 321.01278 204.83691 \r\nL 322.107613 204.722777 \r\nL 323.202446 204.973082 \r\nL 324.29728 204.793031 \r\nL 325.392113 204.400702 \r\nL 326.486946 206.909648 \r\nL 327.581779 205.621426 \r\nL 328.676613 205.796352 \r\nL 329.771446 205.437397 \r\nL 330.866279 204.308484 \r\nL 331.961112 206.79621 \r\nL 333.055945 206.292386 \r\nL 334.150779 206.097254 \r\nL 335.245612 207.691559 \r\nL 336.340445 208.302673 \r\nL 337.435278 208.026559 \r\nL 338.530112 208.127135 \r\nL 339.624945 210.247944 \r\nL 340.719778 210.984491 \r\nL 341.814611 210.914231 \r\nL 342.909444 211.42559 \r\nL 344.004278 213.359638 \r\nL 345.099111 209.302874 \r\nL 346.193944 210.240487 \r\nL 347.288777 211.948658 \r\nL 348.383611 212.249334 \r\nL 349.478444 211.217316 \r\nL 350.573277 212.652475 \r\nL 351.66811 213.332056 \r\nL 352.762944 211.4185 \r\nL 353.857777 214.756364 \r\nL 356.047443 212.372477 \r\nL 356.047443 212.372477 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_13\">\r\n    <path clip-path=\"url(#paca74df3fa)\" d=\"M 51.683807 27.841103 \r\nL 52.77864 19.925871 \r\nL 53.873473 17.083636 \r\nL 54.968306 28.595824 \r\nL 56.06314 47.360313 \r\nL 58.252806 70.414868 \r\nL 59.347639 82.95539 \r\nL 60.442473 89.077773 \r\nL 61.537306 92.964938 \r\nL 62.632139 100.791577 \r\nL 63.726972 105.348417 \r\nL 65.916639 116.64266 \r\nL 67.011472 120.565731 \r\nL 68.106305 121.902583 \r\nL 69.201138 123.049513 \r\nL 70.295972 125.477027 \r\nL 71.390805 130.027129 \r\nL 72.485638 131.03447 \r\nL 73.580471 131.49295 \r\nL 74.675305 132.89139 \r\nL 75.770138 132.633366 \r\nL 76.864971 133.92327 \r\nL 77.959804 135.396733 \r\nL 79.054637 130.592049 \r\nL 80.149471 138.65087 \r\nL 81.244304 138.57677 \r\nL 82.339137 138.874857 \r\nL 83.43397 140.327647 \r\nL 84.528804 139.579332 \r\nL 85.623637 142.499114 \r\nL 86.71847 139.272491 \r\nL 88.908136 138.498579 \r\nL 90.00297 133.508366 \r\nL 91.097803 135.889637 \r\nL 92.192636 141.074885 \r\nL 93.287469 138.017966 \r\nL 94.382303 137.604263 \r\nL 95.477136 142.220555 \r\nL 96.571969 136.470882 \r\nL 97.666802 135.757939 \r\nL 98.761635 137.458087 \r\nL 99.856469 141.545281 \r\nL 100.951302 139.619884 \r\nL 102.046135 136.881059 \r\nL 103.140968 143.803477 \r\nL 104.235802 145.209729 \r\nL 105.330635 144.932199 \r\nL 106.425468 148.373642 \r\nL 107.520301 150.659068 \r\nL 108.615134 145.987765 \r\nL 109.709968 146.367993 \r\nL 110.804801 152.468862 \r\nL 111.899634 149.368354 \r\nL 112.994467 149.3112 \r\nL 114.089301 150.001753 \r\nL 115.184134 153.961531 \r\nL 116.278967 154.93065 \r\nL 118.468634 153.831145 \r\nL 119.563467 153.840925 \r\nL 120.6583 152.360505 \r\nL 121.753133 153.395401 \r\nL 122.847966 156.484509 \r\nL 123.9428 153.669787 \r\nL 125.037633 153.793433 \r\nL 126.132466 155.03826 \r\nL 127.227299 155.722543 \r\nL 128.322133 153.551393 \r\nL 129.416966 156.60325 \r\nL 130.511799 155.194606 \r\nL 131.606632 155.893059 \r\nL 132.701465 154.847011 \r\nL 133.796299 152.297164 \r\nL 134.891132 156.892458 \r\nL 135.985965 158.928722 \r\nL 137.080798 155.496314 \r\nL 138.175632 156.603219 \r\nL 139.270465 158.020704 \r\nL 140.365298 157.301503 \r\nL 141.460131 155.232771 \r\nL 142.554964 157.756246 \r\nL 143.649798 158.43456 \r\nL 144.744631 157.945829 \r\nL 145.839464 158.149526 \r\nL 146.934297 157.216795 \r\nL 148.029131 159.21134 \r\nL 149.123964 159.499997 \r\nL 150.218797 156.191973 \r\nL 151.31363 157.537323 \r\nL 152.408463 157.367274 \r\nL 153.503297 159.316997 \r\nL 154.59813 159.484967 \r\nL 155.692963 157.908579 \r\nL 156.787796 159.633208 \r\nL 157.88263 160.634116 \r\nL 158.977463 158.898535 \r\nL 160.072296 161.158842 \r\nL 161.167129 163.739033 \r\nL 162.261962 162.864419 \r\nL 163.356796 160.300999 \r\nL 164.451629 161.668756 \r\nL 165.546462 161.020405 \r\nL 166.641295 161.252391 \r\nL 167.736129 162.620433 \r\nL 168.830962 162.84914 \r\nL 169.925795 162.760669 \r\nL 171.020628 159.460988 \r\nL 172.115461 157.778868 \r\nL 173.210295 161.856946 \r\nL 174.305128 161.710711 \r\nL 175.399961 162.304287 \r\nL 176.494794 162.311993 \r\nL 177.589628 161.955948 \r\nL 178.684461 161.363981 \r\nL 180.874127 159.788284 \r\nL 181.968961 159.602862 \r\nL 183.063794 159.244747 \r\nL 184.158627 160.360964 \r\nL 185.25346 161.081435 \r\nL 186.348293 162.376133 \r\nL 187.443127 162.246082 \r\nL 188.53796 162.437463 \r\nL 189.632793 162.441267 \r\nL 190.727626 163.947826 \r\nL 192.917293 161.794793 \r\nL 194.012126 160.528862 \r\nL 195.106959 161.033928 \r\nL 196.201792 161.281951 \r\nL 198.391459 161.111474 \r\nL 199.486292 161.995965 \r\nL 200.581125 163.869592 \r\nL 201.675959 162.969308 \r\nL 202.770792 162.691435 \r\nL 203.865625 162.038288 \r\nL 204.960458 161.907368 \r\nL 206.055291 162.014152 \r\nL 207.150125 161.273734 \r\nL 209.339791 164.456582 \r\nL 211.529458 163.897883 \r\nL 213.719124 163.820668 \r\nL 214.813957 164.612857 \r\nL 217.003624 163.508028 \r\nL 218.098457 166.450807 \r\nL 219.19329 166.6658 \r\nL 220.288123 163.428123 \r\nL 221.382957 164.989708 \r\nL 222.47779 165.966601 \r\nL 223.572623 163.468017 \r\nL 224.667456 162.396808 \r\nL 226.857123 163.885845 \r\nL 227.951956 164.746171 \r\nL 229.046789 165.465139 \r\nL 230.141622 164.930781 \r\nL 231.236456 164.529934 \r\nL 232.331289 165.343988 \r\nL 233.426122 166.669272 \r\nL 234.520955 167.378242 \r\nL 235.615789 168.5334 \r\nL 236.710622 168.158993 \r\nL 237.805455 168.718559 \r\nL 238.900288 168.473565 \r\nL 239.995121 168.970428 \r\nL 241.089955 167.613066 \r\nL 242.184788 167.664069 \r\nL 243.279621 166.617214 \r\nL 244.374454 166.172837 \r\nL 245.469288 166.021224 \r\nL 246.564121 165.218956 \r\nL 247.658954 165.731077 \r\nL 248.753787 163.924467 \r\nL 249.84862 163.43618 \r\nL 250.943454 164.126242 \r\nL 252.038287 166.250268 \r\nL 254.227953 167.934649 \r\nL 255.322787 167.173807 \r\nL 256.41762 166.948649 \r\nL 257.512453 165.490592 \r\nL 258.607286 163.577323 \r\nL 259.702119 162.918396 \r\nL 260.796953 163.41807 \r\nL 261.891786 165.646421 \r\nL 262.986619 167.511884 \r\nL 264.081452 167.63757 \r\nL 265.176286 168.062208 \r\nL 266.271119 167.237295 \r\nL 267.365952 167.443681 \r\nL 268.460785 169.621854 \r\nL 269.555618 168.425143 \r\nL 270.650452 167.607218 \r\nL 271.745285 166.630912 \r\nL 273.934951 165.03629 \r\nL 275.029785 164.660273 \r\nL 276.124618 165.340719 \r\nL 277.219451 165.679069 \r\nL 278.314284 166.460435 \r\nL 279.409117 167.416469 \r\nL 280.503951 167.627884 \r\nL 282.693617 163.794424 \r\nL 283.78845 163.742888 \r\nL 284.883284 165.201637 \r\nL 285.978117 164.759303 \r\nL 287.07295 166.262244 \r\nL 288.167783 168.015287 \r\nL 289.262616 169.458742 \r\nL 290.35745 169.760634 \r\nL 291.452283 169.52621 \r\nL 292.547116 168.744406 \r\nL 293.641949 169.086348 \r\nL 295.831616 168.53571 \r\nL 296.926449 167.988785 \r\nL 298.021282 167.628475 \r\nL 299.116116 166.574185 \r\nL 300.210949 168.313252 \r\nL 302.400615 167.546797 \r\nL 303.495448 166.665652 \r\nL 304.590282 165.294072 \r\nL 305.685115 164.511983 \r\nL 306.779948 163.874685 \r\nL 307.874781 164.834647 \r\nL 308.969615 165.21224 \r\nL 310.064448 165.343675 \r\nL 311.159281 164.982649 \r\nL 312.254114 165.188045 \r\nL 313.348947 166.381503 \r\nL 314.443781 168.922479 \r\nL 315.538614 168.658375 \r\nL 316.633447 168.902489 \r\nL 317.72828 167.858454 \r\nL 318.823114 168.564254 \r\nL 319.917947 167.201017 \r\nL 322.107613 165.109602 \r\nL 323.202446 165.191332 \r\nL 324.29728 166.414721 \r\nL 326.486946 167.487584 \r\nL 327.581779 165.709947 \r\nL 328.676613 163.383735 \r\nL 329.771446 163.227463 \r\nL 330.866279 164.593017 \r\nL 331.961112 166.802867 \r\nL 333.055945 165.666541 \r\nL 334.150779 168.702771 \r\nL 335.245612 169.198268 \r\nL 336.340445 168.574471 \r\nL 337.435278 166.58576 \r\nL 338.530112 165.854575 \r\nL 339.624945 164.802145 \r\nL 340.719778 166.195382 \r\nL 341.814611 168.375733 \r\nL 342.909444 168.478078 \r\nL 344.004278 167.396047 \r\nL 345.099111 168.793926 \r\nL 346.193944 166.989691 \r\nL 347.288777 167.34945 \r\nL 348.383611 167.90246 \r\nL 349.478444 166.199969 \r\nL 350.573277 166.176486 \r\nL 351.66811 168.321653 \r\nL 352.762944 168.003853 \r\nL 353.857777 168.373295 \r\nL 354.95261 168.370924 \r\nL 356.047443 168.627386 \r\nL 356.047443 168.627386 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 36.465625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 371.265625 224.64 \r\nL 371.265625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 371.265625 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 36.465625 7.2 \r\nL 371.265625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"paca74df3fa\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"36.465625\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "#clf.history['loss']\n",
    "plt.plot(clf.history['loss'][5:])\n",
    "plt.plot(clf.history['val_0_logloss'][5:])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "predict = clf.predict_proba(X_val.values)\n",
    "predict = np.array(predict)\n",
    "predict = pd.DataFrame(predict.reshape(predict.shape[1],3), columns=[0.0, 1.0, 2.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.8292999066046411"
      ]
     },
     "metadata": {},
     "execution_count": 53
    }
   ],
   "source": [
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "log_loss(y_val_onehot, predict)"
   ]
  },
  {
   "source": [
    "* n_d=8, n_a=8, n_steps=5, lambda_sparse=1e-1, gamma = 1.3, scheduler_params = {\"gamma\": 0.95, \"step_size\": 20} = **0.8380463815163395**\n",
    "* n_d=8, n_a=8, n_steps=5, lambda_sparse=1e-2, gamma = 1.3, scheduler_params = {\"gamma\": 0.95, \"step_size\": 20} = **0.8327229226728547**\n",
    "* n_d=8, n_a=8, n_steps=5, lambda_sparse=1e-3, gamma = 1.3, scheduler_params = {\"gamma\": 0.95, \"step_size\": 20} = **0.8373788992342593**\n",
    "* n_d=16, n_a=16, n_steps=5, lambda_sparse=1e-2, gamma = 1.3, scheduler_params = {\"gamma\": 0.95, \"step_size\": 20} = **0.8265579702642575**\n",
    "* 나머지 hyperparameter에 대해서는 logloss가 잘 나오지 않았음. 추후 이것저것 더 해볼 예정"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "array([0.00882648, 0.03877237, 0.04557721, 0.11152691, 0.01798807,\n",
       "       0.0165193 , 0.03604957, 0.07089741, 0.02188411, 0.04779797,\n",
       "       0.08276292, 0.02145794, 0.07950223, 0.05027626, 0.03550007,\n",
       "       0.07715966, 0.23750152])"
      ]
     },
     "metadata": {},
     "execution_count": 54
    }
   ],
   "source": [
    "clf.feature_importances_"
   ]
  },
  {
   "source": [
    "### 6_1) unsupervised pretraining test\n",
    "* 논문에서 unsupervised training시 효과가 더 좋다고 나와있고, TabNet을 구현한 github repo에서도 해당 기능을 만들어놨는데, 아직까지는 별로 효과가 없어보임."
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Device used : cuda\n",
      "epoch 0  | loss: 14736106.94823| val_0_unsup_loss: 2141943.5|  0:00:01s\n",
      "epoch 1  | loss: 451335.21824| val_0_unsup_loss: 765179.4375|  0:00:03s\n",
      "epoch 2  | loss: 126466.2554| val_0_unsup_loss: 105598.03906|  0:00:05s\n",
      "epoch 3  | loss: 57144.52084| val_0_unsup_loss: 231466.78125|  0:00:07s\n",
      "epoch 4  | loss: 69760.87634| val_0_unsup_loss: 593715.125|  0:00:09s\n",
      "epoch 5  | loss: 39018.68808| val_0_unsup_loss: 2260838.5|  0:00:11s\n",
      "epoch 6  | loss: 31996.30558| val_0_unsup_loss: 1529929.75|  0:00:13s\n",
      "epoch 7  | loss: 56922.8018| val_0_unsup_loss: 1733234.5|  0:00:15s\n",
      "epoch 8  | loss: 22323.86927| val_0_unsup_loss: 1830981.25|  0:00:17s\n",
      "epoch 9  | loss: 15799.52398| val_0_unsup_loss: 2272647.75|  0:00:18s\n",
      "epoch 10 | loss: 13863.3387| val_0_unsup_loss: 592114.875|  0:00:20s\n",
      "epoch 11 | loss: 16134.6312| val_0_unsup_loss: 855067.9375|  0:00:22s\n",
      "epoch 12 | loss: 23537.24983| val_0_unsup_loss: 581907.625|  0:00:24s\n",
      "\n",
      "Early stopping occurred at epoch 12 with best_epoch = 2 and best_val_0_unsup_loss = 105598.03906\n",
      "Best weights from best epoch are automatically used!\n",
      "Device used : cuda\n",
      "epoch 0  | loss: 1.88974 | val_0_logloss: 1.43832 |  0:00:02s\n",
      "epoch 1  | loss: 0.92165 | val_0_logloss: 0.96391 |  0:00:04s\n",
      "epoch 2  | loss: 0.894   | val_0_logloss: 0.9187  |  0:00:06s\n",
      "epoch 3  | loss: 0.88475 | val_0_logloss: 0.88917 |  0:00:09s\n",
      "epoch 4  | loss: 0.87701 | val_0_logloss: 0.87152 |  0:00:11s\n",
      "epoch 5  | loss: 0.86727 | val_0_logloss: 0.86395 |  0:00:14s\n",
      "epoch 6  | loss: 0.86418 | val_0_logloss: 0.86417 |  0:00:17s\n",
      "epoch 7  | loss: 0.86081 | val_0_logloss: 0.8531  |  0:00:19s\n",
      "epoch 8  | loss: 0.85838 | val_0_logloss: 0.85465 |  0:00:21s\n",
      "epoch 9  | loss: 0.85729 | val_0_logloss: 0.85502 |  0:00:24s\n",
      "epoch 10 | loss: 0.8556  | val_0_logloss: 0.84817 |  0:00:26s\n",
      "epoch 11 | loss: 0.85656 | val_0_logloss: 0.85098 |  0:00:28s\n",
      "epoch 12 | loss: 0.85229 | val_0_logloss: 0.84276 |  0:00:30s\n",
      "epoch 13 | loss: 0.85204 | val_0_logloss: 0.84683 |  0:00:32s\n",
      "epoch 14 | loss: 0.85055 | val_0_logloss: 0.84815 |  0:00:34s\n",
      "epoch 15 | loss: 0.85043 | val_0_logloss: 0.84787 |  0:00:36s\n",
      "epoch 16 | loss: 0.8468  | val_0_logloss: 0.84639 |  0:00:38s\n",
      "epoch 17 | loss: 0.84986 | val_0_logloss: 0.84662 |  0:00:41s\n",
      "epoch 18 | loss: 0.84709 | val_0_logloss: 0.84516 |  0:00:43s\n",
      "epoch 19 | loss: 0.84653 | val_0_logloss: 0.8433  |  0:00:45s\n",
      "epoch 20 | loss: 0.84546 | val_0_logloss: 0.84249 |  0:00:47s\n",
      "epoch 21 | loss: 0.84569 | val_0_logloss: 0.8459  |  0:00:49s\n",
      "epoch 22 | loss: 0.84526 | val_0_logloss: 0.83973 |  0:00:51s\n",
      "epoch 23 | loss: 0.84491 | val_0_logloss: 0.84152 |  0:00:54s\n",
      "epoch 24 | loss: 0.84373 | val_0_logloss: 0.84205 |  0:00:56s\n",
      "epoch 25 | loss: 0.84259 | val_0_logloss: 0.84243 |  0:00:58s\n",
      "epoch 26 | loss: 0.84261 | val_0_logloss: 0.84093 |  0:01:01s\n",
      "epoch 27 | loss: 0.84112 | val_0_logloss: 0.84049 |  0:01:03s\n",
      "epoch 28 | loss: 0.83907 | val_0_logloss: 0.83988 |  0:01:06s\n",
      "epoch 29 | loss: 0.83834 | val_0_logloss: 0.84103 |  0:01:08s\n",
      "epoch 30 | loss: 0.84018 | val_0_logloss: 0.84035 |  0:01:10s\n",
      "epoch 31 | loss: 0.83829 | val_0_logloss: 0.8412  |  0:01:12s\n",
      "epoch 32 | loss: 0.83696 | val_0_logloss: 0.84156 |  0:01:15s\n",
      "epoch 33 | loss: 0.83651 | val_0_logloss: 0.83996 |  0:01:17s\n",
      "epoch 34 | loss: 0.83592 | val_0_logloss: 0.84048 |  0:01:19s\n",
      "epoch 35 | loss: 0.83608 | val_0_logloss: 0.83915 |  0:01:21s\n",
      "epoch 36 | loss: 0.83508 | val_0_logloss: 0.8422  |  0:01:23s\n",
      "epoch 37 | loss: 0.83471 | val_0_logloss: 0.83834 |  0:01:25s\n",
      "epoch 38 | loss: 0.83405 | val_0_logloss: 0.83936 |  0:01:27s\n",
      "epoch 39 | loss: 0.83404 | val_0_logloss: 0.83999 |  0:01:30s\n",
      "epoch 40 | loss: 0.83582 | val_0_logloss: 0.84102 |  0:01:33s\n",
      "epoch 41 | loss: 0.83659 | val_0_logloss: 0.84053 |  0:01:35s\n",
      "epoch 42 | loss: 0.83588 | val_0_logloss: 0.84046 |  0:01:37s\n",
      "epoch 43 | loss: 0.83489 | val_0_logloss: 0.83903 |  0:01:39s\n",
      "epoch 44 | loss: 0.83568 | val_0_logloss: 0.84566 |  0:01:41s\n",
      "epoch 45 | loss: 0.83908 | val_0_logloss: 0.8407  |  0:01:43s\n",
      "epoch 46 | loss: 0.83765 | val_0_logloss: 0.84775 |  0:01:45s\n",
      "epoch 47 | loss: 0.83659 | val_0_logloss: 0.83983 |  0:01:48s\n",
      "epoch 48 | loss: 0.83622 | val_0_logloss: 0.84018 |  0:01:50s\n",
      "epoch 49 | loss: 0.83389 | val_0_logloss: 0.83935 |  0:01:52s\n",
      "epoch 50 | loss: 0.8336  | val_0_logloss: 0.84191 |  0:01:54s\n",
      "epoch 51 | loss: 0.83422 | val_0_logloss: 0.83881 |  0:01:56s\n",
      "epoch 52 | loss: 0.83315 | val_0_logloss: 0.83724 |  0:01:58s\n",
      "epoch 53 | loss: 0.83255 | val_0_logloss: 0.83854 |  0:02:00s\n",
      "epoch 54 | loss: 0.83295 | val_0_logloss: 0.84099 |  0:02:03s\n",
      "epoch 55 | loss: 0.83235 | val_0_logloss: 0.83983 |  0:02:05s\n",
      "epoch 56 | loss: 0.83177 | val_0_logloss: 0.83957 |  0:02:07s\n",
      "epoch 57 | loss: 0.82985 | val_0_logloss: 0.84064 |  0:02:09s\n",
      "epoch 58 | loss: 0.83048 | val_0_logloss: 0.83933 |  0:02:11s\n",
      "epoch 59 | loss: 0.83033 | val_0_logloss: 0.83875 |  0:02:14s\n",
      "epoch 60 | loss: 0.83056 | val_0_logloss: 0.84037 |  0:02:16s\n",
      "epoch 61 | loss: 0.8318  | val_0_logloss: 0.84268 |  0:02:18s\n",
      "epoch 62 | loss: 0.83299 | val_0_logloss: 0.84098 |  0:02:21s\n",
      "epoch 63 | loss: 0.83299 | val_0_logloss: 0.84077 |  0:02:23s\n",
      "epoch 64 | loss: 0.83148 | val_0_logloss: 0.83962 |  0:02:27s\n",
      "epoch 65 | loss: 0.83055 | val_0_logloss: 0.84152 |  0:02:30s\n",
      "epoch 66 | loss: 0.82993 | val_0_logloss: 0.84382 |  0:02:33s\n",
      "epoch 67 | loss: 0.82979 | val_0_logloss: 0.84261 |  0:02:35s\n",
      "epoch 68 | loss: 0.82608 | val_0_logloss: 0.84331 |  0:02:37s\n",
      "epoch 69 | loss: 0.82683 | val_0_logloss: 0.84376 |  0:02:40s\n",
      "epoch 70 | loss: 0.82669 | val_0_logloss: 0.84185 |  0:02:42s\n",
      "epoch 71 | loss: 0.82694 | val_0_logloss: 0.84071 |  0:02:45s\n",
      "epoch 72 | loss: 0.82731 | val_0_logloss: 0.84347 |  0:02:47s\n",
      "epoch 73 | loss: 0.82748 | val_0_logloss: 0.84428 |  0:02:49s\n",
      "epoch 74 | loss: 0.82718 | val_0_logloss: 0.84091 |  0:02:51s\n",
      "epoch 75 | loss: 0.82493 | val_0_logloss: 0.84334 |  0:02:53s\n",
      "epoch 76 | loss: 0.82595 | val_0_logloss: 0.84467 |  0:02:55s\n",
      "epoch 77 | loss: 0.82539 | val_0_logloss: 0.84558 |  0:02:58s\n",
      "epoch 78 | loss: 0.82285 | val_0_logloss: 0.84989 |  0:03:00s\n",
      "epoch 79 | loss: 0.82221 | val_0_logloss: 0.84371 |  0:03:02s\n",
      "epoch 80 | loss: 0.82213 | val_0_logloss: 0.84415 |  0:03:05s\n",
      "epoch 81 | loss: 0.82067 | val_0_logloss: 0.84241 |  0:03:07s\n",
      "epoch 82 | loss: 0.82252 | val_0_logloss: 0.84381 |  0:03:09s\n",
      "epoch 83 | loss: 0.81983 | val_0_logloss: 0.84223 |  0:03:12s\n",
      "epoch 84 | loss: 0.82115 | val_0_logloss: 0.84455 |  0:03:14s\n",
      "epoch 85 | loss: 0.82055 | val_0_logloss: 0.84263 |  0:03:16s\n",
      "epoch 86 | loss: 0.8212  | val_0_logloss: 0.84477 |  0:03:18s\n",
      "epoch 87 | loss: 0.82073 | val_0_logloss: 0.84224 |  0:03:20s\n",
      "epoch 88 | loss: 0.81973 | val_0_logloss: 0.84226 |  0:03:22s\n",
      "epoch 89 | loss: 0.81659 | val_0_logloss: 0.8415  |  0:03:25s\n",
      "epoch 90 | loss: 0.81654 | val_0_logloss: 0.84339 |  0:03:27s\n",
      "epoch 91 | loss: 0.81581 | val_0_logloss: 0.8428  |  0:03:29s\n",
      "epoch 92 | loss: 0.81354 | val_0_logloss: 0.8438  |  0:03:31s\n",
      "epoch 93 | loss: 0.81551 | val_0_logloss: 0.84423 |  0:03:33s\n",
      "epoch 94 | loss: 0.81307 | val_0_logloss: 0.84457 |  0:03:36s\n",
      "epoch 95 | loss: 0.82087 | val_0_logloss: 0.8456  |  0:03:38s\n",
      "epoch 96 | loss: 0.81915 | val_0_logloss: 0.8428  |  0:03:40s\n",
      "epoch 97 | loss: 0.81667 | val_0_logloss: 0.84592 |  0:03:42s\n",
      "epoch 98 | loss: 0.81694 | val_0_logloss: 0.84747 |  0:03:44s\n",
      "epoch 99 | loss: 0.81709 | val_0_logloss: 0.84307 |  0:03:46s\n",
      "epoch 100| loss: 0.8148  | val_0_logloss: 0.84304 |  0:03:49s\n",
      "epoch 101| loss: 0.81305 | val_0_logloss: 0.84587 |  0:03:51s\n",
      "epoch 102| loss: 0.81292 | val_0_logloss: 0.84545 |  0:03:53s\n",
      "\n",
      "Early stopping occurred at epoch 102 with best_epoch = 52 and best_val_0_logloss = 0.83724\n",
      "Best weights from best epoch are automatically used!\n"
     ]
    }
   ],
   "source": [
    "from pytorch_tabnet.pretraining import TabNetPretrainer\n",
    "from pytorch_tabnet.multitask import TabNetMultiTaskClassifier\n",
    "\n",
    "unsupervised_model = TabNetPretrainer(\n",
    "    optimizer_fn = torch.optim.Adam,\n",
    "    optimizer_params = dict(lr=2e-2),\n",
    "    mask_type='entmax'\n",
    ")\n",
    "\n",
    "unsupervised_model.fit(\n",
    "    X_train = X_train.values,\n",
    "    eval_set = [X_val.values],\n",
    "    pretraining_ratio=0.8\n",
    ")\n",
    "\n",
    "clf = TabNetMultiTaskClassifier(\n",
    "    n_d=16, n_a=16, n_steps=5,\n",
    "    lambda_sparse=1e-2,\n",
    "    gamma = 1.3,\n",
    "    optimizer_fn=torch.optim.Adam,\n",
    "    optimizer_params=dict(lr=1e-2),\n",
    "    scheduler_params = {\"gamma\": 0.9, \"step_size\": 20},\n",
    "    scheduler_fn=torch.optim.lr_scheduler.StepLR,\n",
    "    mask_type='entmax',\n",
    "    device_name='cuda'\n",
    ")\n",
    "\n",
    "clf.fit(\n",
    "    X_train = X_train.values, y_train = np.array(Y_train).reshape(Y_train.shape[0],1),\n",
    "    eval_set = [(X_val.values, np.array(Y_val).reshape(Y_val.shape[0],1))],\n",
    "    max_epochs=300,\n",
    "    patience=50,\n",
    "    batch_size=1024,\n",
    "    virtual_batch_size=128,\n",
    "    num_workers=0,\n",
    "    drop_last=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "0.837241908002339"
      ]
     },
     "metadata": {},
     "execution_count": 64
    }
   ],
   "source": [
    "predict = clf.predict_proba(X_val.values)\n",
    "predict = pd.DataFrame(np.reshape(np.array(predict), (np.array(predict).shape[1],3)), columns=[0.0, 1.0, 2.0])\n",
    "y_val_onehot = pd.get_dummies(Y_val)\n",
    "log_loss(y_val_onehot, predict)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x27573e3bc70>]"
      ]
     },
     "metadata": {},
     "execution_count": 65
    },
    {
     "output_type": "display_data",
     "data": {
      "text/plain": "<Figure size 432x288 with 1 Axes>",
      "image/svg+xml": "<?xml version=\"1.0\" encoding=\"utf-8\" standalone=\"no\"?>\r\n<!DOCTYPE svg PUBLIC \"-//W3C//DTD SVG 1.1//EN\"\r\n  \"http://www.w3.org/Graphics/SVG/1.1/DTD/svg11.dtd\">\r\n<svg height=\"248.518125pt\" version=\"1.1\" viewBox=\"0 0 382.204502 248.518125\" width=\"382.204502pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\r\n <metadata>\r\n  <rdf:RDF xmlns:cc=\"http://creativecommons.org/ns#\" xmlns:dc=\"http://purl.org/dc/elements/1.1/\" xmlns:rdf=\"http://www.w3.org/1999/02/22-rdf-syntax-ns#\">\r\n   <cc:Work>\r\n    <dc:type rdf:resource=\"http://purl.org/dc/dcmitype/StillImage\"/>\r\n    <dc:date>2021-05-11T01:34:14.241290</dc:date>\r\n    <dc:format>image/svg+xml</dc:format>\r\n    <dc:creator>\r\n     <cc:Agent>\r\n      <dc:title>Matplotlib v3.4.1, https://matplotlib.org/</dc:title>\r\n     </cc:Agent>\r\n    </dc:creator>\r\n   </cc:Work>\r\n  </rdf:RDF>\r\n </metadata>\r\n <defs>\r\n  <style type=\"text/css\">*{stroke-linecap:butt;stroke-linejoin:round;}</style>\r\n </defs>\r\n <g id=\"figure_1\">\r\n  <g id=\"patch_1\">\r\n   <path d=\"M 0 248.518125 \r\nL 382.204502 248.518125 \r\nL 382.204502 0 \r\nL 0 0 \r\nz\r\n\" style=\"fill:none;\"/>\r\n  </g>\r\n  <g id=\"axes_1\">\r\n   <g id=\"patch_2\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 371.265625 224.64 \r\nL 371.265625 7.2 \r\nL 36.465625 7.2 \r\nz\r\n\" style=\"fill:#ffffff;\"/>\r\n   </g>\r\n   <g id=\"matplotlib.axis_1\">\r\n    <g id=\"xtick_1\">\r\n     <g id=\"line2d_1\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL 0 3.5 \r\n\" id=\"m7de9e960fe\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"51.683807\" xlink:href=\"#m7de9e960fe\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_1\">\r\n      <!-- 0 -->\r\n      <g transform=\"translate(48.502557 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 4250 \r\nQ 1547 4250 1301 3770 \r\nQ 1056 3291 1056 2328 \r\nQ 1056 1369 1301 889 \r\nQ 1547 409 2034 409 \r\nQ 2525 409 2770 889 \r\nQ 3016 1369 3016 2328 \r\nQ 3016 3291 2770 3770 \r\nQ 2525 4250 2034 4250 \r\nz\r\nM 2034 4750 \r\nQ 2819 4750 3233 4129 \r\nQ 3647 3509 3647 2328 \r\nQ 3647 1150 3233 529 \r\nQ 2819 -91 2034 -91 \r\nQ 1250 -91 836 529 \r\nQ 422 1150 422 2328 \r\nQ 422 3509 836 4129 \r\nQ 1250 4750 2034 4750 \r\nz\r\n\" id=\"DejaVuSans-30\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_2\">\r\n     <g id=\"line2d_2\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"114.439196\" xlink:href=\"#m7de9e960fe\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_2\">\r\n      <!-- 20 -->\r\n      <g transform=\"translate(108.076696 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 1228 531 \r\nL 3431 531 \r\nL 3431 0 \r\nL 469 0 \r\nL 469 531 \r\nQ 828 903 1448 1529 \r\nQ 2069 2156 2228 2338 \r\nQ 2531 2678 2651 2914 \r\nQ 2772 3150 2772 3378 \r\nQ 2772 3750 2511 3984 \r\nQ 2250 4219 1831 4219 \r\nQ 1534 4219 1204 4116 \r\nQ 875 4013 500 3803 \r\nL 500 4441 \r\nQ 881 4594 1212 4672 \r\nQ 1544 4750 1819 4750 \r\nQ 2544 4750 2975 4387 \r\nQ 3406 4025 3406 3419 \r\nQ 3406 3131 3298 2873 \r\nQ 3191 2616 2906 2266 \r\nQ 2828 2175 2409 1742 \r\nQ 1991 1309 1228 531 \r\nz\r\n\" id=\"DejaVuSans-32\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-32\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_3\">\r\n     <g id=\"line2d_3\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"177.194585\" xlink:href=\"#m7de9e960fe\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_3\">\r\n      <!-- 40 -->\r\n      <g transform=\"translate(170.832085 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2419 4116 \r\nL 825 1625 \r\nL 2419 1625 \r\nL 2419 4116 \r\nz\r\nM 2253 4666 \r\nL 3047 4666 \r\nL 3047 1625 \r\nL 3713 1625 \r\nL 3713 1100 \r\nL 3047 1100 \r\nL 3047 0 \r\nL 2419 0 \r\nL 2419 1100 \r\nL 313 1100 \r\nL 313 1709 \r\nL 2253 4666 \r\nz\r\n\" id=\"DejaVuSans-34\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-34\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_4\">\r\n     <g id=\"line2d_4\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"239.949974\" xlink:href=\"#m7de9e960fe\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_4\">\r\n      <!-- 60 -->\r\n      <g transform=\"translate(233.587474 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2113 2584 \r\nQ 1688 2584 1439 2293 \r\nQ 1191 2003 1191 1497 \r\nQ 1191 994 1439 701 \r\nQ 1688 409 2113 409 \r\nQ 2538 409 2786 701 \r\nQ 3034 994 3034 1497 \r\nQ 3034 2003 2786 2293 \r\nQ 2538 2584 2113 2584 \r\nz\r\nM 3366 4563 \r\nL 3366 3988 \r\nQ 3128 4100 2886 4159 \r\nQ 2644 4219 2406 4219 \r\nQ 1781 4219 1451 3797 \r\nQ 1122 3375 1075 2522 \r\nQ 1259 2794 1537 2939 \r\nQ 1816 3084 2150 3084 \r\nQ 2853 3084 3261 2657 \r\nQ 3669 2231 3669 1497 \r\nQ 3669 778 3244 343 \r\nQ 2819 -91 2113 -91 \r\nQ 1303 -91 875 529 \r\nQ 447 1150 447 2328 \r\nQ 447 3434 972 4092 \r\nQ 1497 4750 2381 4750 \r\nQ 2619 4750 2861 4703 \r\nQ 3103 4656 3366 4563 \r\nz\r\n\" id=\"DejaVuSans-36\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-36\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_5\">\r\n     <g id=\"line2d_5\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"302.705363\" xlink:href=\"#m7de9e960fe\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_5\">\r\n      <!-- 80 -->\r\n      <g transform=\"translate(296.342863 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2034 2216 \r\nQ 1584 2216 1326 1975 \r\nQ 1069 1734 1069 1313 \r\nQ 1069 891 1326 650 \r\nQ 1584 409 2034 409 \r\nQ 2484 409 2743 651 \r\nQ 3003 894 3003 1313 \r\nQ 3003 1734 2745 1975 \r\nQ 2488 2216 2034 2216 \r\nz\r\nM 1403 2484 \r\nQ 997 2584 770 2862 \r\nQ 544 3141 544 3541 \r\nQ 544 4100 942 4425 \r\nQ 1341 4750 2034 4750 \r\nQ 2731 4750 3128 4425 \r\nQ 3525 4100 3525 3541 \r\nQ 3525 3141 3298 2862 \r\nQ 3072 2584 2669 2484 \r\nQ 3125 2378 3379 2068 \r\nQ 3634 1759 3634 1313 \r\nQ 3634 634 3220 271 \r\nQ 2806 -91 2034 -91 \r\nQ 1263 -91 848 271 \r\nQ 434 634 434 1313 \r\nQ 434 1759 690 2068 \r\nQ 947 2378 1403 2484 \r\nz\r\nM 1172 3481 \r\nQ 1172 3119 1398 2916 \r\nQ 1625 2713 2034 2713 \r\nQ 2441 2713 2670 2916 \r\nQ 2900 3119 2900 3481 \r\nQ 2900 3844 2670 4047 \r\nQ 2441 4250 2034 4250 \r\nQ 1625 4250 1398 4047 \r\nQ 1172 3844 1172 3481 \r\nz\r\n\" id=\"DejaVuSans-38\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"xtick_6\">\r\n     <g id=\"line2d_6\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"365.460752\" xlink:href=\"#m7de9e960fe\" y=\"224.64\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_6\">\r\n      <!-- 100 -->\r\n      <g transform=\"translate(355.917002 239.238438)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 794 531 \r\nL 1825 531 \r\nL 1825 4091 \r\nL 703 3866 \r\nL 703 4441 \r\nL 1819 4666 \r\nL 2450 4666 \r\nL 2450 531 \r\nL 3481 531 \r\nL 3481 0 \r\nL 794 0 \r\nL 794 531 \r\nz\r\n\" id=\"DejaVuSans-31\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-31\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"127.246094\" xlink:href=\"#DejaVuSans-30\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"matplotlib.axis_2\">\r\n    <g id=\"ytick_1\">\r\n     <g id=\"line2d_7\">\r\n      <defs>\r\n       <path d=\"M 0 0 \r\nL -3.5 0 \r\n\" id=\"m9dda32fabc\" style=\"stroke:#000000;stroke-width:0.8;\"/>\r\n      </defs>\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m9dda32fabc\" y=\"189.005928\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_7\">\r\n      <!-- 0.82 -->\r\n      <g transform=\"translate(7.2 192.805147)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 684 794 \r\nL 1344 794 \r\nL 1344 0 \r\nL 684 0 \r\nL 684 794 \r\nz\r\n\" id=\"DejaVuSans-2e\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-32\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_2\">\r\n     <g id=\"line2d_8\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m9dda32fabc\" y=\"152.632189\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_8\">\r\n      <!-- 0.83 -->\r\n      <g transform=\"translate(7.2 156.431407)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 2597 2516 \r\nQ 3050 2419 3304 2112 \r\nQ 3559 1806 3559 1356 \r\nQ 3559 666 3084 287 \r\nQ 2609 -91 1734 -91 \r\nQ 1441 -91 1130 -33 \r\nQ 819 25 488 141 \r\nL 488 750 \r\nQ 750 597 1062 519 \r\nQ 1375 441 1716 441 \r\nQ 2309 441 2620 675 \r\nQ 2931 909 2931 1356 \r\nQ 2931 1769 2642 2001 \r\nQ 2353 2234 1838 2234 \r\nL 1294 2234 \r\nL 1294 2753 \r\nL 1863 2753 \r\nQ 2328 2753 2575 2939 \r\nQ 2822 3125 2822 3475 \r\nQ 2822 3834 2567 4026 \r\nQ 2313 4219 1838 4219 \r\nQ 1578 4219 1281 4162 \r\nQ 984 4106 628 3988 \r\nL 628 4550 \r\nQ 988 4650 1302 4700 \r\nQ 1616 4750 1894 4750 \r\nQ 2613 4750 3031 4423 \r\nQ 3450 4097 3450 3541 \r\nQ 3450 3153 3228 2886 \r\nQ 3006 2619 2597 2516 \r\nz\r\n\" id=\"DejaVuSans-33\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-33\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_3\">\r\n     <g id=\"line2d_9\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m9dda32fabc\" y=\"116.258449\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_9\">\r\n      <!-- 0.84 -->\r\n      <g transform=\"translate(7.2 120.057668)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-34\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_4\">\r\n     <g id=\"line2d_10\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m9dda32fabc\" y=\"79.88471\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_10\">\r\n      <!-- 0.85 -->\r\n      <g transform=\"translate(7.2 83.683929)scale(0.1 -0.1)\">\r\n       <defs>\r\n        <path d=\"M 691 4666 \r\nL 3169 4666 \r\nL 3169 4134 \r\nL 1269 4134 \r\nL 1269 2991 \r\nQ 1406 3038 1543 3061 \r\nQ 1681 3084 1819 3084 \r\nQ 2600 3084 3056 2656 \r\nQ 3513 2228 3513 1497 \r\nQ 3513 744 3044 326 \r\nQ 2575 -91 1722 -91 \r\nQ 1428 -91 1123 -41 \r\nQ 819 9 494 109 \r\nL 494 744 \r\nQ 775 591 1075 516 \r\nQ 1375 441 1709 441 \r\nQ 2250 441 2565 725 \r\nQ 2881 1009 2881 1497 \r\nQ 2881 1984 2565 2268 \r\nQ 2250 2553 1709 2553 \r\nQ 1456 2553 1204 2497 \r\nQ 953 2441 691 2322 \r\nL 691 4666 \r\nz\r\n\" id=\"DejaVuSans-35\" transform=\"scale(0.015625)\"/>\r\n       </defs>\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-35\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n    <g id=\"ytick_5\">\r\n     <g id=\"line2d_11\">\r\n      <g>\r\n       <use style=\"stroke:#000000;stroke-width:0.8;\" x=\"36.465625\" xlink:href=\"#m9dda32fabc\" y=\"43.510971\"/>\r\n      </g>\r\n     </g>\r\n     <g id=\"text_11\">\r\n      <!-- 0.86 -->\r\n      <g transform=\"translate(7.2 47.31019)scale(0.1 -0.1)\">\r\n       <use xlink:href=\"#DejaVuSans-30\"/>\r\n       <use x=\"63.623047\" xlink:href=\"#DejaVuSans-2e\"/>\r\n       <use x=\"95.410156\" xlink:href=\"#DejaVuSans-38\"/>\r\n       <use x=\"159.033203\" xlink:href=\"#DejaVuSans-36\"/>\r\n      </g>\r\n     </g>\r\n    </g>\r\n   </g>\r\n   <g id=\"line2d_12\">\r\n    <path clip-path=\"url(#p0d492dc58c)\" d=\"M 51.683807 17.083636 \r\nL 54.821576 28.319296 \r\nL 57.959346 40.576272 \r\nL 61.097115 49.421216 \r\nL 64.234885 53.369885 \r\nL 67.372654 59.519109 \r\nL 70.510424 56.036761 \r\nL 73.648193 71.55119 \r\nL 76.785962 72.468919 \r\nL 79.923732 77.871869 \r\nL 83.061501 78.314361 \r\nL 86.199271 91.51084 \r\nL 89.33704 80.408987 \r\nL 92.47481 90.460335 \r\nL 95.612579 92.50985 \r\nL 98.750349 96.413775 \r\nL 101.888118 95.567945 \r\nL 105.025887 97.138219 \r\nL 108.163657 98.383202 \r\nL 111.301426 102.692367 \r\nL 114.439196 106.827412 \r\nL 117.576965 106.75148 \r\nL 120.714735 112.193233 \r\nL 123.852504 119.640446 \r\nL 126.990274 122.290048 \r\nL 130.128043 115.606448 \r\nL 133.265812 122.471671 \r\nL 136.403582 127.304544 \r\nL 139.541351 128.938529 \r\nL 142.679121 131.08379 \r\nL 145.81689 130.528299 \r\nL 148.95466 134.16619 \r\nL 152.092429 135.487258 \r\nL 155.230199 137.884339 \r\nL 158.367968 137.930439 \r\nL 161.505737 131.453981 \r\nL 164.643507 128.648519 \r\nL 167.781276 131.234933 \r\nL 170.919046 134.85923 \r\nL 174.056815 131.956718 \r\nL 177.194585 119.590132 \r\nL 180.332354 124.801398 \r\nL 183.470124 128.658152 \r\nL 186.607893 130.025106 \r\nL 189.745662 138.465169 \r\nL 192.883432 139.541938 \r\nL 196.021201 137.292606 \r\nL 199.158971 141.15823 \r\nL 202.29674 143.345992 \r\nL 205.43451 141.904245 \r\nL 208.572279 144.074903 \r\nL 211.710049 146.178525 \r\nL 214.847818 153.18393 \r\nL 217.985588 150.869297 \r\nL 221.123357 151.433751 \r\nL 224.261126 150.610488 \r\nL 227.398896 146.089775 \r\nL 230.536665 141.740532 \r\nL 233.674435 141.769838 \r\nL 236.812204 147.25717 \r\nL 239.949974 150.616082 \r\nL 243.087743 152.901371 \r\nL 246.225513 153.391478 \r\nL 249.363282 166.877836 \r\nL 252.501051 164.159033 \r\nL 255.638821 164.658372 \r\nL 258.77659 163.750381 \r\nL 261.91436 162.399257 \r\nL 265.052129 161.811661 \r\nL 268.189899 162.902615 \r\nL 271.327668 171.065925 \r\nL 274.465438 167.376867 \r\nL 277.603207 169.418199 \r\nL 280.740976 178.626457 \r\nL 283.878746 180.952297 \r\nL 287.016515 181.250172 \r\nL 290.154285 186.574011 \r\nL 293.292054 179.827776 \r\nL 296.429824 189.619863 \r\nL 299.567593 184.837358 \r\nL 302.705363 186.988783 \r\nL 305.843132 184.6397 \r\nL 308.980901 186.343243 \r\nL 312.118671 190.00136 \r\nL 315.25644 201.413877 \r\nL 318.39421 201.588296 \r\nL 321.531979 204.242657 \r\nL 324.669749 212.491252 \r\nL 327.807518 205.344327 \r\nL 330.945288 214.206435 \r\nL 334.083057 185.835614 \r\nL 337.220826 192.101741 \r\nL 340.358596 201.130291 \r\nL 343.496365 200.123394 \r\nL 346.634135 199.599057 \r\nL 349.771904 207.924502 \r\nL 352.909674 214.292271 \r\nL 356.047443 214.756364 \r\n\" style=\"fill:none;stroke:#1f77b4;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"line2d_13\">\r\n    <path clip-path=\"url(#p0d492dc58c)\" d=\"M 51.683807 29.139103 \r\nL 54.821576 28.327331 \r\nL 57.959346 68.619571 \r\nL 61.097115 62.955864 \r\nL 64.234885 61.620282 \r\nL 67.372654 86.530414 \r\nL 70.510424 76.304635 \r\nL 73.648193 106.236228 \r\nL 76.785962 91.412607 \r\nL 79.923732 86.614186 \r\nL 83.061501 87.628004 \r\nL 86.199271 93.02116 \r\nL 89.33704 92.166 \r\nL 92.47481 97.500042 \r\nL 95.612579 104.254133 \r\nL 98.750349 107.208862 \r\nL 101.888118 94.784466 \r\nL 105.025887 117.228427 \r\nL 108.163657 110.736704 \r\nL 111.301426 108.806634 \r\nL 114.439196 107.427278 \r\nL 117.576965 112.889512 \r\nL 120.714735 114.472569 \r\nL 123.852504 116.690385 \r\nL 126.990274 112.520951 \r\nL 130.128043 114.990611 \r\nL 133.265812 111.878103 \r\nL 136.403582 110.569719 \r\nL 139.541351 116.413934 \r\nL 142.679121 114.508797 \r\nL 145.81689 119.357309 \r\nL 148.95466 108.271888 \r\nL 152.092429 122.304545 \r\nL 155.230199 118.572397 \r\nL 158.367968 116.305565 \r\nL 161.505737 112.544205 \r\nL 164.643507 114.31445 \r\nL 167.781276 114.567673 \r\nL 170.919046 119.790641 \r\nL 174.056815 95.65906 \r\nL 177.194585 113.711302 \r\nL 180.332354 88.078136 \r\nL 183.470124 116.87523 \r\nL 186.607893 115.601517 \r\nL 189.745662 118.620266 \r\nL 192.883432 109.298358 \r\nL 196.021201 120.576811 \r\nL 199.158971 126.290661 \r\nL 202.29674 121.577755 \r\nL 205.43451 112.661853 \r\nL 208.572279 116.877811 \r\nL 211.710049 117.819183 \r\nL 214.847818 113.932471 \r\nL 217.985588 118.704347 \r\nL 221.123357 120.814713 \r\nL 224.261126 114.895112 \r\nL 227.398896 106.517645 \r\nL 230.536665 112.705698 \r\nL 233.674435 113.450969 \r\nL 236.812204 117.631113 \r\nL 239.949974 110.741222 \r\nL 243.087743 102.362777 \r\nL 246.225513 106.780134 \r\nL 249.363282 104.213502 \r\nL 252.501051 102.583773 \r\nL 255.638821 109.536193 \r\nL 258.77659 113.693514 \r\nL 261.91436 103.619574 \r\nL 265.052129 100.692463 \r\nL 268.189899 112.95071 \r\nL 271.327668 104.098566 \r\nL 274.465438 99.264868 \r\nL 277.603207 95.968645 \r\nL 280.740976 80.297351 \r\nL 283.878746 102.763222 \r\nL 287.016515 101.172379 \r\nL 290.154285 107.487574 \r\nL 293.292054 102.389964 \r\nL 296.429824 108.130422 \r\nL 299.567593 99.72082 \r\nL 302.705363 106.702129 \r\nL 305.843132 98.911738 \r\nL 308.980901 108.105306 \r\nL 312.118671 108.044972 \r\nL 315.25644 110.809046 \r\nL 318.39421 103.928876 \r\nL 321.531979 106.08806 \r\nL 324.669749 102.42535 \r\nL 327.807518 100.865514 \r\nL 330.945288 99.637024 \r\nL 334.083057 95.893598 \r\nL 337.220826 106.076578 \r\nL 340.358596 94.717058 \r\nL 343.496365 89.092392 \r\nL 346.634135 105.086467 \r\nL 349.771904 105.183621 \r\nL 352.909674 94.890651 \r\nL 356.047443 96.433541 \r\n\" style=\"fill:none;stroke:#ff7f0e;stroke-linecap:square;stroke-width:1.5;\"/>\r\n   </g>\r\n   <g id=\"patch_3\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 36.465625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_4\">\r\n    <path d=\"M 371.265625 224.64 \r\nL 371.265625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_5\">\r\n    <path d=\"M 36.465625 224.64 \r\nL 371.265625 224.64 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n   <g id=\"patch_6\">\r\n    <path d=\"M 36.465625 7.2 \r\nL 371.265625 7.2 \r\n\" style=\"fill:none;stroke:#000000;stroke-linecap:square;stroke-linejoin:miter;stroke-width:0.8;\"/>\r\n   </g>\r\n  </g>\r\n </g>\r\n <defs>\r\n  <clipPath id=\"p0d492dc58c\">\r\n   <rect height=\"217.44\" width=\"334.8\" x=\"36.465625\" y=\"7.2\"/>\r\n  </clipPath>\r\n </defs>\r\n</svg>\r\n",
      "image/png": "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\n"
     },
     "metadata": {
      "needs_background": "light"
     }
    }
   ],
   "source": [
    "plt.plot(clf.history['loss'][5:])\n",
    "plt.plot(clf.history['val_0_logloss'][5:])"
   ]
  },
  {
   "source": [
    "## 7) Neural Network using PyTorch(미완성)\n",
    "* PyTorch 튜토리얼 참고함\n",
    "* 보기에는 loss가 낮게 나오긴 한데 validation set에 대한 accuracy도 낮고, 실제로 predict값을 확인해보면 학습할수록 예측값이 2로 치우쳐져 있음... 이것저것 해보면서 수정할 예정\n",
    "* logloss는 아직 안뽑아봄"
   ],
   "cell_type": "markdown",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "device : cuda\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "from  torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print('device : {}'.format(device))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_ts = torch.from_numpy(X_train.values)\n",
    "Y_train_ts = np.reshape(Y_train.values, ((Y_train.values.shape[0], 1)))\n",
    "Y_train_ts = torch.from_numpy(Y_train_ts)\n",
    "\n",
    "X_val_ts = torch.from_numpy(X_val.values)\n",
    "Y_val_ts = np.reshape(Y_val.values, ((Y_val.values.shape[0], 1)))\n",
    "Y_val_ts = torch.from_numpy(Y_val_ts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = TensorDataset(X_train_ts, Y_train_ts)\n",
    "train_dataloader = DataLoader(train_dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "val_dataset = TensorDataset(X_val_ts, Y_val_ts)\n",
    "val_dataloader = DataLoader(val_dataset, batch_size=10, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "Epoch 1\n",
      "-------------------------------\n",
      "loss: 1.043623\n",
      "Test Error: \n",
      " Accuracy: 54.4%, Avg loss: 0.105044 \n",
      "\n",
      "Epoch 2\n",
      "-------------------------------\n",
      "loss: 0.990763\n",
      "Test Error: \n",
      " Accuracy: 62.8%, Avg loss: 0.099727 \n",
      "\n",
      "Epoch 3\n",
      "-------------------------------\n",
      "loss: 0.967593\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.096124 \n",
      "\n",
      "Epoch 4\n",
      "-------------------------------\n",
      "loss: 0.924047\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.094059 \n",
      "\n",
      "Epoch 5\n",
      "-------------------------------\n",
      "loss: 0.919947\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.092716 \n",
      "\n",
      "Epoch 6\n",
      "-------------------------------\n",
      "loss: 0.917004\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.091642 \n",
      "\n",
      "Epoch 7\n",
      "-------------------------------\n",
      "loss: 0.901252\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.091211 \n",
      "\n",
      "Epoch 8\n",
      "-------------------------------\n",
      "loss: 0.911933\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.090974 \n",
      "\n",
      "Epoch 9\n",
      "-------------------------------\n",
      "loss: 0.904229\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.090656 \n",
      "\n",
      "Epoch 10\n",
      "-------------------------------\n",
      "loss: 0.907615\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.090468 \n",
      "\n",
      "Epoch 11\n",
      "-------------------------------\n",
      "loss: 0.889362\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.090115 \n",
      "\n",
      "Epoch 12\n",
      "-------------------------------\n",
      "loss: 0.897094\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.089890 \n",
      "\n",
      "Epoch 13\n",
      "-------------------------------\n",
      "loss: 0.900430\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.089993 \n",
      "\n",
      "Epoch 14\n",
      "-------------------------------\n",
      "loss: 0.877917\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.090012 \n",
      "\n",
      "Epoch 15\n",
      "-------------------------------\n",
      "loss: 0.889668\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.089879 \n",
      "\n",
      "Epoch 16\n",
      "-------------------------------\n",
      "loss: 0.898133\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.090001 \n",
      "\n",
      "Epoch 17\n",
      "-------------------------------\n",
      "loss: 0.909671\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.089980 \n",
      "\n",
      "Epoch 18\n",
      "-------------------------------\n",
      "loss: 0.871264\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.089995 \n",
      "\n",
      "Epoch 19\n",
      "-------------------------------\n",
      "loss: 0.886300\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.089697 \n",
      "\n",
      "Epoch 20\n",
      "-------------------------------\n",
      "loss: 0.884738\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.089882 \n",
      "\n",
      "Epoch 21\n",
      "-------------------------------\n",
      "loss: 0.911368\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.089790 \n",
      "\n",
      "Epoch 22\n",
      "-------------------------------\n",
      "loss: 0.859194\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.089896 \n",
      "\n",
      "Epoch 23\n",
      "-------------------------------\n",
      "loss: 0.866716\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.089482 \n",
      "\n",
      "Epoch 24\n",
      "-------------------------------\n",
      "loss: 0.898161\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.089586 \n",
      "\n",
      "Epoch 25\n",
      "-------------------------------\n",
      "loss: 0.880817\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.089919 \n",
      "\n",
      "Epoch 26\n",
      "-------------------------------\n",
      "loss: 0.879343\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.089797 \n",
      "\n",
      "Epoch 27\n",
      "-------------------------------\n",
      "loss: 0.875105\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.089692 \n",
      "\n",
      "Epoch 28\n",
      "-------------------------------\n",
      "loss: 0.862707\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.089723 \n",
      "\n",
      "Epoch 29\n",
      "-------------------------------\n",
      "loss: 0.873609\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.090011 \n",
      "\n",
      "Epoch 30\n",
      "-------------------------------\n",
      "loss: 0.866503\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.089687 \n",
      "\n",
      "Epoch 31\n",
      "-------------------------------\n",
      "loss: 0.883527\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.089848 \n",
      "\n",
      "Epoch 32\n",
      "-------------------------------\n",
      "loss: 0.918122\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.089732 \n",
      "\n",
      "Epoch 33\n",
      "-------------------------------\n",
      "loss: 0.900352\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.089873 \n",
      "\n",
      "Epoch 34\n",
      "-------------------------------\n",
      "loss: 0.879232\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.089675 \n",
      "\n",
      "Epoch 35\n",
      "-------------------------------\n",
      "loss: 0.897815\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.089624 \n",
      "\n",
      "Epoch 36\n",
      "-------------------------------\n",
      "loss: 0.890801\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.089800 \n",
      "\n",
      "Epoch 37\n",
      "-------------------------------\n",
      "loss: 0.896767\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.089614 \n",
      "\n",
      "Epoch 38\n",
      "-------------------------------\n",
      "loss: 0.857145\n",
      "Test Error: \n",
      " Accuracy: 65.0%, Avg loss: 0.089405 \n",
      "\n",
      "Epoch 39\n",
      "-------------------------------\n",
      "loss: 0.901287\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.090101 \n",
      "\n",
      "Epoch 40\n",
      "-------------------------------\n",
      "loss: 0.885089\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.089949 \n",
      "\n",
      "Epoch 41\n",
      "-------------------------------\n",
      "loss: 0.879122\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.089662 \n",
      "\n",
      "Epoch 42\n",
      "-------------------------------\n",
      "loss: 0.883504\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.090109 \n",
      "\n",
      "Epoch 43\n",
      "-------------------------------\n",
      "loss: 0.877569\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.089980 \n",
      "\n",
      "Epoch 44\n",
      "-------------------------------\n",
      "loss: 0.887266\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.089818 \n",
      "\n",
      "Epoch 45\n",
      "-------------------------------\n",
      "loss: 0.875162\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 0.090355 \n",
      "\n",
      "Epoch 46\n",
      "-------------------------------\n",
      "loss: 0.871140\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.089585 \n",
      "\n",
      "Epoch 47\n",
      "-------------------------------\n",
      "loss: 0.883717\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.090328 \n",
      "\n",
      "Epoch 48\n",
      "-------------------------------\n",
      "loss: 0.886134\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.090142 \n",
      "\n",
      "Epoch 49\n",
      "-------------------------------\n",
      "loss: 0.868526\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.090019 \n",
      "\n",
      "Epoch 50\n",
      "-------------------------------\n",
      "loss: 0.890184\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.089559 \n",
      "\n",
      "Epoch 51\n",
      "-------------------------------\n",
      "loss: 0.892244\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.090019 \n",
      "\n",
      "Epoch 52\n",
      "-------------------------------\n",
      "loss: 0.838882\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.090144 \n",
      "\n",
      "Epoch 53\n",
      "-------------------------------\n",
      "loss: 0.841668\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.089943 \n",
      "\n",
      "Epoch 54\n",
      "-------------------------------\n",
      "loss: 0.881349\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.089868 \n",
      "\n",
      "Epoch 55\n",
      "-------------------------------\n",
      "loss: 0.861265\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.089724 \n",
      "\n",
      "Epoch 56\n",
      "-------------------------------\n",
      "loss: 0.858283\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.089846 \n",
      "\n",
      "Epoch 57\n",
      "-------------------------------\n",
      "loss: 0.872125\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.089912 \n",
      "\n",
      "Epoch 58\n",
      "-------------------------------\n",
      "loss: 0.876353\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.090085 \n",
      "\n",
      "Epoch 59\n",
      "-------------------------------\n",
      "loss: 0.868764\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.089791 \n",
      "\n",
      "Epoch 60\n",
      "-------------------------------\n",
      "loss: 0.863776\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.089815 \n",
      "\n",
      "Epoch 61\n",
      "-------------------------------\n",
      "loss: 0.884288\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.090148 \n",
      "\n",
      "Epoch 62\n",
      "-------------------------------\n",
      "loss: 0.879640\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.089848 \n",
      "\n",
      "Epoch 63\n",
      "-------------------------------\n",
      "loss: 0.877372\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.089806 \n",
      "\n",
      "Epoch 64\n",
      "-------------------------------\n",
      "loss: 0.871152\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.090219 \n",
      "\n",
      "Epoch 65\n",
      "-------------------------------\n",
      "loss: 0.868123\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.089740 \n",
      "\n",
      "Epoch 66\n",
      "-------------------------------\n",
      "loss: 0.876085\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.089855 \n",
      "\n",
      "Epoch 67\n",
      "-------------------------------\n",
      "loss: 0.875566\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.090351 \n",
      "\n",
      "Epoch 68\n",
      "-------------------------------\n",
      "loss: 0.882686\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.089833 \n",
      "\n",
      "Epoch 69\n",
      "-------------------------------\n",
      "loss: 0.870370\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.089977 \n",
      "\n",
      "Epoch 70\n",
      "-------------------------------\n",
      "loss: 0.861005\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.090185 \n",
      "\n",
      "Epoch 71\n",
      "-------------------------------\n",
      "loss: 0.859879\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.089601 \n",
      "\n",
      "Epoch 72\n",
      "-------------------------------\n",
      "loss: 0.855822\n",
      "Test Error: \n",
      " Accuracy: 65.3%, Avg loss: 0.089502 \n",
      "\n",
      "Epoch 73\n",
      "-------------------------------\n",
      "loss: 0.894916\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.090028 \n",
      "\n",
      "Epoch 74\n",
      "-------------------------------\n",
      "loss: 0.863057\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.089669 \n",
      "\n",
      "Epoch 75\n",
      "-------------------------------\n",
      "loss: 0.872066\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.090047 \n",
      "\n",
      "Epoch 76\n",
      "-------------------------------\n",
      "loss: 0.906616\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.089969 \n",
      "\n",
      "Epoch 77\n",
      "-------------------------------\n",
      "loss: 0.891612\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.090166 \n",
      "\n",
      "Epoch 78\n",
      "-------------------------------\n",
      "loss: 0.881250\n",
      "Test Error: \n",
      " Accuracy: 64.8%, Avg loss: 0.089732 \n",
      "\n",
      "Epoch 79\n",
      "-------------------------------\n",
      "loss: 0.836982\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.090144 \n",
      "\n",
      "Epoch 80\n",
      "-------------------------------\n",
      "loss: 0.858561\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.090088 \n",
      "\n",
      "Epoch 81\n",
      "-------------------------------\n",
      "loss: 0.890011\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.090039 \n",
      "\n",
      "Epoch 82\n",
      "-------------------------------\n",
      "loss: 0.898369\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.090254 \n",
      "\n",
      "Epoch 83\n",
      "-------------------------------\n",
      "loss: 0.835010\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.089741 \n",
      "\n",
      "Epoch 84\n",
      "-------------------------------\n",
      "loss: 0.868988\n",
      "Test Error: \n",
      " Accuracy: 64.3%, Avg loss: 0.090171 \n",
      "\n",
      "Epoch 85\n",
      "-------------------------------\n",
      "loss: 0.835389\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.089831 \n",
      "\n",
      "Epoch 86\n",
      "-------------------------------\n",
      "loss: 0.861649\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.089952 \n",
      "\n",
      "Epoch 87\n",
      "-------------------------------\n",
      "loss: 0.879637\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.090183 \n",
      "\n",
      "Epoch 88\n",
      "-------------------------------\n",
      "loss: 0.864085\n",
      "Test Error: \n",
      " Accuracy: 65.1%, Avg loss: 0.089729 \n",
      "\n",
      "Epoch 89\n",
      "-------------------------------\n",
      "loss: 0.866816\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.089728 \n",
      "\n",
      "Epoch 90\n",
      "-------------------------------\n",
      "loss: 0.889097\n",
      "Test Error: \n",
      " Accuracy: 64.7%, Avg loss: 0.089898 \n",
      "\n",
      "Epoch 91\n",
      "-------------------------------\n",
      "loss: 0.852480\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.090253 \n",
      "\n",
      "Epoch 92\n",
      "-------------------------------\n",
      "loss: 0.841112\n",
      "Test Error: \n",
      " Accuracy: 64.5%, Avg loss: 0.090104 \n",
      "\n",
      "Epoch 93\n",
      "-------------------------------\n",
      "loss: 0.874286\n",
      "Test Error: \n",
      " Accuracy: 64.1%, Avg loss: 0.090410 \n",
      "\n",
      "Epoch 94\n",
      "-------------------------------\n",
      "loss: 0.849242\n",
      "Test Error: \n",
      " Accuracy: 64.2%, Avg loss: 0.090349 \n",
      "\n",
      "Epoch 95\n",
      "-------------------------------\n",
      "loss: 0.856006\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.089670 \n",
      "\n",
      "Epoch 96\n",
      "-------------------------------\n",
      "loss: 0.838270\n",
      "Test Error: \n",
      " Accuracy: 64.4%, Avg loss: 0.089902 \n",
      "\n",
      "Epoch 97\n",
      "-------------------------------\n",
      "loss: 0.848676\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.090012 \n",
      "\n",
      "Epoch 98\n",
      "-------------------------------\n",
      "loss: 0.879288\n",
      "Test Error: \n",
      " Accuracy: 64.6%, Avg loss: 0.090069 \n",
      "\n",
      "Epoch 99\n",
      "-------------------------------\n",
      "loss: 0.821361\n",
      "Test Error: \n",
      " Accuracy: 64.9%, Avg loss: 0.089899 \n",
      "\n",
      "Epoch 100\n",
      "-------------------------------\n",
      "loss: 0.845332\n",
      "Test Error: \n",
      " Accuracy: 63.9%, Avg loss: 0.090336 \n",
      "\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(NeuralNetwork, self).__init__()\n",
    "        self.flatten = nn.Flatten()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(17, 50),\n",
    "            nn.BatchNorm1d(50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 50),\n",
    "            nn.BatchNorm1d(50),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(50, 3),\n",
    "            nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.flatten(x)\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "model = NeuralNetwork()\n",
    "learning_rate = 1e-3\n",
    "\n",
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # 예측(prediction)과 손실(loss) 계산\n",
    "        X=X.float()\n",
    "        pred = model(X)\n",
    "        y = y.reshape(y.shape[0],).long()\n",
    "        #print(f\"{pred.shape}, {y.shape}\")\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # 역전파\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    loss, current = loss.item(), batch * len(X)\n",
    "    print(f\"loss: {loss:>7f}\")\n",
    "            \n",
    "\n",
    "def test_loop(dataloader, model, loss_fn):\n",
    "    size = len(dataloader.dataset)\n",
    "    test_loss, correct = 0, 0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for X, y in dataloader:\n",
    "            X = X.float()\n",
    "            pred = model(X)\n",
    "            y = y.reshape(y.shape[0],).long()\n",
    "            test_loss += loss_fn(pred, y).item()\n",
    "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
    "\n",
    "    test_loss /= size\n",
    "    correct /= size\n",
    "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")\n",
    "\n",
    "loss_fn = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "epochs = 100\n",
    "for t in range(epochs):\n",
    "    print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "    train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "    test_loop(val_dataloader, model, loss_fn)\n",
    "print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ]
}